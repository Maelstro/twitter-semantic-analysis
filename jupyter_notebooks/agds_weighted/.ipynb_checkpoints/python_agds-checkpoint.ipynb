{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted AGDS\n",
    "# Maciej WÃ³jcik\n",
    "\n",
    "# Dependencies\n",
    "import yaml\n",
    "import nltk\n",
    "import gensim\n",
    "from pymongo import MongoClient\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(400)\n",
    "\n",
    "### FUNCTION DEFINITIONS\n",
    "\n",
    "# Create a stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Functions for stemming and lemmatization\n",
    "def stem_and_lemmatize(text:str) -> str:\n",
    "    \"\"\"Stems and lemmatizes a given text.\"\"\"\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess_texts(text_list: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Processes text to remove all unwanted words and symbols.\"\"\"\n",
    "\n",
    "    # Lowercase the tweets\n",
    "    text_list['processed_tweet'] = text_list['tweet_text'].str.lower()\n",
    "\n",
    "    # Regex patterns\n",
    "    url_pattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    user_pattern       = '@[^\\s]+'\n",
    "    alpha_pattern      = \"[^a-zA-Z]\"\n",
    "    sequence_pattern   = r\"(.)\\1\\1+\"\n",
    "    seq_replace_pattern = r\"\\1\\1\"\n",
    "\n",
    "    # Remove URLs from the tweet text\n",
    "    text_list['processed_tweet'] = [re.sub(url_pattern, ' ', str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "    # Remove username from the tweet text\n",
    "    text_list['processed_tweet'] = [re.sub(user_pattern, ' ', str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "    # Remove all non-alphanumeric symbols\n",
    "    text_list['processed_tweet'] = [re.sub(alpha_pattern, ' ', str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "    # Replace all 3 or more consecutive letters with 2 letters\n",
    "    text_list['processed_tweet'] = [re.sub(sequence_pattern, seq_replace_pattern, str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "\n",
    "    \n",
    "    full_tweet_list = []\n",
    "    for x in text_list['processed_tweet']:\n",
    "        full_tweet = ''\n",
    "        for word in x.split():\n",
    "            word = stem_and_lemmatize(word)\n",
    "            full_tweet += (word + ' ')\n",
    "        full_tweet_list.append(full_tweet)\n",
    "\n",
    "    text_list['processed_tweet'] = full_tweet_list\n",
    "\n",
    "    return text_list\n",
    "\n",
    "def preprocess_single_tweet(text: str) -> list:\n",
    "    # Lowercase the tweets\n",
    "    lc_text = text.lower()\n",
    "\n",
    "    # Regex patterns\n",
    "    url_pattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    user_pattern       = '@[^\\s]+'\n",
    "    alpha_pattern      = \"[^a-zA-Z]\"\n",
    "    sequence_pattern   = r\"(.)\\1\\1+\"\n",
    "    seq_replace_pattern = r\"\\1\\1\"\n",
    "\n",
    "    # Remove URLs from the tweet text\n",
    "    lc_text = re.sub(url_pattern, ' ', lc_text)\n",
    "\n",
    "    # Remove username from the tweet text\n",
    "    lc_text = re.sub(user_pattern, ' ', lc_text)\n",
    "\n",
    "    # Remove all non-alphanumeric symbols\n",
    "    lc_text = re.sub(alpha_pattern, ' ', lc_text)\n",
    "\n",
    "    # Replace all 3 or more consecutive letters with 2 letters\n",
    "    lc_text = re.sub(sequence_pattern, seq_replace_pattern, lc_text)\n",
    "\n",
    "    processed_text = []\n",
    "    for word in lc_text.split():\n",
    "        if word not in gensim.parsing.preprocessing.STOPWORDS and len(word) > 2:\n",
    "            word = stem_and_lemmatize(word)\n",
    "            processed_text.append(word)\n",
    "    return processed_text\n",
    "\n",
    "# DB connector\n",
    "def mongo_connect(server_name: str) -> MongoClient:\n",
    "    \"\"\"Creates connection to the MongoDB database with given server name.\"\"\"\n",
    "    client = MongoClient(server_name)\n",
    "    db = client.twitter_db\n",
    "    return db\n",
    "\n",
    "# Credential loader\n",
    "def load_db_credentials(file_path: str) -> (str, str):\n",
    "    \"\"\"Loads username and password from YAML file.\"\"\"\n",
    "    with open(file_path) as f:\n",
    "        key_data = yaml.safe_load(f)\n",
    "        username = key_data['mongo-db']['username']\n",
    "        passwd = key_data['mongo-db']['passwd']\n",
    "    return (username, passwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        _id  \\\n",
      "0  5f9f1c36b38e10f823bf2cdc   \n",
      "1  5f9f1c36b38e10f823bf2cdd   \n",
      "2  5f9f1c36b38e10f823bf2cde   \n",
      "3  5f9f1c36b38e10f823bf2cdf   \n",
      "4  5f9f1c36b38e10f823bf2ce0   \n",
      "\n",
      "                                          tweet_text    username  \\\n",
      "0  @AndruEdwards The hard work has paid off, this...  LEGO_Group   \n",
      "1  @soosupersam A great way to surprise your love...  LEGO_Group   \n",
      "2  You can now just bring the fun home, and reliv...  LEGO_Group   \n",
      "3  @at_knb Happy birthday to the master builder! ...  LEGO_Group   \n",
      "4                                      @dizunatsu ðŸ˜€ðŸ˜€  LEGO_Group   \n",
      "\n",
      "           created_at timestamp archetype  \n",
      "0 2020-11-01 19:32:05       NaT    artist  \n",
      "1 2020-11-01 19:09:40       NaT    artist  \n",
      "2 2020-11-01 14:00:36       NaT    artist  \n",
      "3 2020-10-31 17:16:57       NaT    artist  \n",
      "4 2020-10-31 15:18:50       NaT    artist  \n"
     ]
    }
   ],
   "source": [
    "# Extract data from MongoDB\n",
    "# Load credentials\n",
    "username, passwd = load_db_credentials('../../auth/read_only.yaml')\n",
    "\n",
    "# Connect user to MongoDB database\n",
    "db = mongo_connect(f\"mongodb+srv://{username}:{passwd}@tweetdb.kpcmn.mongodb.net/twitter_db?retryWrites=true&w=majority\")\n",
    "\n",
    "# Dataframe for all Tweets\n",
    "df_tweets = pd.DataFrame(columns=['_id',\n",
    "                                  'tweet_text',\n",
    "                                  'username',\n",
    "                                  'created_at'])\n",
    "\n",
    "# List of archetypes\n",
    "#TODO: Migrate list to single file\n",
    "archetype_list = ['artist',\n",
    "                 'caregiver',\n",
    "                 'everyman',\n",
    "                 'explorer',\n",
    "                 'guru',\n",
    "                 'hero',\n",
    "                 'innocent',\n",
    "                 'jester',\n",
    "                 'magician',\n",
    "                 'rebel',\n",
    "                 'ruler',\n",
    "                 'seducer']\n",
    "\n",
    "# Get all tweets from the database\n",
    "for archetype in archetype_list:\n",
    "    # Create a cursor for acquiring all posts from the collection\n",
    "    cursor = db[archetype].find()\n",
    "    \n",
    "    df_archetype = pd.DataFrame(list(cursor))\n",
    "    df_archetype['archetype'] = archetype\n",
    "    df_tweets = df_tweets.append(df_archetype, ignore_index=True)\n",
    "\n",
    "print(df_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess tweets\n",
    "df_tweets = preprocess_texts(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        _id  \\\n",
      "0  5f9f1c36b38e10f823bf2cdc   \n",
      "1  5f9f1c36b38e10f823bf2cdd   \n",
      "2  5f9f1c36b38e10f823bf2cde   \n",
      "3  5f9f1c36b38e10f823bf2cdf   \n",
      "4  5f9f1c36b38e10f823bf2ce0   \n",
      "\n",
      "                                          tweet_text    username  \\\n",
      "0  @AndruEdwards The hard work has paid off, this...  LEGO_Group   \n",
      "1  @soosupersam A great way to surprise your love...  LEGO_Group   \n",
      "2  You can now just bring the fun home, and reliv...  LEGO_Group   \n",
      "3  @at_knb Happy birthday to the master builder! ...  LEGO_Group   \n",
      "4                                      @dizunatsu ðŸ˜€ðŸ˜€  LEGO_Group   \n",
      "\n",
      "           created_at timestamp archetype  \\\n",
      "0 2020-11-01 19:32:05       NaT    artist   \n",
      "1 2020-11-01 19:09:40       NaT    artist   \n",
      "2 2020-11-01 14:00:36       NaT    artist   \n",
      "3 2020-10-31 17:16:57       NaT    artist   \n",
      "4 2020-10-31 15:18:50       NaT    artist   \n",
      "\n",
      "                                     processed_tweet  \n",
      "0  [the, hard, work, have, pay, off, this, be, aw...  \n",
      "1      [a, great, way, to, surpris, your, love, one]  \n",
      "2  [you, can, now, just, bring, the, fun, home, a...  \n",
      "3  [happi, birthday, to, the, master, builder, we...  \n",
      "4                                                 []  \n"
     ]
    }
   ],
   "source": [
    "# Tokenize the words of each Tweet\n",
    "df_tweets['processed_tweet'] = df_tweets['processed_tweet'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Print the processed dataframe\n",
    "print(df_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop tweets that have no words after processing\n",
    "df_tweets = df_tweets.drop(df_tweets[df_tweets['processed_tweet'].map(len) < 2].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        _id  \\\n",
      "0  5f9f1c36b38e10f823bf2cdc   \n",
      "1  5f9f1c36b38e10f823bf2cdd   \n",
      "2  5f9f1c36b38e10f823bf2cde   \n",
      "3  5f9f1c36b38e10f823bf2cdf   \n",
      "4  5f9f1c36b38e10f823bf2ce2   \n",
      "\n",
      "                                          tweet_text    username  \\\n",
      "0  @AndruEdwards The hard work has paid off, this...  LEGO_Group   \n",
      "1  @soosupersam A great way to surprise your love...  LEGO_Group   \n",
      "2  You can now just bring the fun home, and reliv...  LEGO_Group   \n",
      "3  @at_knb Happy birthday to the master builder! ...  LEGO_Group   \n",
      "4                        @Ranchie This is the way! ðŸ˜€  LEGO_Group   \n",
      "\n",
      "           created_at timestamp archetype  \\\n",
      "0 2020-11-01 19:32:05       NaT    artist   \n",
      "1 2020-11-01 19:09:40       NaT    artist   \n",
      "2 2020-11-01 14:00:36       NaT    artist   \n",
      "3 2020-10-31 17:16:57       NaT    artist   \n",
      "4 2020-10-31 15:16:26       NaT    artist   \n",
      "\n",
      "                                     processed_tweet  \n",
      "0  [the, hard, work, have, pay, off, this, be, aw...  \n",
      "1      [a, great, way, to, surpris, your, love, one]  \n",
      "2  [you, can, now, just, bring, the, fun, home, a...  \n",
      "3  [happi, birthday, to, the, master, builder, we...  \n",
      "4                               [this, be, the, way]  \n"
     ]
    }
   ],
   "source": [
    "# Print the processed dataframe (after dropping empty tweets)\n",
    "df_tweets = df_tweets.reset_index(drop=True)\n",
    "print(df_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maelstro/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorizer - trying to assign weights\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def fcn_stub(stub):\n",
    "    return stub\n",
    "\n",
    "vectorizer = TfidfVectorizer(input=\"content\", max_features=None, tokenizer=fcn_stub, preprocessor=fcn_stub)\n",
    "vectorizer.fit(df_tweets['processed_tweet'])\n",
    "df_tfidf = vectorizer.transform(df_tweets['processed_tweet'])\n",
    "df_tfidf = df_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>aa</th>\n",
       "      <th>aadarsha</th>\n",
       "      <th>aah</th>\n",
       "      <th>aan</th>\n",
       "      <th>aanhoudt</th>\n",
       "      <th>aankoop</th>\n",
       "      <th>aankoopbewij</th>\n",
       "      <th>aankopen</th>\n",
       "      <th>aarchi</th>\n",
       "      <th>...</th>\n",
       "      <th>zv</th>\n",
       "      <th>zwanesh</th>\n",
       "      <th>zwart</th>\n",
       "      <th>zwei</th>\n",
       "      <th>zwift</th>\n",
       "      <th>zy</th>\n",
       "      <th>zyciora</th>\n",
       "      <th>zyoom</th>\n",
       "      <th>zyra</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.177691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.115838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28972 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          a   aa  aadarsha  aah  aan  aanhoudt  aankoop  aankoopbewij  \\\n",
       "0  0.000000  0.0       0.0  0.0  0.0       0.0      0.0           0.0   \n",
       "1  0.177691  0.0       0.0  0.0  0.0       0.0      0.0           0.0   \n",
       "2  0.000000  0.0       0.0  0.0  0.0       0.0      0.0           0.0   \n",
       "3  0.115838  0.0       0.0  0.0  0.0       0.0      0.0           0.0   \n",
       "4  0.000000  0.0       0.0  0.0  0.0       0.0      0.0           0.0   \n",
       "\n",
       "   aankopen  aarchi  ...   zv  zwanesh  zwart  zwei  zwift   zy  zyciora  \\\n",
       "0       0.0     0.0  ...  0.0      0.0    0.0   0.0    0.0  0.0      0.0   \n",
       "1       0.0     0.0  ...  0.0      0.0    0.0   0.0    0.0  0.0      0.0   \n",
       "2       0.0     0.0  ...  0.0      0.0    0.0   0.0    0.0  0.0      0.0   \n",
       "3       0.0     0.0  ...  0.0      0.0    0.0   0.0    0.0  0.0      0.0   \n",
       "4       0.0     0.0  ...  0.0      0.0    0.0   0.0    0.0  0.0      0.0   \n",
       "\n",
       "   zyoom  zyra   zz  \n",
       "0    0.0   0.0  0.0  \n",
       "1    0.0   0.0  0.0  \n",
       "2    0.0   0.0  0.0  \n",
       "3    0.0   0.0  0.0  \n",
       "4    0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 28972 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get feature names\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocab = np.array(vocab)\n",
    "\n",
    "df_tfidf = pd.DataFrame(df_tfidf, columns=vocab)\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries - each layer with weighted edges\n",
    "AGDS_word_tweet = {}\n",
    "AGDS_tweet_class = {}\n",
    "\n",
    "# Populate layers\n",
    "for idx, tweet in df_tweets.iterrows():\n",
    "    if tweet['archetype'] not in AGDS_tweet_class:\n",
    "        AGDS_tweet_class[tweet['archetype']] = [idx]\n",
    "    else:\n",
    "        AGDS_tweet_class[tweet['archetype']].append(idx)\n",
    "    \n",
    "    unique_words = np.unique(tweet['processed_tweet'])\n",
    "    for word in unique_words:\n",
    "        if word not in AGDS_word_tweet.keys():\n",
    "            AGDS_word_tweet[word] = {idx: df_tfidf.iloc[idx][word]}\n",
    "        else:\n",
    "            try:\n",
    "                AGDS_word_tweet[word].update({idx: df_tfidf.iloc[idx][word]})\n",
    "            except:\n",
    "                print(f\"Error on index {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGDS representation size:  1311448\n",
      "Matrix representation size:  19459968\n",
      "AGDS needs ~ 14.84 times less space than standard matrix representation.\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "print(\"AGDS representation size: \", getsizeof(AGDS_word_tweet)+getsizeof(AGDS_tweet_class))\n",
    "print(\"Matrix representation size: \", getsizeof(df_tweets['processed_tweet']))\n",
    "print(\"AGDS needs ~\", round(getsizeof(\n",
    "    df_tweets['processed_tweet'])/(getsizeof(AGDS_word_tweet)+getsizeof(AGDS_tweet_class)), 2), \n",
    "      \"times less space than standard matrix representation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF as a measurement of similarity\n",
    "def get_similarity(tweet_text):\n",
    "    # Create dictionary with a sum of weights for every archetype\n",
    "    archetype_weights = {}\n",
    "    for arch in archetype_list:\n",
    "        archetype_weights[arch] = 0.0\n",
    "    weight_sum = 0.0\n",
    "    \n",
    "    # Process and tokenize the text\n",
    "    processed_text = preprocess_single_tweet(tweet_text)\n",
    "    \n",
    "    # Iterate over graph\n",
    "    for word in processed_text:\n",
    "        if word in AGDS_word_tweet.keys():\n",
    "            for k, v in AGDS_word_tweet[word].items():\n",
    "                for key_d, val_d in AGDS_tweet_class.items():\n",
    "                    if k in val_d:\n",
    "                        archetype_weights[key_d] += v\n",
    "                        weight_sum += v\n",
    "                        break\n",
    "    \n",
    "    # Normalize weights\n",
    "    for arch in archetype_list:\n",
    "        archetype_weights[arch] /= weight_sum\n",
    "    \n",
    "    import operator\n",
    "    max_arch = max(archetype_weights.items(), key=operator.itemgetter(1))[0]\n",
    "    \n",
    "    return (max_arch, archetype_weights[max_arch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real archetype: artist - classification: ('artist', 0.16662292317551186)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Real archetype: {df_tweets.iloc[1]['archetype']} - classification: {get_similarity(df_tweets.iloc[1]['tweet_text'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
