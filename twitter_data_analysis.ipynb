{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans, DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-ba4e779e9955>:11: FutureWarning: Treating datetime data as categorical rather than numeric in `.describe` is deprecated and will be removed in a future version of pandas. Specify `datetime_is_numeric=True` to silence this warning and adopt the future behavior now.\n",
      "  df.describe()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>username</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>303</td>\n",
       "      <td>34</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Make your mark – the new GLE Coupé is just mad...</td>\n",
       "      <td>DollarShaveClub</td>\n",
       "      <td>2020-10-15 18:34:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-10-08 07:26:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-10-16 10:16:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet_text         username  \\\n",
       "count                                                 303              303   \n",
       "unique                                                303               34   \n",
       "top     Make your mark – the new GLE Coupé is just mad...  DollarShaveClub   \n",
       "freq                                                    1               10   \n",
       "first                                                 NaN              NaN   \n",
       "last                                                  NaN              NaN   \n",
       "\n",
       "                 created_at  \n",
       "count                   303  \n",
       "unique                  293  \n",
       "top     2020-10-15 18:34:26  \n",
       "freq                      3  \n",
       "first   2020-10-08 07:26:26  \n",
       "last    2020-10-16 10:16:44  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log into MongoDB\n",
    "client = MongoClient('localhost')\n",
    "db = client.twitter_db\n",
    "\n",
    "# Read all tweets into DataFrame\n",
    "cursor = db.twitter_posts.find()\n",
    "df = pd.DataFrame(list(cursor))\n",
    "\n",
    "# Delete ID column\n",
    "del df['_id']\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_lemm = WordNetLemmatizer()\n",
    "\n",
    "# Tweet preprocessing\n",
    "def preprocess_texts(text_list: pd.DataFrame):\n",
    "\n",
    "    # Lowercase the tweets\n",
    "    text_list['processed_tweet'] = text_list['tweet_text'].str.lower()\n",
    "\n",
    "    # Regex patterns\n",
    "    url_pattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    user_pattern       = '@[^\\s]+'\n",
    "    alpha_pattern      = \"[^a-zA-Z0-9]\"\n",
    "    sequence_pattern   = r\"(.)\\1\\1+\"\n",
    "    seq_replace_pattern = r\"\\1\\1\"\n",
    "\n",
    "    # Remove URLs from the tweet text\n",
    "    text_list['processed_tweet'] = [re.sub(url_pattern, ' ', str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "    # Remove username from the tweet text\n",
    "    text_list['processed_tweet'] = [re.sub(user_pattern, ' ', str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "    # Remove all non-alphanumeric symbols\n",
    "    text_list['processed_tweet'] = [re.sub(alpha_pattern, ' ', str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "    # Replace all 3 or more consecutive letters with 2 letters\n",
    "    text_list['processed_tweet'] = [re.sub(sequence_pattern, seq_replace_pattern, str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "\n",
    "    full_tweet = ''\n",
    "    full_tweet_list = []\n",
    "    for x in text_list['processed_tweet']:\n",
    "        for word in x.split():\n",
    "            if word not in stopwords.words('english'):\n",
    "                if len(word) > 1:\n",
    "                    word = word_lemm.lemmatize(word)\n",
    "                    full_tweet += (word + ' ')\n",
    "        full_tweet_list.append(full_tweet)\n",
    "\n",
    "    text_list['processed_tweet'] = full_tweet_list\n",
    "\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer fitting ended in 1 seconds\n",
      "Number of feature_words: 5245\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "df_processed = preprocess_texts(df)\n",
    "\n",
    "# TF-IDF Vectorization - CountVectorize (Bag of Words), and then apply IF-IDF Transformer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=800000)\n",
    "start = time.time()\n",
    "vectorizer.fit(df_processed['processed_tweet'])\n",
    "print(f'Vectorizer fitting ended in {round(time.time()-start)} seconds')\n",
    "print(f'Number of feature_words: {len(vectorizer.get_feature_names())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " chanel\n",
      " dm\n",
      " team\n",
      " look\n",
      " please\n",
      " thanks\n",
      " louisvuitton\n",
      " new\n",
      " plastic\n",
      " ikea\n",
      " collection\n",
      " latest\n",
      " time\n",
      " experience\n",
      " like\n",
      " learn\n",
      " sorry\n",
      " help\n",
      " year\n",
      " show\n",
      "Cluster 1:\n",
      " team\n",
      " please\n",
      " number\n",
      " one\n",
      " year\n",
      " watch\n",
      " longwayup\n",
      " hey\n",
      " hear\n",
      " 800\n",
      " help\n",
      " customer\n",
      " call\n",
      " change\n",
      " climate\n",
      " hello\n",
      " hi\n",
      " check\n",
      " happy\n",
      " see\n",
      "Cluster 2:\n",
      " chanel\n",
      " dm\n",
      " team\n",
      " ikea\n",
      " please\n",
      " experience\n",
      " help\n",
      " sorry\n",
      " new\n",
      " thanks\n",
      " learn\n",
      " hear\n",
      " year\n",
      " look\n",
      " gabrielle\n",
      " gabrielle chanel\n",
      " design\n",
      " send\n",
      " life\n",
      " like\n",
      "Cluster 3:\n",
      " chanel\n",
      " dm\n",
      " team\n",
      " thanks\n",
      " max\n",
      " new\n",
      " louisvuitton\n",
      " look\n",
      " please\n",
      " je\n",
      " collection\n",
      " plastic\n",
      " like\n",
      " adobe\n",
      " adobe max\n",
      " latest\n",
      " ikea\n",
      " hear\n",
      " time\n",
      " help\n",
      "Cluster 4:\n",
      " team\n",
      " dyson\n",
      " year\n",
      " disneyplus\n",
      " dm\n",
      " please\n",
      " great\n",
      " number\n",
      " first\n",
      " change\n",
      " hi\n",
      " real\n",
      " streaming\n",
      " look\n",
      " thanks\n",
      " hey\n",
      " new\n",
      " know\n",
      " available\n",
      " 20\n",
      "Cluster 5:\n",
      " team\n",
      " climate\n",
      " event\n",
      " change\n",
      " greenest\n",
      " climate change\n",
      " framework\n",
      " signing\n",
      " year\n",
      " youth\n",
      " activist\n",
      " event watch\n",
      " activist latest\n",
      " youth climate\n",
      " live\n",
      " latest\n",
      " live event\n",
      " video\n",
      " video event\n",
      " raceagainstclimatechange\n",
      "Cluster 6:\n",
      " chanel\n",
      " thanks\n",
      " team\n",
      " help\n",
      " please\n",
      " dm\n",
      " hi\n",
      " woman\n",
      " new\n",
      " look\n",
      " bean\n",
      " max\n",
      " louisvuitton\n",
      " je\n",
      " someone hand\n",
      " someone\n",
      " hand help\n",
      " google\n",
      " hear\n",
      " time\n",
      "Cluster 7:\n",
      " chanel\n",
      " team\n",
      " dm\n",
      " please\n",
      " gabrielle chanel\n",
      " gabrielle\n",
      " year\n",
      " thanks\n",
      " look\n",
      " learn\n",
      " gabriellechanel\n",
      " chanel gabriellechanel\n",
      " inside chanel\n",
      " inside\n",
      " great\n",
      " number\n",
      " literature\n",
      " help\n",
      " experience\n",
      " sorry\n",
      "Cluster 8:\n",
      " team\n",
      " disneyplus\n",
      " year\n",
      " streaming\n",
      " new\n",
      " please\n",
      " first\n",
      " number\n",
      " real\n",
      " life\n",
      " one\n",
      " look\n",
      " watch\n",
      " know\n",
      " hi\n",
      " dm\n",
      " hear\n",
      " day\n",
      " change\n",
      " 20\n",
      "Cluster 9:\n",
      " team\n",
      " number\n",
      " year\n",
      " change\n",
      " climate\n",
      " call\n",
      " one\n",
      " customer\n",
      " please\n",
      " tv\n",
      " geraldine\n",
      " 800\n",
      " step\n",
      " help\n",
      " framework\n",
      " greenest\n",
      " signing\n",
      " programme\n",
      " climate change\n",
      " event\n"
     ]
    }
   ],
   "source": [
    "transformed_tweets = vectorizer.transform(df_processed['processed_tweet'])\n",
    "\n",
    "kmpp_model = KMeans(n_clusters=10, init='k-means++', n_init=1, max_iter=1000, tol=1e-5,\n",
    "                    random_state=2200)\n",
    "kmpp_model.fit(transformed_tweets)\n",
    "\n",
    "def show_training_results(model, n_clusters):\n",
    "    order_centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        print(\"Cluster %d:\" % i)\n",
    "        for ind in order_centroids[i, :20]:\n",
    "            print(' %s' % terms[ind])\n",
    "\n",
    "show_training_results(kmpp_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity          id                          date     query  \\\n",
       "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load new dataset - for final model\n",
    "new_dataset = pd.read_csv('data/trainingandtestdata/training.1600000.processed.noemoticon.csv',  \n",
    "                          header=None,\n",
    "                          parse_dates=True,\n",
    "                          encoding='ISO-8859-1',\n",
    "                          names=['polarity', 'id', 'date', 'query', 'user', 'text']\n",
    "                          )\n",
    "new_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lemm_new = WordNetLemmatizer()\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "\n",
    "# Tweet preprocessing\n",
    "def preprocess_texts_new(text_list: list):\n",
    "\n",
    "    # Lowercase the tweets\n",
    "    tweets = text_list.str.lower()\n",
    "\n",
    "    # Regex patterns\n",
    "    url_pattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    user_pattern       = '@[^\\s]+'\n",
    "    alpha_pattern      = \"[^a-zA-Z0-9]\"\n",
    "    sequence_pattern   = r\"(.)\\1\\1+\"\n",
    "    seq_replace_pattern = r\"\\1\\1\"\n",
    "\n",
    "    # Remove URLs from the tweet text\n",
    "    tweets = [re.sub(url_pattern, ' ', str(x))\n",
    "                                    for x in tweets]\n",
    "    # Remove username from the tweet text\n",
    "    tweets = [re.sub(user_pattern, ' ', str(x))\n",
    "                                    for x in tweets]\n",
    "    # Remove all non-alphanumeric symbols\n",
    "    tweets = [re.sub(alpha_pattern, ' ', str(x))\n",
    "                                    for x in tweets]\n",
    "    # Replace all 3 or more consecutive letters with 2 letters\n",
    "    tweets = [re.sub(sequence_pattern, seq_replace_pattern, str(x))\n",
    "                                    for x in tweets]\n",
    "\n",
    "    full_tweet = ''\n",
    "    full_tweet_list = []\n",
    "    for x in tweets:\n",
    "        for word in x.split():\n",
    "            if word not in nltk_stopwords:\n",
    "                if len(word) > 1:\n",
    "                    word = word_lemm_new.lemmatize(word)\n",
    "                    full_tweet += (word + ' ')\n",
    "        full_tweet_list.append(full_tweet)\n",
    "\n",
    "    return full_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "processed_texts = preprocess_texts_new(new_dataset['text'])\n",
    "\n",
    "# TF-IDF Vectorization - CountVectorize (Bag of Words), and then apply IF-IDF Transformer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "start = time.time()\n",
    "vectorizer.fit(processed_texts)\n",
    "print(f'Vectorizer fitting ended in {round(time.time()-start)} seconds')\n",
    "print(f'Number of feature_words: {len(vectorizer.get_feature_names())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_tweets = vectorizer.transform(df_processed['processed_tweet'])\n",
    "\n",
    "kmpp_model = KMeans(n_clusters=10, init='k-means++', n_init=1, max_iter=1000, tol=1e-5,\n",
    "                    random_state=2200)\n",
    "kmpp_model.fit(transformed_tweets)\n",
    "\n",
    "def show_training_results(model, n_clusters):\n",
    "    order_centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        print(\"Cluster %d:\" % i)\n",
    "        for ind in order_centroids[i, :20]:\n",
    "            print(' %s' % terms[ind])\n",
    "\n",
    "show_training_results(kmpp_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}