{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03778673-2505-4ab7-b3e7-76f903172a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate words with archetypes/character traits as intermediate layer\n",
    "# and with influencer as the \"last\" layer\n",
    "\n",
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "import toml\n",
    "import re\n",
    "import itertools\n",
    "from text_cleaner import *\n",
    "import operator\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "def extract_hashtags(post_text):\n",
    "    HASH_RE = re.compile(r\"\\#\\w+\")\n",
    "    out_list = re.findall(HASH_RE, post_text)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798a8899-ef4c-46e3-9046-ac5aa6a7dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['innocent', 'sage', 'explorer', 'outlaw', 'magician', 'hero', 'lover', 'jester', 'everyman', 'caregiver', 'ruler', 'creator', 'dominant', 'submissive', 'maximalist', 'minimalist', 'inspiring', 'systematic', 'discovering', 'conservative', 'verifying', 'overlooking', 'sharpening', 'harmonic', 'empathic', 'matter_of_fact', 'brave', 'protective', 'generous', 'thrifty', 'favourable', 'balanced', 'sensuality', 'intelligent', 'believe', 'egocentric', 'allocentric']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>innocent</th>\n",
       "      <th>sage</th>\n",
       "      <th>explorer</th>\n",
       "      <th>outlaw</th>\n",
       "      <th>magician</th>\n",
       "      <th>hero</th>\n",
       "      <th>lover</th>\n",
       "      <th>jester</th>\n",
       "      <th>everyman</th>\n",
       "      <th>caregiver</th>\n",
       "      <th>...</th>\n",
       "      <th>protective</th>\n",
       "      <th>generous</th>\n",
       "      <th>thrifty</th>\n",
       "      <th>favourable</th>\n",
       "      <th>balanced</th>\n",
       "      <th>sensuality</th>\n",
       "      <th>intelligent</th>\n",
       "      <th>believe</th>\n",
       "      <th>egocentric</th>\n",
       "      <th>allocentric</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>marek_grodzki</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vege_style_life</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oliwka__2007</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z_przestrzeni_serca</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zaradne_warsztaty</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     innocent  sage  explorer  outlaw  magician  hero  lover  \\\n",
       "id                                                                             \n",
       "marek_grodzki             0.0   0.0       0.0     0.0       0.0   0.0    0.0   \n",
       "vege_style_life           0.0   0.0       4.0     0.0       0.0   0.0    0.0   \n",
       "oliwka__2007              0.0   0.0       0.0     0.0       0.0   0.0    0.0   \n",
       "z_przestrzeni_serca       4.0   0.0       0.0     0.0       0.0   0.0    0.0   \n",
       "zaradne_warsztaty         3.0   0.0       0.0     0.0       3.0   0.0    0.0   \n",
       "\n",
       "                     jester  everyman  caregiver  ...  protective  generous  \\\n",
       "id                                                ...                         \n",
       "marek_grodzki           4.0       0.0        0.0  ...         2.0       3.0   \n",
       "vege_style_life         0.0       0.0        0.0  ...         4.0       4.0   \n",
       "oliwka__2007            0.0       4.0        0.0  ...         2.0       2.0   \n",
       "z_przestrzeni_serca     0.0       0.0        0.0  ...         4.0       3.0   \n",
       "zaradne_warsztaty       2.0       3.0        4.0  ...         3.0       4.0   \n",
       "\n",
       "                     thrifty  favourable  balanced  sensuality  intelligent  \\\n",
       "id                                                                            \n",
       "marek_grodzki            4.0         4.0       3.0         4.0          4.0   \n",
       "vege_style_life          4.0         4.0       3.0         3.0          3.0   \n",
       "oliwka__2007             0.0         3.0       1.0         2.0          4.0   \n",
       "z_przestrzeni_serca      0.0         4.0       4.0         3.0          4.0   \n",
       "zaradne_warsztaty        0.0         2.0       2.0         4.0          2.0   \n",
       "\n",
       "                     believe  egocentric  allocentric  \n",
       "id                                                     \n",
       "marek_grodzki            3.0         0.0          0.0  \n",
       "vege_style_life          2.0         0.0          3.0  \n",
       "oliwka__2007             1.0         0.0          3.0  \n",
       "z_przestrzeni_serca      4.0         0.0          1.0  \n",
       "zaradne_warsztaty        3.0         1.0          3.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the .csv with archetypes\n",
    "arch_df = pd.read_csv('archetypes_pl.csv', index_col=0)\n",
    "\n",
    "# Save the order of columns\n",
    "trait_list = arch_df.columns.tolist()\n",
    "\n",
    "# Show the table header and column list\n",
    "print(trait_list)\n",
    "arch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1f03a8-fd7c-4348-935f-a1f57e90af6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>innocent</th>\n",
       "      <th>sage</th>\n",
       "      <th>explorer</th>\n",
       "      <th>outlaw</th>\n",
       "      <th>magician</th>\n",
       "      <th>hero</th>\n",
       "      <th>lover</th>\n",
       "      <th>jester</th>\n",
       "      <th>everyman</th>\n",
       "      <th>caregiver</th>\n",
       "      <th>...</th>\n",
       "      <th>protective</th>\n",
       "      <th>generous</th>\n",
       "      <th>thrifty</th>\n",
       "      <th>favourable</th>\n",
       "      <th>balanced</th>\n",
       "      <th>sensuality</th>\n",
       "      <th>intelligent</th>\n",
       "      <th>believe</th>\n",
       "      <th>egocentric</th>\n",
       "      <th>allocentric</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>marek_grodzki</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vege_style_life</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oliwka__2007</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z_przestrzeni_serca</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zaradne_warsztaty</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     innocent  sage  explorer  outlaw  magician  hero  lover  \\\n",
       "id                                                                             \n",
       "marek_grodzki             0.0   0.0       0.0     0.0       0.0   0.0    0.0   \n",
       "vege_style_life           0.0   0.0       4.0     0.0       0.0   0.0    0.0   \n",
       "oliwka__2007              0.0   0.0       0.0     0.0       0.0   0.0    0.0   \n",
       "z_przestrzeni_serca       4.0   0.0       0.0     0.0       0.0   0.0    0.0   \n",
       "zaradne_warsztaty         3.0   0.0       0.0     0.0       3.0   0.0    0.0   \n",
       "\n",
       "                     jester  everyman  caregiver  ...  protective  generous  \\\n",
       "id                                                ...                         \n",
       "marek_grodzki           4.0       0.0        0.0  ...         2.0       3.0   \n",
       "vege_style_life         0.0       0.0        0.0  ...         4.0       4.0   \n",
       "oliwka__2007            0.0       4.0        0.0  ...         2.0       2.0   \n",
       "z_przestrzeni_serca     0.0       0.0        0.0  ...         4.0       3.0   \n",
       "zaradne_warsztaty       2.0       3.0        4.0  ...         3.0       4.0   \n",
       "\n",
       "                     thrifty  favourable  balanced  sensuality  intelligent  \\\n",
       "id                                                                            \n",
       "marek_grodzki            4.0         4.0       3.0         4.0          4.0   \n",
       "vege_style_life          4.0         4.0       3.0         3.0          3.0   \n",
       "oliwka__2007             0.0         3.0       1.0         2.0          4.0   \n",
       "z_przestrzeni_serca      0.0         4.0       4.0         3.0          4.0   \n",
       "zaradne_warsztaty        0.0         2.0       2.0         4.0          2.0   \n",
       "\n",
       "                     believe  egocentric  allocentric  \n",
       "id                                                     \n",
       "marek_grodzki            3.0         0.0          0.0  \n",
       "vege_style_life          2.0         0.0          3.0  \n",
       "oliwka__2007             1.0         0.0          3.0  \n",
       "z_przestrzeni_serca      4.0         0.0          1.0  \n",
       "zaradne_warsztaty        3.0         1.0          3.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table preprocessing - replace all NaN with 2 (Unrelated/Don't know class), replace 0-5 values with the ones in range -1.0 - 1.0\n",
    "arch_df = arch_df.fillna(2)\n",
    "\n",
    "# Remove duplicated annotations, to exclude conflicting entries\n",
    "arch_df = arch_df[~arch_df.index.duplicated(keep='first')]\n",
    "\n",
    "# Print the head of the dataset after modification\n",
    "arch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0653277a-27ba-465c-9656-0d8fd315b08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "508it [00:12, 41.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Check if a user has a non-empty directory in the dataset, otherwise delete the user from the list\n",
    "available_arch_df = copy.deepcopy(arch_df)\n",
    "posts = []\n",
    "\n",
    "BASE_DIR = \"instagram_cleared\"\n",
    "\n",
    "# Iterate over whole DataFrame\n",
    "for i, row in tqdm(arch_df.iterrows()):\n",
    "    profile_posts = []\n",
    "    profile_hashtags = []\n",
    "    \n",
    "    # Get all posts per profile\n",
    "    profile_path = os.path.join(BASE_DIR, i)\n",
    "    for file in os.listdir(profile_path):\n",
    "        if not file.endswith(\".toml\"):\n",
    "            with open(os.path.join(profile_path, file), \"r\") as post_f:\n",
    "                read_text = post_f.read()\n",
    "                profile_posts.append(remove_stopwords(clean_up_text(read_text)))\n",
    "                profile_hashtags.append(extract_hashtags(read_text))\n",
    "\n",
    "    # Merge lists - a single list for a single influencer\n",
    "    profile_hashtags = list(itertools.chain.from_iterable(profile_hashtags))\n",
    "    posts.append(list(itertools.chain.from_iterable([profile_posts, [profile_hashtags]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867dd704-2bb4-4772-85f7-28d166405c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map usernames to indices\n",
    "users = list(available_arch_df.index.values)\n",
    "user_indices = {k: users.index(k) for k in users}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7285c65-eecd-4e9d-9848-c67189bf2e69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [04:59<00:00,  8.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get the word count and create a dataframe, where columns are archetypes/traits, and rows are single words\n",
    "# Initialize a word DataFrame\n",
    "word_df = pd.DataFrame()\n",
    "\n",
    "def merge_dicts(dict_a, dict_b) -> dict:\n",
    "    out_dict = dict_a\n",
    "    for k, v in dict_b.items():\n",
    "        if k in out_dict.keys():\n",
    "            out_dict[k] += v\n",
    "        else:\n",
    "            out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "# Iterate over all of the traits/archetypes\n",
    "for trait in tqdm(trait_list):\n",
    "    # Select influencers which have the given archetype annotated\n",
    "    subset_df = arch_df[trait]\n",
    "    subset_indices = [user_indices[idx] for idx in subset_df.index.values]\n",
    "    trait_weights = subset_df.tolist()\n",
    "    \n",
    "    # Get all posts for the list of influencers\n",
    "    f = operator.itemgetter(*subset_indices)\n",
    "    sublist = list(f(posts))\n",
    "    \n",
    "    # Counter to calculate each word occurrences\n",
    "    trait_total = 0\n",
    "    out_dict = {}\n",
    "    for i, post_set in enumerate(sublist):\n",
    "        trait_ctr = Counter(itertools.chain.from_iterable(post_set))\n",
    "        trait_total += sum(trait_ctr.values())\n",
    "        for key in trait_ctr:\n",
    "            trait_ctr[key] *= trait_weights[i]\n",
    "        out_dict = merge_dicts(out_dict, trait_ctr)\n",
    "    out_dict = {k: float(v / trait_total) for k, v in out_dict.items()}\n",
    "    trait_ctr = {trait: out_dict}\n",
    "    \n",
    "    # Append the new dataframe\n",
    "    tmp_df = pd.DataFrame.from_dict(trait_ctr, orient=\"index\")\n",
    "    word_df = word_df.append(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d42093-4046-4822-bcaf-26490d18e749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [12:50<00:00, 20.84s/it]\n"
     ]
    }
   ],
   "source": [
    "# AGDS - discrete approach, classification-like\n",
    "# Try to associate word with trait association class, not the trait itself\n",
    "\n",
    "def merge_dicts(dict_a, dict_b) -> dict:\n",
    "    out_dict = dict_a\n",
    "    for k, v in dict_b.items():\n",
    "        if k in out_dict.keys():\n",
    "            out_dict[k] += v\n",
    "        else:\n",
    "            out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "# Iterate over all of the traits/archetypes\n",
    "word_set = set()\n",
    "trait_df_list = []\n",
    "for trait in tqdm(trait_list):\n",
    "    # Select influencers which have the given archetype annotated\n",
    "    subset_df = arch_df[trait]\n",
    "    trait_subframe = pd.DataFrame()\n",
    "    for trait_class in range(5):\n",
    "        class_df = subset_df.loc[subset_df == trait_class]\n",
    "        subset_indices = [user_indices[idx] for idx in class_df.index.values]\n",
    "\n",
    "        # Get all posts for the list of influencers\n",
    "        f = operator.itemgetter(*subset_indices)\n",
    "        sublist = list(f(posts))\n",
    "\n",
    "        # Counter to calculate each word occurrences\n",
    "        trait_total = 0\n",
    "        out_dict = {}\n",
    "        for i, post_set in enumerate(sublist):\n",
    "            trait_ctr = Counter(itertools.chain.from_iterable(post_set))\n",
    "            trait_total += sum(trait_ctr.values())\n",
    "            out_dict = merge_dicts(out_dict, trait_ctr)\n",
    "        out_dict = {k: float(v / trait_total) for k, v in out_dict.items()}\n",
    "        word_set.update(out_dict.keys())\n",
    "        trait_ctr = {trait_class: out_dict}\n",
    "        trait_tmp_df = pd.DataFrame.from_dict(trait_ctr, orient=\"index\")\n",
    "        trait_subframe = trait_subframe.append(trait_tmp_df)\n",
    "\n",
    "    # Append the new dataframe\n",
    "    #word_df = word_df.append(trait_subframe)\n",
    "    trait_df_list.append(trait_subframe)\n",
    "    \n",
    "softmax_word_df = pd.concat(trait_df_list, keys=trait_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a63b1934-f86e-497e-9f24-c5ab2fe433cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>zainspirowany</th>\n",
       "      <th>wczorajszy</th>\n",
       "      <th>wywiad</th>\n",
       "      <th>odnośnie</th>\n",
       "      <th>relacja</th>\n",
       "      <th>chcieć</th>\n",
       "      <th>przekazać</th>\n",
       "      <th>okres</th>\n",
       "      <th>kwarantanny</th>\n",
       "      <th>świetny</th>\n",
       "      <th>...</th>\n",
       "      <th>#ciemneklimaty</th>\n",
       "      <th>#dziewczynkab</th>\n",
       "      <th>#dziewczynaks</th>\n",
       "      <th>#spacerzleśnymechem_kobiecość</th>\n",
       "      <th>#samamama</th>\n",
       "      <th>#kobietamama</th>\n",
       "      <th>#DZIENMATKI</th>\n",
       "      <th>#tematnadziś</th>\n",
       "      <th>#wieczory</th>\n",
       "      <th>#jellycat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">innocent</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">allocentric</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.002295</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185 rows × 141838 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               zainspirowany  wczorajszy    wywiad  odnośnie   relacja  \\\n",
       "innocent    0       0.000021    0.000085  0.000021  0.000043  0.000298   \n",
       "            1            NaN    0.000122  0.000027  0.000014  0.000595   \n",
       "            2       0.000003    0.000134  0.000073  0.000043  0.000379   \n",
       "            3       0.000006    0.000178  0.000024  0.000036  0.000525   \n",
       "            4            NaN    0.000130  0.000012  0.000059  0.000272   \n",
       "...                      ...         ...       ...       ...       ...   \n",
       "allocentric 0       0.000017    0.000086  0.000069  0.000052  0.000449   \n",
       "            1       0.000005    0.000131  0.000010  0.000039  0.000535   \n",
       "            2            NaN    0.000152  0.000076  0.000028  0.000290   \n",
       "            3       0.000005    0.000165  0.000033  0.000046  0.000474   \n",
       "            4       0.000003    0.000169  0.000039  0.000026  0.000474   \n",
       "\n",
       "                 chcieć  przekazać     okres  kwarantanny   świetny  ...  \\\n",
       "innocent    0  0.002450   0.000064  0.000362     0.000128  0.000682  ...   \n",
       "            1  0.002705   0.000041  0.000149     0.000041  0.000162  ...   \n",
       "            2  0.002418   0.000051  0.000247     0.000035  0.000349  ...   \n",
       "            3  0.002308   0.000040  0.000155     0.000039  0.000304  ...   \n",
       "            4  0.002154   0.000012  0.000189     0.000047  0.000154  ...   \n",
       "...                 ...        ...       ...          ...       ...  ...   \n",
       "allocentric 0  0.002950   0.000104  0.000190     0.000069  0.000397  ...   \n",
       "            1  0.002345   0.000024  0.000092     0.000054  0.000238  ...   \n",
       "            2  0.001982   0.000021  0.000380     0.000014  0.000318  ...   \n",
       "            3  0.002295   0.000022  0.000186     0.000038  0.000342  ...   \n",
       "            4  0.002548   0.000091  0.000179     0.000049  0.000289  ...   \n",
       "\n",
       "               #ciemneklimaty  #dziewczynkab  #dziewczynaks  \\\n",
       "innocent    0             NaN            NaN            NaN   \n",
       "            1             NaN            NaN            NaN   \n",
       "            2             NaN            NaN            NaN   \n",
       "            3             NaN            NaN            NaN   \n",
       "            4        0.000012       0.000012       0.000012   \n",
       "...                       ...            ...            ...   \n",
       "allocentric 0             NaN            NaN            NaN   \n",
       "            1             NaN            NaN            NaN   \n",
       "            2             NaN            NaN            NaN   \n",
       "            3             NaN            NaN            NaN   \n",
       "            4        0.000003       0.000003       0.000003   \n",
       "\n",
       "               #spacerzleśnymechem_kobiecość  #samamama  #kobietamama  \\\n",
       "innocent    0                            NaN        NaN           NaN   \n",
       "            1                            NaN        NaN           NaN   \n",
       "            2                            NaN        NaN           NaN   \n",
       "            3                            NaN        NaN           NaN   \n",
       "            4                       0.000012   0.000012      0.000012   \n",
       "...                                      ...        ...           ...   \n",
       "allocentric 0                            NaN        NaN           NaN   \n",
       "            1                            NaN        NaN           NaN   \n",
       "            2                            NaN        NaN           NaN   \n",
       "            3                            NaN        NaN           NaN   \n",
       "            4                       0.000003   0.000003      0.000003   \n",
       "\n",
       "               #DZIENMATKI  #tematnadziś  #wieczory  #jellycat  \n",
       "innocent    0          NaN           NaN        NaN        NaN  \n",
       "            1          NaN           NaN        NaN        NaN  \n",
       "            2          NaN           NaN        NaN        NaN  \n",
       "            3          NaN           NaN        NaN        NaN  \n",
       "            4     0.000012      0.000012   0.000012   0.000012  \n",
       "...                    ...           ...        ...        ...  \n",
       "allocentric 0          NaN           NaN        NaN        NaN  \n",
       "            1          NaN           NaN        NaN        NaN  \n",
       "            2          NaN           NaN        NaN        NaN  \n",
       "            3          NaN           NaN        NaN        NaN  \n",
       "            4     0.000003      0.000003   0.000003   0.000003  \n",
       "\n",
       "[185 rows x 141838 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the calculation results\n",
    "softmax_word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b8f4a82-7641-47e7-afe7-02f8e6069c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with 0\n",
    "softmax_word_df = softmax_word_df.fillna(0)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"softmax_lemma_influencer_index_map.pickle\", \"wb\") as f:\n",
    "    pickle.dump(user_indices, f)\n",
    "    \n",
    "softmax_word_df.to_pickle(\"softmax_lemma_word_trait_array.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0ff9f49-13a4-4aca-bac4-108fd76f8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with 0\n",
    "word_df = word_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2f6429-eaaf-4c16-ad21-d1a003e3aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to a pickle\n",
    "import pickle\n",
    "\n",
    "with open(\"v5_influencer_index_map.pickle\", \"wb\") as f:\n",
    "    pickle.dump(user_indices, f)\n",
    "    \n",
    "word_df.to_pickle(\"v5_word_trait_array.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeee2626-f7c1-4c09-8a85-16da91584c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word map and save it to a pickle (this file must be loaded in pair with word_trait_array)\n",
    "word_map = word_df.columns.tolist()\n",
    "\n",
    "with open(\"new_word_map.pickle\", \"wb\") as f:\n",
    "    pickle.dump(word_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4dbb5b-0be4-449f-b6f5-88888f1d9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trait_dot_product(post_text: str, word_map: list, word_dataframe: pd.DataFrame) -> list:\n",
    "    # Filter out the text\n",
    "    filtered_post = remove_stopwords(clean_up_text(post_text))\n",
    "    filtered_post += extract_hashtags(post_text)\n",
    "    \n",
    "    # Create a vector for dot product vector\n",
    "    post_vector = [0] * len(word_map)\n",
    "    \n",
    "    # Calculate word occurrences\n",
    "    word_ctr = Counter(filtered_post)\n",
    "    \n",
    "    for word, freq in word_ctr.items():\n",
    "        if word in word_map:\n",
    "            post_vector[word_map.index(word)] = freq\n",
    "    \n",
    "    # Calculate dot product for a given text\n",
    "    word_dot = word_dataframe.dot(post_vector)\n",
    "    \n",
    "    return word_dot\n",
    "\n",
    "# Replace NaN with 0 in word_frequency_table\n",
    "word_df = word_df.fillna(0)\n",
    "\n",
    "# Method for calculating the dot product of trait <-> influencer relation\n",
    "def get_influencer_dot_product(trait_output: list, influencer_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    return influencer_dataframe.dot(trait_output)\n",
    "\n",
    "# Method for calculating the similarity\n",
    "def calculate_similarity(post_text: str, \n",
    "                         word_map: list, \n",
    "                         word_dataframe: pd.DataFrame,\n",
    "                         influencer_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Calculate word-trait dot product\n",
    "    post_result = get_trait_dot_product(post_text, word_map, word_dataframe)\n",
    "    \n",
    "    # Calculate trate-influencer dot-product\n",
    "    inf_dot_product = get_influencer_dot_product(post_result, influencer_dataframe)\n",
    "    \n",
    "    # Get the sum of influencer traits\n",
    "    influencer_sum = influencer_dataframe.sum(axis=1)\n",
    "    \n",
    "    # Divide the dot product by the sum calculated above\n",
    "    inf_dot_product = inf_dot_product.divide(influencer_sum)\n",
    "    \n",
    "    return inf_dot_product\n",
    "\n",
    "# Trait accuracy - round the results\n",
    "def natural_round(x: float) -> int:\n",
    "    out = int(x // 1)\n",
    "    return out + 1 if (x - out) >= 0.5 else out\n",
    "\n",
    "def accuracy_per_trait(input_vector: pd.Series, annotated_vector: pd.Series) -> np.array:\n",
    "    out_array = np.array([0] * 37, dtype=np.int)\n",
    "    for i in range(len(out_array)):\n",
    "        if natural_round(input_vector[i]) == annotated_vector[i]:\n",
    "            out_array[i] = 1\n",
    "    return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8707b6d4-97f8-4354-9b18-0bb0182b7f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average accuracy: 15.48: : 508it [24:26,  2.89s/it]\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(arch_df.iterrows())\n",
    "\n",
    "# Out accuracy vector\n",
    "total_accuracy = np.array([0] * 37, dtype=np.int)\n",
    "\n",
    "for idx, row in pbar:\n",
    "    user_text = list(itertools.chain.from_iterable(posts[users.index(idx)]))\n",
    "    user_text = \" \".join(user_text)\n",
    "    sim_output = get_trait_dot_product(user_text, word_map, word_df)\n",
    "    user_accuracy = accuracy_per_trait(sim_output, row)\n",
    "    total_accuracy += user_accuracy\n",
    "    pbar.set_description(f\"Average accuracy: {round(np.mean(np.divide(total_accuracy, users.index(idx)+1))*100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ff0d3-e883-49ea-905c-63db705d1132",
   "metadata": {},
   "source": [
    "# Softmax version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bd250a4-395f-41b0-aa8e-9d6e9463b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def get_trait_dot_product(post_text: str, word_map: list, word_dataframe: pd.DataFrame) -> list:\n",
    "    # Filter out the text\n",
    "    filtered_post = remove_stopwords(clean_up_text(post_text))\n",
    "    filtered_post += extract_hashtags(post_text)\n",
    "    \n",
    "    # Create a vector for dot product vector\n",
    "    post_vector = [0] * len(word_map)\n",
    "    \n",
    "    # Calculate word occurrences\n",
    "    word_ctr = Counter(filtered_post)\n",
    "    \n",
    "    for word, freq in word_ctr.items():\n",
    "        if word in word_map:\n",
    "            post_vector[word_map.index(word)] = freq\n",
    "    \n",
    "    # Calculate dot product for a given text\n",
    "    word_dot = word_dataframe.dot(post_vector)\n",
    "    \n",
    "    out_vec = pd.Series()\n",
    "    for trait in trait_list:\n",
    "        out_vec = out_vec.append(pd.Series([np.argmax(softmax(word_dot.loc[trait]))], index=[trait]))\n",
    "    \n",
    "    return out_vec\n",
    "\n",
    "# Trait accuracy - round the results\n",
    "def natural_round(x: float) -> int:\n",
    "    out = int(x // 1)\n",
    "    return out + 1 if (x - out) >= 0.5 else out\n",
    "\n",
    "def accuracy_per_trait(input_vector: pd.Series, annotated_vector: pd.Series) -> np.array:\n",
    "    out_array = np.array([0] * 37, dtype=np.int)\n",
    "    for i in range(len(out_array)):\n",
    "        if input_vector[i] == annotated_vector[i]:\n",
    "            out_array[i] = 1\n",
    "    return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51967b33-759b-4a3b-9fb5-a357c9b0dbe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'softmax_word_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4c3e8e748869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0muser_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0muser_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msim_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trait_dot_product\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_word_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0muser_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_per_trait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtotal_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0muser_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'softmax_word_df' is not defined"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(arch_df.iterrows())\n",
    "accuracy = 0\n",
    "\n",
    "# Out accuracy vector\n",
    "total_accuracy = np.array([0] * 37, dtype=np.int)\n",
    "\n",
    "for idx, row in pbar:\n",
    "    user_text = list(itertools.chain.from_iterable(posts[users.index(idx)]))\n",
    "    user_text = \" \".join(user_text)\n",
    "    sim_output = get_trait_dot_product(user_text, word_map, softmax_word_df)\n",
    "    user_accuracy = accuracy_per_trait(sim_output, row)\n",
    "    total_accuracy += user_accuracy\n",
    "    pbar.set_description(f\"Average accuracy: {round(np.mean(np.divide(total_accuracy, users.index(idx)+1))*100, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d7a9fe3-ac17-4317-ac00-73d41eb1807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['innocent', 'sage', 'explorer', 'outlaw', 'magician', 'hero', 'lover', 'jester', 'everyman', 'caregiver', 'ruler', 'creator', 'dominant', 'submissive', 'maximalist', 'minimalist', 'inspiring', 'systematic', 'discovering', 'conservative', 'verifying', 'overlooking', 'sharpening', 'harmonic', 'empathic', 'matter_of_fact', 'brave', 'protective', 'generous', 'thrifty', 'favourable', 'balanced', 'sensuality', 'intelligent', 'believe', 'egocentric', 'allocentric']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "177it [01:25,  2.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test dataset\n",
    "softmax_word_map = softmax_word_df.columns.tolist()\n",
    "\n",
    "# Load the .csv with archetypes\n",
    "arch_df = pd.read_csv('test_archetypes_pl.csv', index_col=0)\n",
    "\n",
    "# Save the order of columns\n",
    "trait_list = arch_df.columns.tolist()\n",
    "\n",
    "# Show the table header and column list\n",
    "print(trait_list)\n",
    "arch_df.head()\n",
    "\n",
    "# Table preprocessing - replace all NaN with 2 (Unrelated/Don't know class), replace 0-5 values with the ones in range -1.0 - 1.0\n",
    "arch_df = arch_df.fillna(2)\n",
    "\n",
    "# Remove duplicated annotations, to exclude conflicting entries\n",
    "arch_df = arch_df[~arch_df.index.duplicated(keep='first')]\n",
    "\n",
    "# Print the head of the dataset after modification\n",
    "arch_df.head()\n",
    "\n",
    "# Check if a user has a non-empty directory in the dataset, otherwise delete the user from the list\n",
    "available_arch_df = copy.deepcopy(arch_df)\n",
    "posts = []\n",
    "\n",
    "BASE_DIR = \"instagram_cleared\"\n",
    "\n",
    "# Iterate over whole DataFrame\n",
    "for i, row in tqdm(arch_df.iterrows()):\n",
    "    profile_posts = []\n",
    "    profile_hashtags = []\n",
    "    \n",
    "    # Get all posts per profile\n",
    "    profile_path = os.path.join(BASE_DIR, i)\n",
    "    for file in os.listdir(profile_path):\n",
    "        if not file.endswith(\".toml\"):\n",
    "            with open(os.path.join(profile_path, file), \"r\") as post_f:\n",
    "                read_text = post_f.read()\n",
    "                profile_posts.append(remove_stopwords(clean_up_text(read_text)))\n",
    "                profile_hashtags.append(extract_hashtags(read_text))\n",
    "\n",
    "    # Merge lists - a single list for a single influencer\n",
    "    profile_hashtags = list(itertools.chain.from_iterable(profile_hashtags))\n",
    "    posts.append(list(itertools.chain.from_iterable([profile_posts, [profile_hashtags]])))\n",
    "    \n",
    "# Map usernames to indices\n",
    "users = list(arch_df.index.values)\n",
    "user_indices = {k: users.index(k) for k in users}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69ea18f0-e418-4f90-81ba-b18c600f225e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-20-4ffef00e153a>:21: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  out_vec = pd.Series()\n",
      "Average test dataset accuracy: 21.62: : 4it [00:14,  3.51s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3852b9e82fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mread_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0muser_text\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mread_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msim_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trait_dot_product\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_word_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_word_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0muser_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_per_trait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtest_total_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0muser_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-4ffef00e153a>\u001b[0m in \u001b[0;36mget_trait_dot_product\u001b[0;34m(post_text, word_map, word_dataframe)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_ctr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mpost_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Calculate dot product for a given text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pbar = tqdm(arch_df.iterrows())\n",
    "\n",
    "# Out accuracy vector\n",
    "test_total_accuracy = np.array([0] * 37, dtype=np.int)\n",
    "\n",
    "for idx, row in pbar:\n",
    "    profile_path = os.path.join(BASE_DIR, idx)\n",
    "    user_text = \"\"\n",
    "    for file in os.listdir(profile_path):\n",
    "        if not file.endswith(\".toml\"):\n",
    "            with open(os.path.join(profile_path, file), \"r\") as post_f:\n",
    "                read_text = post_f.read()\n",
    "                user_text += read_text\n",
    "    sim_output = get_trait_dot_product(user_text, softmax_word_map, softmax_word_df)\n",
    "    user_accuracy = accuracy_per_trait(sim_output, row)\n",
    "    test_total_accuracy += user_accuracy\n",
    "    pbar.set_description(f\"Average test dataset accuracy: {round(np.mean(np.divide(test_total_accuracy, users.index(idx)+1))*100, 2)}\")\n",
    "    \n",
    "# Show total accuracy\n",
    "scaled_test_accuracy = np.divide(test_total_accuracy, len(arch_df))\n",
    "avg_test_accuracy = np.mean(scaled_test_accuracy)\n",
    "\n",
    "print(\"--- ACCURACY ON TESTING DATASET ---\")\n",
    "\n",
    "print(f\"Average test dataset accuracy: {round(avg_test_accuracy*100, 2)}%\")\n",
    "print(\"Accuracy per trait:\")\n",
    "for i in range(len(trait_list)):\n",
    "    print(f\"{trait_list[i]}: {round(scaled_test_accuracy[i] * 100, 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
