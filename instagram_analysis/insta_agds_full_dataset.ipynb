{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03778673-2505-4ab7-b3e7-76f903172a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate words with archetypes/character traits as intermediate layer\n",
    "# and with influencer as the \"last\" layer\n",
    "\n",
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "import toml\n",
    "import re\n",
    "import itertools\n",
    "from text_cleaner import *\n",
    "import operator\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "def extract_hashtags(post_text):\n",
    "    HASH_RE = re.compile(r\"\\#\\w+\")\n",
    "    out_list = re.findall(HASH_RE, post_text)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798a8899-ef4c-46e3-9046-ac5aa6a7dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['innocent', 'sage', 'explorer', 'outlaw', 'magician', 'hero', 'lover', 'jester', 'everyman', 'caregiver', 'ruler', 'creator', 'dominant', 'submissive', 'maximalist', 'minimalist', 'inspiring', 'systematic', 'discovering', 'conservative', 'verifying', 'overlooking', 'sharpening', 'harmonic', 'empathic', 'matter_of_fact', 'brave', 'protective', 'generous', 'thrifty', 'favourable', 'balanced', 'sensuality', 'intelligent', 'believe', 'egocentric', 'allocentric']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>innocent</th>\n",
       "      <th>sage</th>\n",
       "      <th>explorer</th>\n",
       "      <th>outlaw</th>\n",
       "      <th>magician</th>\n",
       "      <th>hero</th>\n",
       "      <th>lover</th>\n",
       "      <th>jester</th>\n",
       "      <th>everyman</th>\n",
       "      <th>caregiver</th>\n",
       "      <th>...</th>\n",
       "      <th>protective</th>\n",
       "      <th>generous</th>\n",
       "      <th>thrifty</th>\n",
       "      <th>favourable</th>\n",
       "      <th>balanced</th>\n",
       "      <th>sensuality</th>\n",
       "      <th>intelligent</th>\n",
       "      <th>believe</th>\n",
       "      <th>egocentric</th>\n",
       "      <th>allocentric</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>marek_grodzki</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vege_style_life</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oliwka__2007</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z_przestrzeni_serca</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zaradne_warsztaty</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     innocent  sage  explorer  outlaw  magician  hero  lover  \\\n",
       "id                                                                             \n",
       "marek_grodzki             0.0   0.0       0.0     0.0       0.0   0.0    0.0   \n",
       "vege_style_life           0.0   0.0       4.0     0.0       0.0   0.0    0.0   \n",
       "oliwka__2007              0.0   0.0       0.0     0.0       0.0   0.0    0.0   \n",
       "z_przestrzeni_serca       4.0   0.0       0.0     0.0       0.0   0.0    0.0   \n",
       "zaradne_warsztaty         3.0   0.0       0.0     0.0       3.0   0.0    0.0   \n",
       "\n",
       "                     jester  everyman  caregiver  ...  protective  generous  \\\n",
       "id                                                ...                         \n",
       "marek_grodzki           4.0       0.0        0.0  ...         2.0       3.0   \n",
       "vege_style_life         0.0       0.0        0.0  ...         4.0       4.0   \n",
       "oliwka__2007            0.0       4.0        0.0  ...         2.0       2.0   \n",
       "z_przestrzeni_serca     0.0       0.0        0.0  ...         4.0       3.0   \n",
       "zaradne_warsztaty       2.0       3.0        4.0  ...         3.0       4.0   \n",
       "\n",
       "                     thrifty  favourable  balanced  sensuality  intelligent  \\\n",
       "id                                                                            \n",
       "marek_grodzki            4.0         4.0       3.0         4.0          4.0   \n",
       "vege_style_life          4.0         4.0       3.0         3.0          3.0   \n",
       "oliwka__2007             0.0         3.0       1.0         2.0          4.0   \n",
       "z_przestrzeni_serca      0.0         4.0       4.0         3.0          4.0   \n",
       "zaradne_warsztaty        0.0         2.0       2.0         4.0          2.0   \n",
       "\n",
       "                     believe  egocentric  allocentric  \n",
       "id                                                     \n",
       "marek_grodzki            3.0         0.0          0.0  \n",
       "vege_style_life          2.0         0.0          3.0  \n",
       "oliwka__2007             1.0         0.0          3.0  \n",
       "z_przestrzeni_serca      4.0         0.0          1.0  \n",
       "zaradne_warsztaty        3.0         1.0          3.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the .csv with archetypes\n",
    "arch_df = pd.read_csv('archetypes_pl_new.csv', index_col=0)\n",
    "\n",
    "# Save the order of columns\n",
    "trait_list = arch_df.columns.tolist()\n",
    "\n",
    "# Show the table header and column list\n",
    "print(trait_list)\n",
    "arch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1f03a8-fd7c-4348-935f-a1f57e90af6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>innocent</th>\n",
       "      <th>sage</th>\n",
       "      <th>explorer</th>\n",
       "      <th>outlaw</th>\n",
       "      <th>magician</th>\n",
       "      <th>hero</th>\n",
       "      <th>lover</th>\n",
       "      <th>jester</th>\n",
       "      <th>everyman</th>\n",
       "      <th>caregiver</th>\n",
       "      <th>...</th>\n",
       "      <th>protective</th>\n",
       "      <th>generous</th>\n",
       "      <th>thrifty</th>\n",
       "      <th>favourable</th>\n",
       "      <th>balanced</th>\n",
       "      <th>sensuality</th>\n",
       "      <th>intelligent</th>\n",
       "      <th>believe</th>\n",
       "      <th>egocentric</th>\n",
       "      <th>allocentric</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>marek_grodzki</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vege_style_life</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oliwka__2007</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z_przestrzeni_serca</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zaradne_warsztaty</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     innocent  sage  explorer  outlaw  magician  hero  lover  \\\n",
       "id                                                                             \n",
       "marek_grodzki             0.0   0.0       0.0     0.0       0.0   0.0    0.0   \n",
       "vege_style_life           0.0   0.0       4.0     0.0       0.0   0.0    0.0   \n",
       "oliwka__2007              0.0   0.0       0.0     0.0       0.0   0.0    0.0   \n",
       "z_przestrzeni_serca       4.0   0.0       0.0     0.0       0.0   0.0    0.0   \n",
       "zaradne_warsztaty         3.0   0.0       0.0     0.0       3.0   0.0    0.0   \n",
       "\n",
       "                     jester  everyman  caregiver  ...  protective  generous  \\\n",
       "id                                                ...                         \n",
       "marek_grodzki           4.0       0.0        0.0  ...         2.0       3.0   \n",
       "vege_style_life         0.0       0.0        0.0  ...         4.0       4.0   \n",
       "oliwka__2007            0.0       4.0        0.0  ...         2.0       2.0   \n",
       "z_przestrzeni_serca     0.0       0.0        0.0  ...         4.0       3.0   \n",
       "zaradne_warsztaty       2.0       3.0        4.0  ...         3.0       4.0   \n",
       "\n",
       "                     thrifty  favourable  balanced  sensuality  intelligent  \\\n",
       "id                                                                            \n",
       "marek_grodzki            4.0         4.0       3.0         4.0          4.0   \n",
       "vege_style_life          4.0         4.0       3.0         3.0          3.0   \n",
       "oliwka__2007             0.0         3.0       1.0         2.0          4.0   \n",
       "z_przestrzeni_serca      0.0         4.0       4.0         3.0          4.0   \n",
       "zaradne_warsztaty        0.0         2.0       2.0         4.0          2.0   \n",
       "\n",
       "                     believe  egocentric  allocentric  \n",
       "id                                                     \n",
       "marek_grodzki            3.0         0.0          0.0  \n",
       "vege_style_life          2.0         0.0          3.0  \n",
       "oliwka__2007             1.0         0.0          3.0  \n",
       "z_przestrzeni_serca      4.0         0.0          1.0  \n",
       "zaradne_warsztaty        3.0         1.0          3.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table preprocessing - replace all NaN with 2 (Unrelated/Don't know class), replace 0-5 values with the ones in range -1.0 - 1.0\n",
    "arch_df = arch_df.fillna(2)\n",
    "\n",
    "# Remove duplicated annotations, to exclude conflicting entries\n",
    "arch_df = arch_df[~arch_df.index.duplicated(keep='first')]\n",
    "\n",
    "# Print the head of the dataset after modification\n",
    "arch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0653277a-27ba-465c-9656-0d8fd315b08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "685it [00:20, 33.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Check if a user has a non-empty directory in the dataset, otherwise delete the user from the list\n",
    "available_arch_df = copy.deepcopy(arch_df)\n",
    "posts = []\n",
    "\n",
    "BASE_DIR = \"instagram_cleared\"\n",
    "\n",
    "# Iterate over whole DataFrame\n",
    "for i, row in tqdm(arch_df.iterrows()):\n",
    "    profile_posts = []\n",
    "    profile_hashtags = []\n",
    "    \n",
    "    # Get all posts per profile\n",
    "    profile_path = os.path.join(BASE_DIR, i)\n",
    "    for file in os.listdir(profile_path):\n",
    "        if not file.endswith(\".toml\"):\n",
    "            with open(os.path.join(profile_path, file), \"r\") as post_f:\n",
    "                read_text = post_f.read()\n",
    "                profile_posts.append(remove_stopwords(clean_up_text(read_text)))\n",
    "                profile_hashtags.append(extract_hashtags(read_text))\n",
    "\n",
    "    # Merge lists - a single list for a single influencer\n",
    "    profile_hashtags = list(itertools.chain.from_iterable(profile_hashtags))\n",
    "    posts.append(list(itertools.chain.from_iterable([profile_posts, [profile_hashtags]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "867dd704-2bb4-4772-85f7-28d166405c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map usernames to indices\n",
    "users = list(available_arch_df.index.values)\n",
    "user_indices = {k: users.index(k) for k in users}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7285c65-eecd-4e9d-9848-c67189bf2e69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [05:40<00:00,  9.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get the word count and create a dataframe, where columns are archetypes/traits, and rows are single words\n",
    "# Initialize a word DataFrame\n",
    "word_df = pd.DataFrame()\n",
    "\n",
    "def merge_dicts(dict_a, dict_b) -> dict:\n",
    "    out_dict = dict_a\n",
    "    for k, v in dict_b.items():\n",
    "        if k in out_dict.keys():\n",
    "            out_dict[k] += v\n",
    "        else:\n",
    "            out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "# Iterate over all of the traits/archetypes\n",
    "for trait in tqdm(trait_list):\n",
    "    # Select influencers which have the given archetype annotated\n",
    "    subset_df = arch_df[trait]\n",
    "    subset_indices = [user_indices[idx] for idx in subset_df.index.values]\n",
    "    trait_weights = subset_df.tolist()\n",
    "    \n",
    "    # Get all posts for the list of influencers\n",
    "    f = operator.itemgetter(*subset_indices)\n",
    "    sublist = list(f(posts))\n",
    "    \n",
    "    # Counter to calculate each word occurrences\n",
    "    trait_total = 0\n",
    "    out_dict = {}\n",
    "    for i, post_set in enumerate(sublist):\n",
    "        trait_ctr = Counter(itertools.chain.from_iterable(post_set))\n",
    "        trait_total += sum(trait_ctr.values())\n",
    "        for key in trait_ctr:\n",
    "            trait_ctr[key] *= trait_weights[i]\n",
    "        out_dict = merge_dicts(out_dict, trait_ctr)\n",
    "    out_dict = {k: float(v / trait_total) for k, v in out_dict.items()}\n",
    "    trait_ctr = {trait: out_dict}\n",
    "    \n",
    "    # Append the new dataframe\n",
    "    tmp_df = pd.DataFrame.from_dict(trait_ctr, orient=\"index\")\n",
    "    word_df = word_df.append(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9d42093-4046-4822-bcaf-26490d18e749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [20:25<00:00, 33.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# AGDS - discrete approach, classification-like\n",
    "# Try to associate word with trait association class, not the trait itself\n",
    "\n",
    "def merge_dicts(dict_a, dict_b) -> dict:\n",
    "    out_dict = dict_a\n",
    "    for k, v in dict_b.items():\n",
    "        if k in out_dict.keys():\n",
    "            out_dict[k] += v\n",
    "        else:\n",
    "            out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "# Iterate over all of the traits/archetypes\n",
    "word_set = set()\n",
    "trait_df_list = []\n",
    "for trait in tqdm(trait_list):\n",
    "    # Select influencers which have the given archetype annotated\n",
    "    subset_df = arch_df[trait]\n",
    "    trait_subframe = pd.DataFrame()\n",
    "    for trait_class in range(5):\n",
    "        class_df = subset_df.loc[subset_df == trait_class]\n",
    "        subset_indices = [user_indices[idx] for idx in class_df.index.values]\n",
    "\n",
    "        # Get all posts for the list of influencers\n",
    "        f = operator.itemgetter(*subset_indices)\n",
    "        sublist = list(f(posts))\n",
    "\n",
    "        # Counter to calculate each word occurrences\n",
    "        trait_total = 0\n",
    "        out_dict = {}\n",
    "        for i, post_set in enumerate(sublist):\n",
    "            trait_ctr = Counter(itertools.chain.from_iterable(post_set))\n",
    "            trait_total += sum(trait_ctr.values())\n",
    "            out_dict = merge_dicts(out_dict, trait_ctr)\n",
    "        out_dict = {k: float(v / trait_total) for k, v in out_dict.items()}\n",
    "        word_set.update(out_dict.keys())\n",
    "        trait_ctr = {trait_class: out_dict}\n",
    "        trait_tmp_df = pd.DataFrame.from_dict(trait_ctr, orient=\"index\")\n",
    "        trait_subframe = trait_subframe.append(trait_tmp_df)\n",
    "\n",
    "    # Append the new dataframe\n",
    "    #word_df = word_df.append(trait_subframe)\n",
    "    trait_df_list.append(trait_subframe)\n",
    "    \n",
    "softmax_word_df = pd.concat(trait_df_list, keys=trait_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a63b1934-f86e-497e-9f24-c5ab2fe433cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>zainspirowany</th>\n",
       "      <th>wczorajszym</th>\n",
       "      <th>wywiadem</th>\n",
       "      <th>odnośnie</th>\n",
       "      <th>relacji</th>\n",
       "      <th>chciałem</th>\n",
       "      <th>przekazać</th>\n",
       "      <th>okres</th>\n",
       "      <th>kwarantanny</th>\n",
       "      <th>świetny</th>\n",
       "      <th>...</th>\n",
       "      <th>#piekewdomu</th>\n",
       "      <th>#dish</th>\n",
       "      <th>#kremzcukinii</th>\n",
       "      <th>#zucchina</th>\n",
       "      <th>#pranzoitaliano</th>\n",
       "      <th>#tradycyjnejedzenie</th>\n",
       "      <th>#tatar</th>\n",
       "      <th>#meatlover</th>\n",
       "      <th>#zachcianki</th>\n",
       "      <th>#pierogizkapusta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">innocent</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>2.125173e-05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>9.158188e-07</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">allocentric</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>3.259984e-05</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185 rows × 225423 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               zainspirowany  wczorajszym      wywiadem  odnośnie   relacji  \\\n",
       "innocent    0       0.000021     0.000043  2.125173e-05  0.000043  0.000149   \n",
       "            1            NaN     0.000013           NaN  0.000013  0.000243   \n",
       "            2       0.000002     0.000019           NaN  0.000045  0.000112   \n",
       "            3       0.000006     0.000034  9.158188e-07  0.000037  0.000173   \n",
       "            4            NaN     0.000016           NaN  0.000055  0.000079   \n",
       "...                      ...          ...           ...       ...       ...   \n",
       "allocentric 0       0.000016     0.000033  3.259984e-05  0.000049  0.000228   \n",
       "            1       0.000009     0.000033           NaN  0.000033  0.000164   \n",
       "            2            NaN     0.000021           NaN  0.000036  0.000088   \n",
       "            3       0.000005     0.000027           NaN  0.000046  0.000145   \n",
       "            4       0.000003     0.000032           NaN  0.000026  0.000194   \n",
       "\n",
       "               chciałem  przekazać     okres  kwarantanny   świetny  ...  \\\n",
       "innocent    0  0.000106   0.000064  0.000149     0.000149  0.000128  ...   \n",
       "            1  0.000013   0.000013  0.000040     0.000040  0.000013  ...   \n",
       "            2  0.000074   0.000045  0.000100     0.000045  0.000095  ...   \n",
       "            3  0.000026   0.000030  0.000132     0.000039  0.000071  ...   \n",
       "            4  0.000008   0.000040  0.000079     0.000047  0.000040  ...   \n",
       "...                 ...        ...       ...          ...       ...  ...   \n",
       "allocentric 0  0.000081   0.000081  0.000081     0.000065  0.000081  ...   \n",
       "            1  0.000021   0.000021  0.000270     0.000048  0.000061  ...   \n",
       "            2  0.000047   0.000031  0.000109     0.000026  0.000109  ...   \n",
       "            3  0.000046   0.000028  0.000085     0.000042  0.000076  ...   \n",
       "            4  0.000016   0.000062  0.000055     0.000055  0.000058  ...   \n",
       "\n",
       "               #piekewdomu     #dish  #kremzcukinii  #zucchina  \\\n",
       "innocent    0          NaN       NaN            NaN        NaN   \n",
       "            1          NaN       NaN            NaN        NaN   \n",
       "            2          NaN       NaN            NaN        NaN   \n",
       "            3          NaN       NaN            NaN        NaN   \n",
       "            4     0.000040  0.000016       0.000008   0.000016   \n",
       "...                    ...       ...            ...        ...   \n",
       "allocentric 0          NaN       NaN            NaN        NaN   \n",
       "            1          NaN       NaN            NaN        NaN   \n",
       "            2          NaN       NaN            NaN        NaN   \n",
       "            3     0.000006  0.000002       0.000001   0.000002   \n",
       "            4          NaN       NaN            NaN        NaN   \n",
       "\n",
       "               #pranzoitaliano  #tradycyjnejedzenie    #tatar  #meatlover  \\\n",
       "innocent    0              NaN                  NaN       NaN         NaN   \n",
       "            1              NaN                  NaN       NaN         NaN   \n",
       "            2              NaN                  NaN       NaN         NaN   \n",
       "            3              NaN                  NaN       NaN         NaN   \n",
       "            4         0.000008             0.000008  0.000008    0.000008   \n",
       "...                        ...                  ...       ...         ...   \n",
       "allocentric 0              NaN                  NaN       NaN         NaN   \n",
       "            1              NaN                  NaN       NaN         NaN   \n",
       "            2              NaN                  NaN       NaN         NaN   \n",
       "            3         0.000001             0.000001  0.000001    0.000001   \n",
       "            4              NaN                  NaN       NaN         NaN   \n",
       "\n",
       "               #zachcianki  #pierogizkapusta  \n",
       "innocent    0          NaN               NaN  \n",
       "            1          NaN               NaN  \n",
       "            2          NaN               NaN  \n",
       "            3          NaN               NaN  \n",
       "            4     0.000008          0.000008  \n",
       "...                    ...               ...  \n",
       "allocentric 0          NaN               NaN  \n",
       "            1          NaN               NaN  \n",
       "            2          NaN               NaN  \n",
       "            3     0.000001          0.000001  \n",
       "            4          NaN               NaN  \n",
       "\n",
       "[185 rows x 225423 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the calculation results\n",
    "softmax_word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b8f4a82-7641-47e7-afe7-02f8e6069c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with 0\n",
    "softmax_word_df = softmax_word_df.fillna(0)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"softmax_full_influencer_index_map.pickle\", \"wb\") as f:\n",
    "    pickle.dump(user_indices, f)\n",
    "    \n",
    "softmax_word_df.to_pickle(\"softmax_full_word_trait_array.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0ff9f49-13a4-4aca-bac4-108fd76f8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with 0\n",
    "word_df = word_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d2f6429-eaaf-4c16-ad21-d1a003e3aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to a pickle\n",
    "import pickle\n",
    "\n",
    "with open(\"full_influencer_index_map.pickle\", \"wb\") as f:\n",
    "    pickle.dump(user_indices, f)\n",
    "    \n",
    "word_df.to_pickle(\"full_word_trait_array.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4dbb5b-0be4-449f-b6f5-88888f1d9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trait_dot_product(post_text: str, word_map: list, word_dataframe: pd.DataFrame) -> list:\n",
    "    # Filter out the text\n",
    "    filtered_post = remove_stopwords(clean_up_text(post_text))\n",
    "    filtered_post += extract_hashtags(post_text)\n",
    "    \n",
    "    # Create a vector for dot product vector\n",
    "    post_vector = [0] * len(word_map)\n",
    "    \n",
    "    # Calculate word occurrences\n",
    "    word_ctr = Counter(filtered_post)\n",
    "    \n",
    "    for word, freq in word_ctr.items():\n",
    "        if word in word_map:\n",
    "            post_vector[word_map.index(word)] = freq\n",
    "    \n",
    "    # Calculate dot product for a given text\n",
    "    word_dot = word_dataframe.dot(post_vector)\n",
    "    \n",
    "    return word_dot\n",
    "\n",
    "# Replace NaN with 0 in word_frequency_table\n",
    "word_df = word_df.fillna(0)\n",
    "\n",
    "# Method for calculating the dot product of trait <-> influencer relation\n",
    "def get_influencer_dot_product(trait_output: list, influencer_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    return influencer_dataframe.dot(trait_output)\n",
    "\n",
    "# Method for calculating the similarity\n",
    "def calculate_similarity(post_text: str, \n",
    "                         word_map: list, \n",
    "                         word_dataframe: pd.DataFrame,\n",
    "                         influencer_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Calculate word-trait dot product\n",
    "    post_result = get_trait_dot_product(post_text, word_map, word_dataframe)\n",
    "    \n",
    "    # Calculate trate-influencer dot-product\n",
    "    inf_dot_product = get_influencer_dot_product(post_result, influencer_dataframe)\n",
    "    \n",
    "    # Get the sum of influencer traits\n",
    "    influencer_sum = influencer_dataframe.sum(axis=1)\n",
    "    \n",
    "    # Divide the dot product by the sum calculated above\n",
    "    inf_dot_product = inf_dot_product.divide(influencer_sum)\n",
    "    \n",
    "    return inf_dot_product\n",
    "\n",
    "# Trait accuracy - round the results\n",
    "def natural_round(x: float) -> int:\n",
    "    out = int(x // 1)\n",
    "    return out + 1 if (x - out) >= 0.5 else out\n",
    "\n",
    "def accuracy_per_trait(input_vector: pd.Series, annotated_vector: pd.Series) -> np.array:\n",
    "    out_array = np.array([0] * 37, dtype=np.int)\n",
    "    for i in range(len(out_array)):\n",
    "        if natural_round(input_vector[i]) == annotated_vector[i]:\n",
    "            out_array[i] = 1\n",
    "    return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c890d446-c902-40c2-9cbb-2f7b4d422c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word map for both structures\n",
    "word_map = word_df.columns.tolist()\n",
    "softmax_word_map = softmax_word_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8707b6d4-97f8-4354-9b18-0bb0182b7f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average accuracy: 15.94: : 685it [40:51,  3.58s/it]\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(arch_df.iterrows())\n",
    "\n",
    "# Out accuracy vector\n",
    "total_accuracy = np.array([0] * 37, dtype=np.int)\n",
    "\n",
    "for idx, row in pbar:\n",
    "    user_text = list(itertools.chain.from_iterable(posts[users.index(idx)]))\n",
    "    user_text = \" \".join(user_text)\n",
    "    sim_output = get_trait_dot_product(user_text, word_map, word_df)\n",
    "    user_accuracy = accuracy_per_trait(sim_output, row)\n",
    "    total_accuracy += user_accuracy\n",
    "    pbar.set_description(f\"Average accuracy: {round(np.mean(np.divide(total_accuracy, users.index(idx)+1))*100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ff0d3-e883-49ea-905c-63db705d1132",
   "metadata": {},
   "source": [
    "# Softmax version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bd250a4-395f-41b0-aa8e-9d6e9463b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def get_trait_dot_product(post_text: str, word_map: list, word_dataframe: pd.DataFrame) -> list:\n",
    "    # Filter out the text\n",
    "    filtered_post = remove_stopwords(clean_up_text(post_text))\n",
    "    filtered_post += extract_hashtags(post_text)\n",
    "    \n",
    "    # Create a vector for dot product vector\n",
    "    post_vector = [0] * len(word_map)\n",
    "    \n",
    "    # Calculate word occurrences\n",
    "    word_ctr = Counter(filtered_post)\n",
    "    \n",
    "    for word, freq in word_ctr.items():\n",
    "        if word in word_map:\n",
    "            post_vector[word_map.index(word)] = freq\n",
    "    \n",
    "    # Calculate dot product for a given text\n",
    "    word_dot = word_dataframe.dot(post_vector)\n",
    "    \n",
    "    out_vec = pd.Series()\n",
    "    for trait in trait_list:\n",
    "        out_vec = out_vec.append(pd.Series([np.argmax(softmax(word_dot.loc[trait]))], index=[trait]))\n",
    "    \n",
    "    return out_vec\n",
    "\n",
    "# Trait accuracy - round the results\n",
    "def natural_round(x: float) -> int:\n",
    "    out = int(x // 1)\n",
    "    return out + 1 if (x - out) >= 0.5 else out\n",
    "\n",
    "def accuracy_per_trait(input_vector: pd.Series, annotated_vector: pd.Series) -> np.array:\n",
    "    out_array = np.array([0] * 37, dtype=np.int)\n",
    "    for i in range(len(out_array)):\n",
    "        if input_vector[i] == annotated_vector[i]:\n",
    "            out_array[i] = 1\n",
    "    return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51967b33-759b-4a3b-9fb5-a357c9b0dbe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-17-4ffef00e153a>:21: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  out_vec = pd.Series()\n",
      "Average accuracy: 44.16: : 685it [42:29,  3.72s/it]\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(arch_df.iterrows())\n",
    "accuracy = 0\n",
    "\n",
    "# Out accuracy vector\n",
    "total_accuracy = np.array([0] * 37, dtype=np.int)\n",
    "\n",
    "for idx, row in pbar:\n",
    "    user_text = list(itertools.chain.from_iterable(posts[users.index(idx)]))\n",
    "    user_text = \" \".join(user_text)\n",
    "    sim_output = get_trait_dot_product(user_text, softmax_word_map, softmax_word_df)\n",
    "    user_accuracy = accuracy_per_trait(sim_output, row)\n",
    "    total_accuracy += user_accuracy\n",
    "    pbar.set_description(f\"Average accuracy: {round(np.mean(np.divide(total_accuracy, users.index(idx)+1))*100, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d7a9fe3-ac17-4317-ac00-73d41eb1807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 33.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['innocent', 'sage', 'explorer', 'outlaw', 'magician', 'hero', 'lover', 'jester', 'everyman', 'caregiver', 'ruler', 'creator', 'dominant', 'submissive', 'maximalist', 'minimalist', 'inspiring', 'systematic', 'discovering', 'conservative', 'verifying', 'overlooking', 'sharpening', 'harmonic', 'empathic', 'matter_of_fact', 'brave', 'protective', 'generous', 'thrifty', 'favourable', 'balanced', 'sensuality', 'intelligent', 'believe', 'egocentric', 'allocentric']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "177it [00:04, 40.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test dataset\n",
    "\n",
    "# Load the .csv with archetypes\n",
    "arch_df = pd.read_csv('test_archetypes_pl.csv', index_col=0)\n",
    "\n",
    "# Save the order of columns\n",
    "trait_list = arch_df.columns.tolist()\n",
    "\n",
    "# Show the table header and column list\n",
    "print(trait_list)\n",
    "arch_df.head()\n",
    "\n",
    "# Table preprocessing - replace all NaN with 2 (Unrelated/Don't know class), replace 0-5 values with the ones in range -1.0 - 1.0\n",
    "arch_df = arch_df.fillna(2)\n",
    "\n",
    "# Remove duplicated annotations, to exclude conflicting entries\n",
    "arch_df = arch_df[~arch_df.index.duplicated(keep='first')]\n",
    "\n",
    "# Print the head of the dataset after modification\n",
    "arch_df.head()\n",
    "\n",
    "# Check if a user has a non-empty directory in the dataset, otherwise delete the user from the list\n",
    "available_arch_df = copy.deepcopy(arch_df)\n",
    "posts = []\n",
    "\n",
    "BASE_DIR = \"instagram_cleared\"\n",
    "\n",
    "# Iterate over whole DataFrame\n",
    "for i, row in tqdm(arch_df.iterrows()):\n",
    "    profile_posts = []\n",
    "    profile_hashtags = []\n",
    "    \n",
    "    # Get all posts per profile\n",
    "    profile_path = os.path.join(BASE_DIR, i)\n",
    "    for file in os.listdir(profile_path):\n",
    "        if not file.endswith(\".toml\"):\n",
    "            with open(os.path.join(profile_path, file), \"r\") as post_f:\n",
    "                read_text = post_f.read()\n",
    "                profile_posts.append(remove_stopwords(clean_up_text(read_text)))\n",
    "                profile_hashtags.append(extract_hashtags(read_text))\n",
    "\n",
    "    # Merge lists - a single list for a single influencer\n",
    "    profile_hashtags = list(itertools.chain.from_iterable(profile_hashtags))\n",
    "    posts.append(list(itertools.chain.from_iterable([profile_posts, [profile_hashtags]])))\n",
    "    \n",
    "# Map usernames to indices\n",
    "users = list(arch_df.index.values)\n",
    "user_indices = {k: users.index(k) for k in users}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69ea18f0-e418-4f90-81ba-b18c600f225e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]<ipython-input-17-4ffef00e153a>:21: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  out_vec = pd.Series()\n",
      "Average test dataset accuracy: 37.64: : 177it [15:25,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ACCURACY ON TESTING DATASET ---\n",
      "Average test dataset accuracy: 37.64%\n",
      "Accuracy per trait:\n",
      "innocent: 33.9%\n",
      "sage: 29.94%\n",
      "explorer: 30.51%\n",
      "outlaw: 37.29%\n",
      "magician: 36.72%\n",
      "hero: 55.37%\n",
      "lover: 45.76%\n",
      "jester: 53.67%\n",
      "everyman: 24.86%\n",
      "caregiver: 36.16%\n",
      "ruler: 48.02%\n",
      "creator: 29.38%\n",
      "dominant: 22.03%\n",
      "submissive: 33.9%\n",
      "maximalist: 19.77%\n",
      "minimalist: 18.64%\n",
      "inspiring: 31.07%\n",
      "systematic: 33.9%\n",
      "discovering: 48.02%\n",
      "conservative: 42.94%\n",
      "verifying: 19.21%\n",
      "overlooking: 11.86%\n",
      "sharpening: 54.24%\n",
      "harmonic: 36.16%\n",
      "empathic: 41.24%\n",
      "matter_of_fact: 43.5%\n",
      "brave: 64.97%\n",
      "protective: 43.5%\n",
      "generous: 25.42%\n",
      "thrifty: 54.24%\n",
      "favourable: 59.89%\n",
      "balanced: 28.81%\n",
      "sensuality: 42.94%\n",
      "intelligent: 18.64%\n",
      "believe: 49.72%\n",
      "egocentric: 51.41%\n",
      "allocentric: 35.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(arch_df.iterrows())\n",
    "\n",
    "# Out accuracy vector\n",
    "test_total_accuracy = np.array([0] * 37, dtype=np.int)\n",
    "\n",
    "for idx, row in pbar:\n",
    "    profile_path = os.path.join(BASE_DIR, idx)\n",
    "    user_text = \"\"\n",
    "    for file in os.listdir(profile_path):\n",
    "        if not file.endswith(\".toml\"):\n",
    "            with open(os.path.join(profile_path, file), \"r\") as post_f:\n",
    "                read_text = post_f.read()\n",
    "                user_text += read_text\n",
    "    sim_output = get_trait_dot_product(user_text, softmax_word_map, softmax_word_df)\n",
    "    user_accuracy = accuracy_per_trait(sim_output, row)\n",
    "    test_total_accuracy += user_accuracy\n",
    "    pbar.set_description(f\"Average test dataset accuracy: {round(np.mean(np.divide(test_total_accuracy, users.index(idx)+1))*100, 2)}\")\n",
    "    \n",
    "# Show total accuracy\n",
    "scaled_test_accuracy = np.divide(test_total_accuracy, len(arch_df))\n",
    "avg_test_accuracy = np.mean(scaled_test_accuracy)\n",
    "\n",
    "print(\"--- ACCURACY ON TESTING DATASET ---\")\n",
    "\n",
    "print(f\"Average test dataset accuracy: {round(avg_test_accuracy*100, 2)}%\")\n",
    "print(\"Accuracy per trait:\")\n",
    "for i in range(len(trait_list)):\n",
    "    print(f\"{trait_list[i]}: {round(scaled_test_accuracy[i] * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd3a44d-167e-4637-b77b-5339a45a05c2",
   "metadata": {},
   "source": [
    "## Regression model - testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e084c659-3439-40dc-8b7a-e0acdd27c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods\n",
    "def get_trait_dot_product(post_text: str, word_map: list, word_dataframe: pd.DataFrame) -> list:\n",
    "    # Filter out the text\n",
    "    filtered_post = remove_stopwords(clean_up_text(post_text))\n",
    "    filtered_post += extract_hashtags(post_text)\n",
    "    \n",
    "    # Create a vector for dot product vector\n",
    "    post_vector = [0] * len(word_map)\n",
    "    \n",
    "    # Calculate word occurrences\n",
    "    word_ctr = Counter(filtered_post)\n",
    "    \n",
    "    for word, freq in word_ctr.items():\n",
    "        if word in word_map:\n",
    "            post_vector[word_map.index(word)] = freq\n",
    "    \n",
    "    # Calculate dot product for a given text\n",
    "    word_dot = word_dataframe.dot(post_vector)\n",
    "    \n",
    "    return word_dot\n",
    "\n",
    "# Replace NaN with 0 in word_frequency_table\n",
    "word_df = word_df.fillna(0)\n",
    "\n",
    "# Method for calculating the dot product of trait <-> influencer relation\n",
    "def get_influencer_dot_product(trait_output: list, influencer_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    return influencer_dataframe.dot(trait_output)\n",
    "\n",
    "# Method for calculating the similarity\n",
    "def calculate_similarity(post_text: str, \n",
    "                         word_map: list, \n",
    "                         word_dataframe: pd.DataFrame,\n",
    "                         influencer_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Calculate word-trait dot product\n",
    "    post_result = get_trait_dot_product(post_text, word_map, word_dataframe)\n",
    "    \n",
    "    # Calculate trate-influencer dot-product\n",
    "    inf_dot_product = get_influencer_dot_product(post_result, influencer_dataframe)\n",
    "    \n",
    "    # Get the sum of influencer traits\n",
    "    influencer_sum = influencer_dataframe.sum(axis=1)\n",
    "    \n",
    "    # Divide the dot product by the sum calculated above\n",
    "    inf_dot_product = inf_dot_product.divide(influencer_sum)\n",
    "    \n",
    "    return inf_dot_product\n",
    "\n",
    "# Trait accuracy - round the results\n",
    "def natural_round(x: float) -> int:\n",
    "    out = int(x // 1)\n",
    "    return out + 1 if (x - out) >= 0.5 else out\n",
    "\n",
    "def accuracy_per_trait(input_vector: pd.Series, annotated_vector: pd.Series) -> np.array:\n",
    "    out_array = np.array([0] * 37, dtype=np.int)\n",
    "    for i in range(len(out_array)):\n",
    "        if natural_round(input_vector[i]) == annotated_vector[i]:\n",
    "            out_array[i] = 1\n",
    "    return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1803a011-2c34-431d-8826-105c58929bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average test dataset accuracy: 18.08: : 177it [14:44,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ACCURACY ON TESTING DATASET ---\n",
      "Average test dataset accuracy: 18.08%\n",
      "Accuracy per trait:\n",
      "innocent: 3.95%\n",
      "sage: 10.17%\n",
      "explorer: 33.33%\n",
      "outlaw: 28.25%\n",
      "magician: 20.9%\n",
      "hero: 24.29%\n",
      "lover: 30.51%\n",
      "jester: 31.07%\n",
      "everyman: 6.78%\n",
      "caregiver: 31.07%\n",
      "ruler: 8.47%\n",
      "creator: 7.91%\n",
      "dominant: 3.95%\n",
      "submissive: 35.03%\n",
      "maximalist: 2.82%\n",
      "minimalist: 36.72%\n",
      "inspiring: 8.47%\n",
      "systematic: 7.34%\n",
      "discovering: 16.38%\n",
      "conservative: 22.03%\n",
      "verifying: 3.39%\n",
      "overlooking: 30.51%\n",
      "sharpening: 31.07%\n",
      "harmonic: 6.21%\n",
      "empathic: 16.95%\n",
      "matter_of_fact: 12.99%\n",
      "brave: 31.07%\n",
      "protective: 35.03%\n",
      "generous: 4.52%\n",
      "thrifty: 27.12%\n",
      "favourable: 25.99%\n",
      "balanced: 22.6%\n",
      "sensuality: 2.26%\n",
      "intelligent: 4.52%\n",
      "believe: 7.34%\n",
      "egocentric: 19.21%\n",
      "allocentric: 18.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(arch_df.iterrows())\n",
    "\n",
    "# Out accuracy vector\n",
    "test_reg_total_accuracy = np.array([0] * 37, dtype=np.int)\n",
    "\n",
    "for idx, row in pbar:\n",
    "    profile_path = os.path.join(BASE_DIR, idx)\n",
    "    user_text = \"\"\n",
    "    for file in os.listdir(profile_path):\n",
    "        if not file.endswith(\".toml\"):\n",
    "            with open(os.path.join(profile_path, file), \"r\") as post_f:\n",
    "                read_text = post_f.read()\n",
    "                user_text += read_text\n",
    "    sim_output = get_trait_dot_product(user_text, word_map, word_df)\n",
    "    user_accuracy = accuracy_per_trait(sim_output, row)\n",
    "    test_reg_total_accuracy += user_accuracy\n",
    "    pbar.set_description(f\"Average test dataset accuracy: {round(np.mean(np.divide(test_reg_total_accuracy, users.index(idx)+1))*100, 2)}\")\n",
    "    \n",
    "# Show total accuracy\n",
    "scaled_reg_test_accuracy = np.divide(test_reg_total_accuracy, len(arch_df))\n",
    "avg_reg_test_accuracy = np.mean(scaled_reg_test_accuracy)\n",
    "\n",
    "print(\"--- ACCURACY ON TESTING DATASET ---\")\n",
    "\n",
    "print(f\"Average test dataset accuracy: {round(avg_reg_test_accuracy*100, 2)}%\")\n",
    "print(\"Accuracy per trait:\")\n",
    "for i in range(len(trait_list)):\n",
    "    print(f\"{trait_list[i]}: {round(scaled_reg_test_accuracy[i] * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d08722-03a9-44f6-84f0-60b0cae52084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
