{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2386ecd2-193a-40a0-981e-43d8edce8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate words with archetypes/character traits as intermediate layer\n",
    "# and with influencer as the \"last\" layer\n",
    "\n",
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "import toml\n",
    "import re\n",
    "import itertools\n",
    "from text_cleaner import *\n",
    "import operator\n",
    "from collections import Counter\n",
    "from spmf import Spmf\n",
    "\n",
    "def extract_hashtags(post_text):\n",
    "    HASH_RE = re.compile(r\"\\#\\w+\")\n",
    "    out_list = re.findall(HASH_RE, post_text)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "367d5f27-6428-4e65-8311-3fafe4df4140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['innocent', 'sage', 'explorer', 'outlaw', 'magician', 'hero', 'lover', 'jester', 'everyman', 'caregiver', 'ruler', 'creator', 'dominant', 'submissive', 'maximalist', 'minimalist', 'inspiring', 'systematic', 'discovering', 'conservative', 'verifying', 'overlooking', 'sharpening', 'harmonic', 'empathic', 'matter_of_fact', 'brave', 'protective', 'generous', 'thrifty', 'favourable', 'balanced', 'sensuality', 'intelligent', 'believe', 'egocentric', 'allocentric']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>innocent</th>\n",
       "      <th>sage</th>\n",
       "      <th>explorer</th>\n",
       "      <th>outlaw</th>\n",
       "      <th>magician</th>\n",
       "      <th>hero</th>\n",
       "      <th>lover</th>\n",
       "      <th>jester</th>\n",
       "      <th>everyman</th>\n",
       "      <th>caregiver</th>\n",
       "      <th>...</th>\n",
       "      <th>protective</th>\n",
       "      <th>generous</th>\n",
       "      <th>thrifty</th>\n",
       "      <th>favourable</th>\n",
       "      <th>balanced</th>\n",
       "      <th>sensuality</th>\n",
       "      <th>intelligent</th>\n",
       "      <th>believe</th>\n",
       "      <th>egocentric</th>\n",
       "      <th>allocentric</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>marek_grodzki</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vege_style_life</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oliwka__2007</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z_przestrzeni_serca</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zaradne_warsztaty</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     innocent  sage  explorer  outlaw  magician  hero  lover  \\\n",
       "id                                                                             \n",
       "marek_grodzki             0.0   0.0       0.0     0.0       0.0   0.0    0.0   \n",
       "vege_style_life           0.0   0.0       4.0     0.0       0.0   0.0    0.0   \n",
       "oliwka__2007              0.0   0.0       0.0     0.0       0.0   0.0    0.0   \n",
       "z_przestrzeni_serca       4.0   0.0       0.0     0.0       0.0   0.0    0.0   \n",
       "zaradne_warsztaty         3.0   0.0       0.0     0.0       3.0   0.0    0.0   \n",
       "\n",
       "                     jester  everyman  caregiver  ...  protective  generous  \\\n",
       "id                                                ...                         \n",
       "marek_grodzki           4.0       0.0        0.0  ...         2.0       3.0   \n",
       "vege_style_life         0.0       0.0        0.0  ...         4.0       4.0   \n",
       "oliwka__2007            0.0       4.0        0.0  ...         2.0       2.0   \n",
       "z_przestrzeni_serca     0.0       0.0        0.0  ...         4.0       3.0   \n",
       "zaradne_warsztaty       2.0       3.0        4.0  ...         3.0       4.0   \n",
       "\n",
       "                     thrifty  favourable  balanced  sensuality  intelligent  \\\n",
       "id                                                                            \n",
       "marek_grodzki            4.0         4.0       3.0         4.0          4.0   \n",
       "vege_style_life          4.0         4.0       3.0         3.0          3.0   \n",
       "oliwka__2007             0.0         3.0       1.0         2.0          4.0   \n",
       "z_przestrzeni_serca      0.0         4.0       4.0         3.0          4.0   \n",
       "zaradne_warsztaty        0.0         2.0       2.0         4.0          2.0   \n",
       "\n",
       "                     believe  egocentric  allocentric  \n",
       "id                                                     \n",
       "marek_grodzki            3.0         0.0          0.0  \n",
       "vege_style_life          2.0         0.0          3.0  \n",
       "oliwka__2007             1.0         0.0          3.0  \n",
       "z_przestrzeni_serca      4.0         0.0          1.0  \n",
       "zaradne_warsztaty        3.0         1.0          3.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the .csv with archetypes\n",
    "arch_df = pd.read_csv('archetypes_pl.csv', index_col=0)\n",
    "\n",
    "# Save the order of columns\n",
    "trait_list = arch_df.columns.tolist()\n",
    "\n",
    "# Show the table header and column list\n",
    "print(trait_list)\n",
    "arch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48c394e-8c8b-4712-abd2-2924ad9af4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>innocent</th>\n",
       "      <th>sage</th>\n",
       "      <th>explorer</th>\n",
       "      <th>outlaw</th>\n",
       "      <th>magician</th>\n",
       "      <th>hero</th>\n",
       "      <th>lover</th>\n",
       "      <th>jester</th>\n",
       "      <th>everyman</th>\n",
       "      <th>caregiver</th>\n",
       "      <th>...</th>\n",
       "      <th>protective</th>\n",
       "      <th>generous</th>\n",
       "      <th>thrifty</th>\n",
       "      <th>favourable</th>\n",
       "      <th>balanced</th>\n",
       "      <th>sensuality</th>\n",
       "      <th>intelligent</th>\n",
       "      <th>believe</th>\n",
       "      <th>egocentric</th>\n",
       "      <th>allocentric</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>marek_grodzki</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vege_style_life</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oliwka__2007</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z_przestrzeni_serca</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zaradne_warsztaty</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     innocent  sage  explorer  outlaw  magician  hero  lover  \\\n",
       "id                                                                             \n",
       "marek_grodzki            -1.0  -1.0      -1.0    -1.0      -1.0  -1.0   -1.0   \n",
       "vege_style_life          -1.0  -1.0       1.0    -1.0      -1.0  -1.0   -1.0   \n",
       "oliwka__2007             -1.0  -1.0      -1.0    -1.0      -1.0  -1.0   -1.0   \n",
       "z_przestrzeni_serca       1.0  -1.0      -1.0    -1.0      -1.0  -1.0   -1.0   \n",
       "zaradne_warsztaty         0.5  -1.0      -1.0    -1.0       0.5  -1.0   -1.0   \n",
       "\n",
       "                     jester  everyman  caregiver  ...  protective  generous  \\\n",
       "id                                                ...                         \n",
       "marek_grodzki           1.0      -1.0       -1.0  ...         0.0       0.5   \n",
       "vege_style_life        -1.0      -1.0       -1.0  ...         1.0       1.0   \n",
       "oliwka__2007           -1.0       1.0       -1.0  ...         0.0       0.0   \n",
       "z_przestrzeni_serca    -1.0      -1.0       -1.0  ...         1.0       0.5   \n",
       "zaradne_warsztaty       0.0       0.5        1.0  ...         0.5       1.0   \n",
       "\n",
       "                     thrifty  favourable  balanced  sensuality  intelligent  \\\n",
       "id                                                                            \n",
       "marek_grodzki            1.0         1.0       0.5         1.0          1.0   \n",
       "vege_style_life          1.0         1.0       0.5         0.5          0.5   \n",
       "oliwka__2007            -1.0         0.5      -0.5         0.0          1.0   \n",
       "z_przestrzeni_serca     -1.0         1.0       1.0         0.5          1.0   \n",
       "zaradne_warsztaty       -1.0         0.0       0.0         1.0          0.0   \n",
       "\n",
       "                     believe  egocentric  allocentric  \n",
       "id                                                     \n",
       "marek_grodzki            0.5        -1.0         -1.0  \n",
       "vege_style_life          0.0        -1.0          0.5  \n",
       "oliwka__2007            -0.5        -1.0          0.5  \n",
       "z_przestrzeni_serca      1.0        -1.0         -0.5  \n",
       "zaradne_warsztaty        0.5        -0.5          0.5  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table preprocessing - replace all NaN with 2 (Unrelated/Don't know class), replace 0-5 values with the ones in range -1.0 - 1.0\n",
    "arch_df = arch_df.fillna(2.0)\n",
    "\n",
    "arch_df = arch_df.replace(0.0, -1.0)\n",
    "arch_df = arch_df.replace(1.0, -0.5)\n",
    "arch_df = arch_df.replace(2.0, 0.0)\n",
    "arch_df = arch_df.replace(3.0, 0.5)\n",
    "arch_df = arch_df.replace(4.0, 1.0)\n",
    "\n",
    "# Remove duplicated annotations, to exclude conflicting entries\n",
    "arch_df = arch_df[~arch_df.index.duplicated(keep='first')]\n",
    "\n",
    "# Print the head of the dataset after modification\n",
    "arch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8659fba-1ad3-4029-aabd-09f24b25a0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00, 39.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile zaradne_warsztaty has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:00, 47.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile ilona_browstylist has no posts.\n",
      "Profile pracownia.lepiej has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [00:00, 44.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile kamann_living has no posts.\n",
      "Profile natalie_interiors has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:01, 45.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile krewetkowo has no posts.\n",
      "Profile eliza.gwiazda_official has no posts.\n",
      "Profile gettinenglish has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:01, 45.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile paulina.ihnat has no posts.\n",
      "Profile home_in_garden has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87it [00:01, 44.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile karola_moskal has no posts.\n",
      "Profile wierzbowa_architektura has no posts.\n",
      "Profile justka.ka has no posts.\n",
      "Profile mamologia has no posts.\n",
      "Profile kwejk has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109it [00:02, 37.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile swiatwiedzy has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "131it [00:03, 45.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile klaudia_lasecka has no posts.\n",
      "Profile dom.w.kwiatach has no posts.\n",
      "Profile ak.kingamadej has no posts.\n",
      "Profile owsianapl has no posts.\n",
      "Profile _agnieszka_leszczynska has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "153it [00:03, 45.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile czarna.owieczka has no posts.\n",
      "Profile hellohomla has no posts.\n",
      "Profile kulturoholiczka has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "163it [00:03, 41.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile aniolnaresorach has no posts.\n",
      "Profile krusia_domatorka has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "175it [00:04, 46.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile gotowanie_po_zmianie has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "194it [00:04, 50.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile ahojprzyrodo has no posts.\n",
      "Profile mr.stejku has no posts.\n",
      "Profile kinianieruda has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [00:04, 42.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile remont_ciala has no posts.\n",
      "Profile cooosure has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221it [00:05, 54.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile martapazera has no posts.\n",
      "Profile zdrowy_talerz has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "236it [00:05, 56.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile annamboland has no posts.\n",
      "Profile marketing_w_pigulce has no posts.\n",
      "Profile achdeco_polska has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:05, 51.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile imsokayka has no posts.\n",
      "Profile prosto.w.szarosci has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "264it [00:05, 53.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile podroze.rodzinne has no posts.\n",
      "Profile fitbadurka has no posts.\n",
      "Profile panifortuna has no posts.\n",
      "Profile kinga_strzalka has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "277it [00:06, 52.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile gotowe_projekty_domow_archon has no posts.\n",
      "Profile myblogyoll has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "289it [00:06, 50.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile zuzia_niemczycka has no posts.\n",
      "Profile justa_w_ogrodzie has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "307it [00:06, 52.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile agiksonn has no posts.\n",
      "Profile 620_nad_poziomem_morza has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:06, 47.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile fit_gruszecka has no posts.\n",
      "Profile wydawnictwoznakpl has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "320it [00:07, 48.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile martynagrajcke has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "332it [00:07, 46.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile staraochota_skrawki has no posts.\n",
      "Profile nabakowskapracowniawnetrz has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "354it [00:07, 43.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile paulinarubaszka has no posts.\n",
      "Profile karolina_er_ has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383it [00:08, 43.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile aga_bugaj has no posts.\n",
      "Profile lab.07 has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "405it [00:09, 41.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile pani_tester has no posts.\n",
      "Profile blogtasteaway has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "423it [00:09, 50.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile onajedna_home has no posts.\n",
      "Profile languagebay has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "442it [00:09, 56.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile alabasterfox has no posts.\n",
      "Profile aleksandra.herec has no posts.\n",
      "Profile rykalskaa has no posts.\n",
      "Profile domiogrod_przedsnem has no posts.\n",
      "Profile maddlajnn has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "460it [00:10, 44.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile poczujsielepiej has no posts.\n",
      "Profile naszswiatt has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "472it [00:10, 47.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile zdrowoczylisexy has no posts.\n",
      "Profile rutynowa has no posts.\n",
      "Profile aga.lanius has no posts.\n",
      "Profile balickadesign has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "504it [00:11, 55.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile nowinki.sklepowe has no posts.\n",
      "Profile moda_na_klasyki has no posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "508it [00:11, 45.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Check if a user has a non-empty directory in the dataset, otherwise delete the user from the list\n",
    "available_arch_df = copy.deepcopy(arch_df)\n",
    "posts = []\n",
    "hashtags = []\n",
    "\n",
    "BASE_DIR = \"instagram_dataset/pl\"\n",
    "\n",
    "# Iterate over whole DataFrame\n",
    "for i, row in tqdm(arch_df.iterrows()):\n",
    "    profile_posts = []\n",
    "    profile_hashtags = []\n",
    "    \n",
    "    # Iterate over all categories in base directory\n",
    "    for cat_dir in os.listdir(BASE_DIR):\n",
    "        whole_cat_dir = os.path.join(BASE_DIR, cat_dir)\n",
    "        \n",
    "        # If profile exists in the database\n",
    "        if i in os.listdir(whole_cat_dir):\n",
    "            profile_path = os.path.join(whole_cat_dir, i)\n",
    "            profile_config_path = os.path.join(whole_cat_dir, i, f\"{i}.toml\")\n",
    "            \n",
    "            # Check if there's a .toml file - if not, omit the profile\n",
    "            is_present = False            \n",
    "            if os.path.exists(profile_config_path):\n",
    "                is_present = True\n",
    "                for file in os.listdir(profile_path):\n",
    "                    if not file.endswith(\".toml\"):\n",
    "                        with open(os.path.join(profile_path, file), \"r\") as post_f:\n",
    "                            read_text = post_f.read()\n",
    "                            profile_posts.append(remove_stopwords(clean_up_text(read_text)))\n",
    "                            profile_hashtags.append(extract_hashtags(read_text))\n",
    "            else:\n",
    "                available_arch_df = available_arch_df.drop(i, axis=0)\n",
    "                print(f\"Profile {i} has no posts.\")\n",
    "            # Create new list for a given user    \n",
    "            if is_present:\n",
    "                # Merge lists - a single list for a single influencer\n",
    "                profile_hashtags = list(itertools.chain.from_iterable(profile_hashtags))\n",
    "                hashtags.append(profile_hashtags)\n",
    "                posts.append(list(itertools.chain.from_iterable([profile_posts])))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2006a7b-128b-45db-9115-b79504a4341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     innocent  sage  explorer  outlaw  magician  hero  lover  \\\n",
      "id                                                                             \n",
      "marek_grodzki            -1.0  -1.0      -1.0    -1.0      -1.0  -1.0   -1.0   \n",
      "vege_style_life          -1.0  -1.0       1.0    -1.0      -1.0  -1.0   -1.0   \n",
      "oliwka__2007             -1.0  -1.0      -1.0    -1.0      -1.0  -1.0   -1.0   \n",
      "z_przestrzeni_serca       1.0  -1.0      -1.0    -1.0      -1.0  -1.0   -1.0   \n",
      "snatch.machine           -1.0  -0.5       0.5     0.5       0.0   1.0   -1.0   \n",
      "\n",
      "                     jester  everyman  caregiver  ...  protective  generous  \\\n",
      "id                                                ...                         \n",
      "marek_grodzki           1.0      -1.0       -1.0  ...         0.0       0.5   \n",
      "vege_style_life        -1.0      -1.0       -1.0  ...         1.0       1.0   \n",
      "oliwka__2007           -1.0       1.0       -1.0  ...         0.0       0.0   \n",
      "z_przestrzeni_serca    -1.0      -1.0       -1.0  ...         1.0       0.5   \n",
      "snatch.machine         -0.5       0.0       -1.0  ...        -0.5       0.0   \n",
      "\n",
      "                     thrifty  favourable  balanced  sensuality  intelligent  \\\n",
      "id                                                                            \n",
      "marek_grodzki            1.0         1.0       0.5         1.0          1.0   \n",
      "vege_style_life          1.0         1.0       0.5         0.5          0.5   \n",
      "oliwka__2007            -1.0         0.5      -0.5         0.0          1.0   \n",
      "z_przestrzeni_serca     -1.0         1.0       1.0         0.5          1.0   \n",
      "snatch.machine          -0.5         0.5      -0.5        -0.5          0.0   \n",
      "\n",
      "                     believe  egocentric  allocentric  \n",
      "id                                                     \n",
      "marek_grodzki            0.5        -1.0         -1.0  \n",
      "vege_style_life          0.0        -1.0          0.5  \n",
      "oliwka__2007            -0.5        -1.0          0.5  \n",
      "z_przestrzeni_serca      1.0        -1.0         -0.5  \n",
      "snatch.machine          -0.5         0.5         -0.5  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "Available dataset length: 433\n"
     ]
    }
   ],
   "source": [
    "# Show the current, filtered out database\n",
    "print(available_arch_df.head())\n",
    "print(f\"Available dataset length: {len(available_arch_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b962179-b007-40b8-b34f-b9b038576884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map usernames to indices\n",
    "users = list(available_arch_df.index.values)\n",
    "user_indices = {k: users.index(k) for k in users}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "098add10-36bb-4a3e-bd87-5b552d6f0b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/37 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6171 ms\n",
      " Frequent sequences count : 1862729\n",
      " Max memory (mb) : 1029.111328125\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 1862729\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                                         innocent\n",
      "['zainspirowany']                    1.298465e-05\n",
      "['wczorajszym']                      5.193861e-05\n",
      "['wczorajszym', 'poście']            2.412996e-06\n",
      "['wczorajszym', 'poście', 'innymi']  4.845665e-07\n",
      "['wczorajszym', 'naprawdę']          1.930397e-06\n",
      "...                                           ...\n",
      "#여성                                  3.679975e-06\n",
      "#ﬁtnessgirl                          3.679975e-06\n",
      "#ａｅｓｔｈｅｔｉｃ                           5.151965e-05\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                              3.679975e-06\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą                 3.679975e-06\n",
      "\n",
      "[1913357 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/37 [00:22<13:33, 22.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 7880 ms\n",
      " Frequent sequences count : 3318340\n",
      " Max memory (mb) : 1131.62109375\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 3318340\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                                             sage\n",
      "['zainspirowany']                    1.159027e-05\n",
      "['wczorajszym']                      5.505379e-05\n",
      "['wczorajszym', 'poście']            2.071720e-06\n",
      "['wczorajszym', 'poście', 'innymi']  3.600215e-07\n",
      "['wczorajszym', 'naprawdę']          1.657376e-06\n",
      "...                                           ...\n",
      "#інтер                               4.562231e-06\n",
      "#ねこ                                  9.124462e-06\n",
      "#猫                                   9.124462e-06\n",
      "#ﬁtnessgirl                          4.562231e-06\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą                 4.562231e-06\n",
      "\n",
      "[3362192 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/37 [01:30<28:48, 49.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6922 ms\n",
      " Frequent sequences count : 3474220\n",
      " Max memory (mb) : 1060.6376953125\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 3474220\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                                         explorer\n",
      "['zainspirowany']                    1.625472e-05\n",
      "['wczorajszym']                      5.851698e-05\n",
      "['wczorajszym', 'poście']            2.364591e-06\n",
      "['wczorajszym', 'poście', 'innymi']  3.269538e-07\n",
      "['wczorajszym', 'naprawdę']          1.418754e-06\n",
      "...                                           ...\n",
      "#猫                                   8.769122e-06\n",
      "#ﬁtnessgirl                          4.384561e-06\n",
      "#ａｅｓｔｈｅｔｉｃ                           4.384561e-05\n",
      "#ｔｈｏｕｇｈｔｓ                            4.384561e-06\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą                 4.384561e-06\n",
      "\n",
      "[3519096 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/37 [03:23<44:23, 78.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6257 ms\n",
      " Frequent sequences count : 3025204\n",
      " Max memory (mb) : 1024.6738204956055\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 3025204\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                                           outlaw\n",
      "['zainspirowany']                    1.422910e-05\n",
      "['wczorajszym']                      5.691641e-05\n",
      "['wczorajszym', 'poście']            2.762100e-06\n",
      "['wczorajszym', 'poście', 'innymi']  3.827676e-07\n",
      "['wczorajszym', 'naprawdę']          1.657260e-06\n",
      "...                                           ...\n",
      "#ねこ                                  1.000655e-05\n",
      "#猫                                   1.000655e-05\n",
      "#ﬁtnessgirl                          5.003277e-06\n",
      "#ａｅｓｔｈｅｔｉｃ                           5.003277e-05\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą                 5.003277e-06\n",
      "\n",
      "[3067681 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 4/37 [05:14<50:11, 91.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6314 ms\n",
      " Frequent sequences count : 3051974\n",
      " Max memory (mb) : 942.35009765625\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 3051974\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                                         magician\n",
      "['zainspirowany']                    1.482937e-05\n",
      "['wczorajszym']                      5.931748e-05\n",
      "['wczorajszym', 'poście']            2.782643e-06\n",
      "['wczorajszym', 'poście', 'innymi']  3.768889e-07\n",
      "['wczorajszym', 'naprawdę']          1.669586e-06\n",
      "...                                           ...\n",
      "#猫                                   1.050873e-05\n",
      "#美少女戦士セーラームーン                        5.254364e-06\n",
      "#ﬁtnessgirl                          5.254364e-06\n",
      "#ａｅｓｔｈｅｔｉｃ                           5.254364e-05\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą                 5.254364e-06\n",
      "\n",
      "[3092284 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 5/37 [07:10<53:25, 100.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6152 ms\n",
      " Frequent sequences count : 3045090\n",
      " Max memory (mb) : 956.677734375\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 3045090\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                                             hero\n",
      "['zainspirowany']                    1.849530e-05\n",
      "['wczorajszym']                      7.398119e-05\n",
      "['wczorajszym', 'poście']            2.810517e-06\n",
      "['wczorajszym', 'poście', 'innymi']  3.792890e-07\n",
      "['wczorajszym', 'naprawdę']          1.686310e-06\n",
      "...                                           ...\n",
      "#猫                                   1.039652e-05\n",
      "#ﬁtbody                              5.198262e-06\n",
      "#ﬁtnessgirl                          5.198262e-06\n",
      "#ａｅｓｔｈｅｔｉｃ                           5.198262e-05\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą                 5.198262e-06\n",
      "\n",
      "[3084848 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 6/37 [09:01<53:42, 103.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 5447 ms\n",
      " Frequent sequences count : 1684610\n",
      " Max memory (mb) : 1066.08349609375\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 1684610\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                                            lover\n",
      "['zainspirowany']                    1.217345e-05\n",
      "['wczorajszym']                      4.565043e-05\n",
      "['wczorajszym', 'poście']            2.885330e-06\n",
      "['wczorajszym', 'poście', 'innymi']  5.350438e-07\n",
      "['wczorajszym', 'innymi']            2.308264e-06\n",
      "...                                           ...\n",
      "#ﬁtfam                               4.215105e-06\n",
      "#ﬁtnessgirl                          4.215105e-06\n",
      "#ａｅｓｔｈｅｔｉｃ                           4.215105e-05\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                              4.215105e-06\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą                 4.215105e-06\n",
      "\n",
      "[1730639 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 7/37 [10:22<48:14, 96.49s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6130 ms\n",
      " Frequent sequences count : 3220611\n",
      " Max memory (mb) : 937.3232421875\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 3220611\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                                           jester\n",
      "['zainspirowany']                    1.712851e-05\n",
      "['wczorajszym']                      5.823693e-05\n",
      "['wczorajszym', 'poście']            2.545038e-06\n",
      "['wczorajszym', 'poście', 'innymi']  3.576034e-07\n",
      "['wczorajszym', 'naprawdę']          1.527023e-06\n",
      "...                                           ...\n",
      "#猫                                   9.677076e-06\n",
      "#ﬁtfam                               4.838538e-06\n",
      "#ﬁtnessgirl                          4.838538e-06\n",
      "#ａｅｓｔｈｅｔｉｃ                           4.838538e-05\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą                 4.838538e-06\n",
      "\n",
      "[3263198 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 8/37 [12:07<47:54, 99.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6813 ms\n",
      " Frequent sequences count : 1982895\n",
      " Max memory (mb) : 1155.046875\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 1982895\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                                         everyman\n",
      "['zainspirowany']                    1.008090e-05\n",
      "['wczorajszym']                      6.048540e-05\n",
      "['wczorajszym', 'poście']            2.243302e-06\n",
      "['wczorajszym', 'poście', 'innymi']  4.506976e-07\n",
      "['wczorajszym', 'naprawdę']          1.794642e-06\n",
      "...                                           ...\n",
      "#美少女戦士セーラームーン                        3.542005e-06\n",
      "#여성                                  3.542005e-06\n",
      "#ﬁtnessgirl                          3.542005e-06\n",
      "#ａｅｓｔｈｅｔｉｃ                           4.958806e-05\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą                 3.542005e-06\n",
      "\n",
      "[2033546 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 9/37 [13:23<42:50, 91.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6850 ms\n",
      " Frequent sequences count : 3409258\n",
      " Max memory (mb) : 979.24365234375\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 3409258\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                                        caregiver\n",
      "['zainspirowany']                    1.215166e-05\n",
      "['wczorajszym']                      5.164458e-05\n",
      "['wczorajszym', 'poście']            2.275906e-06\n",
      "['wczorajszym', 'poście', 'innymi']  3.379707e-07\n",
      "['wczorajszym', 'naprawdę']          1.365544e-06\n",
      "...                                           ...\n",
      "#猫                                   9.038690e-06\n",
      "#鋸                                   4.519345e-06\n",
      "#ﬁtnessgirl                          4.519345e-06\n",
      "#ａｅｓｔｈｅｔｉｃ                           4.519345e-05\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą                 4.519345e-06\n",
      "\n",
      "[3453316 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 10/37 [14:59<41:56, 93.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 5625 ms\n",
      " Frequent sequences count : 2845383\n",
      " Max memory (mb) : 939.92236328125\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 2845383\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                                            ruler\n",
      "['zainspirowany']                    1.474252e-05\n",
      "['wczorajszym']                      5.897009e-05\n",
      "['wczorajszym', 'poście']            2.819134e-06\n",
      "['wczorajszym', 'poście', 'innymi']  4.015145e-07\n",
      "['wczorajszym', 'naprawdę']          1.691481e-06\n",
      "...                                           ...\n",
      "#木工教室                                5.330178e-06\n",
      "#猫                                   1.066036e-05\n",
      "#鋸                                   5.330178e-06\n",
      "#ﬁtnessgirl                          5.330178e-06\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą                 5.330178e-06\n",
      "\n",
      "[2884646 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 11/37 [16:58<43:48, 101.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 5961 ms\n",
      " Frequent sequences count : 3116909\n",
      " Max memory (mb) : 1004.85595703125\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 3116909\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                                          creator\n",
      "['zainspirowany']                    1.355128e-05\n",
      "['wczorajszym']                      5.759295e-05\n",
      "['wczorajszym', 'poście']            2.647918e-06\n",
      "['wczorajszym', 'poście', 'innymi']  3.704289e-07\n",
      "['wczorajszym', 'naprawdę']          1.588751e-06\n",
      "...                                           ...\n",
      "#ﬁtnessgirl                          4.515509e-06\n",
      "#ａｅｓｔｈｅｔｉｃ                           4.515509e-05\n",
      "#ｔｈｏｕｇｈｔｓ                            4.515509e-06\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                              4.515509e-06\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą                 4.515509e-06\n",
      "\n",
      "[3160797 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 12/37 [18:46<43:00, 103.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6545 ms\n",
      " Frequent sequences count : 1117635\n",
      " Max memory (mb) : 1224.47900390625\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1117635\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            dominant\n",
      "['zainspirowany']           0.000013\n",
      "['wczorajszym']             0.000053\n",
      "['wczorajszym', 'poście']   0.000002\n",
      "['wczorajszym', 'przepis']  0.000002\n",
      "['odnośnie']                0.000096\n",
      "...                              ...\n",
      "#ﬁtnessgirl                 0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                  0.000042\n",
      "#ｔｈｏｕｇｈｔｓ                   0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                     0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą        0.000003\n",
      "\n",
      "[1175258 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 13/37 [20:49<43:38, 109.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6731 ms\n",
      " Frequent sequences count : 1143408\n",
      " Max memory (mb) : 1174.66796875\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1143408\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            submissive\n",
      "['zainspirowany']             0.000013\n",
      "['wczorajszym']               0.000050\n",
      "['wczorajszym', 'poście']     0.000002\n",
      "['wczorajszym', 'przepis']    0.000002\n",
      "['odnośnie']                  0.000094\n",
      "...                                ...\n",
      "#ﬁtnessgirl                   0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                    0.000041\n",
      "#ｔｈｏｕｇｈｔｓ                     0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                       0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą          0.000003\n",
      "\n",
      "[1202417 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 14/37 [22:54<43:40, 113.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6449 ms\n",
      " Frequent sequences count : 1115610\n",
      " Max memory (mb) : 1156.72900390625\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1115610\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            maximalist\n",
      "['zainspirowany']             0.000013\n",
      "['wczorajszym']               0.000055\n",
      "['wczorajszym', 'poście']     0.000002\n",
      "['wczorajszym', 'przepis']    0.000002\n",
      "['odnośnie']                  0.000091\n",
      "...                                ...\n",
      "#ﬁtnessgirl                   0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                    0.000041\n",
      "#ｔｈｏｕｇｈｔｓ                     0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                       0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą          0.000003\n",
      "\n",
      "[1173796 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 15/37 [25:01<43:15, 117.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6608 ms\n",
      " Frequent sequences count : 1128848\n",
      " Max memory (mb) : 1152.71923828125\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1128848\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            minimalist\n",
      "['zainspirowany']             0.000013\n",
      "['wczorajszym']               0.000054\n",
      "['wczorajszym', 'poście']     0.000002\n",
      "['wczorajszym', 'przepis']    0.000002\n",
      "['odnośnie']                  0.000096\n",
      "...                                ...\n",
      "#ﬁtnessgirl                   0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                    0.000042\n",
      "#ｔｈｏｕｇｈｔｓ                     0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                       0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą          0.000003\n",
      "\n",
      "[1186302 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 16/37 [27:06<41:58, 119.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6766 ms\n",
      " Frequent sequences count : 1215650\n",
      " Max memory (mb) : 1159.3076171875\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1215650\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            inspiring\n",
      "['zainspirowany']            0.000012\n",
      "['wczorajszym']              0.000054\n",
      "['wczorajszym', 'poście']    0.000002\n",
      "['wczorajszym', 'przepis']   0.000002\n",
      "['odnośnie']                 0.000093\n",
      "...                               ...\n",
      "#ﬁtnessgirl                  0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                   0.000041\n",
      "#ｔｈｏｕｇｈｔｓ                    0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                      0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą         0.000003\n",
      "\n",
      "[1275002 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 17/37 [29:17<41:03, 123.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6423 ms\n",
      " Frequent sequences count : 1159984\n",
      " Max memory (mb) : 1161.966796875\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1159984\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            systematic\n",
      "['zainspirowany']             0.000013\n",
      "['wczorajszym']               0.000052\n",
      "['wczorajszym', 'poście']     0.000002\n",
      "['wczorajszym', 'przepis']    0.000002\n",
      "['odnośnie']                  0.000085\n",
      "...                                ...\n",
      "#ﬁtnessgirl                   0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                    0.000043\n",
      "#ｔｈｏｕｇｈｔｓ                     0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                       0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą          0.000003\n",
      "\n",
      "[1217298 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 18/37 [31:25<39:32, 124.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 7056 ms\n",
      " Frequent sequences count : 1222535\n",
      " Max memory (mb) : 1102.037109375\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1222535\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            discovering\n",
      "['zainspirowany']              0.000012\n",
      "['wczorajszym']                0.000052\n",
      "['wczorajszym', 'poście']      0.000002\n",
      "['wczorajszym', 'przepis']     0.000002\n",
      "['odnośnie']                   0.000092\n",
      "...                                 ...\n",
      "#ﬁtnessgirl                    0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                     0.000040\n",
      "#ｔｈｏｕｇｈｔｓ                      0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                        0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą           0.000003\n",
      "\n",
      "[1283415 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 19/37 [33:37<38:02, 126.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 5990 ms\n",
      " Frequent sequences count : 1091872\n",
      " Max memory (mb) : 1241.73291015625\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1091872\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            conservative\n",
      "['zainspirowany']               0.000014\n",
      "['wczorajszym']                 0.000052\n",
      "['wczorajszym', 'poście']       0.000002\n",
      "['wczorajszym', 'przepis']      0.000002\n",
      "['odnośnie']                    0.000097\n",
      "...                                  ...\n",
      "#ﬁtnessgirl                     0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                      0.000044\n",
      "#ｔｈｏｕｇｈｔｓ                       0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                         0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą            0.000003\n",
      "\n",
      "[1148328 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 20/37 [35:41<35:43, 126.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6299 ms\n",
      " Frequent sequences count : 1167901\n",
      " Max memory (mb) : 894.4108734130859\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1167901\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            verifying\n",
      "['zainspirowany']            0.000013\n",
      "['wczorajszym']              0.000053\n",
      "['wczorajszym', 'poście']    0.000002\n",
      "['wczorajszym', 'przepis']   0.000002\n",
      "['odnośnie']                 0.000091\n",
      "...                               ...\n",
      "#ﬁtnessgirl                  0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                   0.000043\n",
      "#ｔｈｏｕｇｈｔｓ                    0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                      0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą         0.000003\n",
      "\n",
      "[1225118 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 21/37 [37:54<34:10, 128.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6231 ms\n",
      " Frequent sequences count : 1144627\n",
      " Max memory (mb) : 1208.8984375\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1144627\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            overlooking\n",
      "['zainspirowany']              0.000013\n",
      "['wczorajszym']                0.000053\n",
      "['wczorajszym', 'przepis']     0.000002\n",
      "['odnośnie']                   0.000099\n",
      "['odnośnie', 'życia']          0.000003\n",
      "...                                 ...\n",
      "#ﬁtnessgirl                    0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                     0.000044\n",
      "#ｔｈｏｕｇｈｔｓ                      0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                        0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą           0.000003\n",
      "\n",
      "[1200713 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 22/37 [40:01<31:58, 127.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6101 ms\n",
      " Frequent sequences count : 1118895\n",
      " Max memory (mb) : 1176.20556640625\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1118895\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            sharpening\n",
      "['zainspirowany']             0.000013\n",
      "['wczorajszym']               0.000053\n",
      "['wczorajszym', 'poście']     0.000002\n",
      "['wczorajszym', 'przepis']    0.000002\n",
      "['odnośnie']                  0.000098\n",
      "...                                ...\n",
      "#ﬁtnessgirl                   0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                    0.000013\n",
      "#ｔｈｏｕｇｈｔｓ                     0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                       0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą          0.000003\n",
      "\n",
      "[1173796 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 23/37 [42:07<29:42, 127.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6326 ms\n",
      " Frequent sequences count : 1174126\n",
      " Max memory (mb) : 1121.947265625\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1174126\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            harmonic\n",
      "['zainspirowany']           0.000013\n",
      "['wczorajszym']             0.000049\n",
      "['wczorajszym', 'poście']   0.000002\n",
      "['wczorajszym', 'przepis']  0.000002\n",
      "['odnośnie']                0.000092\n",
      "...                              ...\n",
      "#ﬁtnessgirl                 0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                  0.000043\n",
      "#ｔｈｏｕｇｈｔｓ                   0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                     0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą        0.000003\n",
      "\n",
      "[1232017 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 24/37 [44:18<27:46, 128.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6587 ms\n",
      " Frequent sequences count : 1144450\n",
      " Max memory (mb) : 1168.9755859375\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1144450\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            empathic\n",
      "['zainspirowany']           0.000013\n",
      "['wczorajszym']             0.000051\n",
      "['wczorajszym', 'poście']   0.000002\n",
      "['wczorajszym', 'przepis']  0.000002\n",
      "['odnośnie']                0.000093\n",
      "...                              ...\n",
      "#ﬁtnessgirl                 0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                  0.000042\n",
      "#ｔｈｏｕｇｈｔｓ                   0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                     0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą        0.000003\n",
      "\n",
      "[1201915 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 25/37 [46:24<25:31, 127.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 8022 ms\n",
      " Frequent sequences count : 2269783\n",
      " Max memory (mb) : 1205.90185546875\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 2269783\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                                     matter_of_fact\n",
      "['zainspirowany']                      1.286281e-05\n",
      "['wczorajszym']                        5.145125e-05\n",
      "['wczorajszym', 'poście']              1.857884e-06\n",
      "['wczorajszym', 'poście', 'innymi']    3.915640e-07\n",
      "['wczorajszym', 'naprawdę']            1.486307e-06\n",
      "...                                             ...\n",
      "#ﬁtnessgirl                            3.227879e-06\n",
      "#ａｅｓｔｈｅｔｉｃ                             4.519030e-05\n",
      "#ｔｈｏｕｇｈｔｓ                              3.227879e-06\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                                3.227879e-06\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą                   3.227879e-06\n",
      "\n",
      "[2324086 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 26/37 [48:28<23:13, 126.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 7245 ms\n",
      " Frequent sequences count : 2030430\n",
      " Max memory (mb) : 1216.26025390625\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 2030430\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                                            brave\n",
      "['zainspirowany']                    1.341787e-05\n",
      "['wczorajszym']                      5.590777e-05\n",
      "['wczorajszym', 'poście']            2.018550e-06\n",
      "['wczorajszym', 'poście', 'innymi']  4.479120e-07\n",
      "['wczorajszym', 'naprawdę']          1.614840e-06\n",
      "...                                           ...\n",
      "#ﬁtnessgirl                          3.222906e-06\n",
      "#ａｅｓｔｈｅｔｉｃ                           1.289162e-05\n",
      "#ｔｈｏｕｇｈｔｓ                            3.222906e-06\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                              3.222906e-06\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą                 3.222906e-06\n",
      "\n",
      "[2084627 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 27/37 [50:18<20:16, 121.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 5914 ms\n",
      " Frequent sequences count : 1084519\n",
      " Max memory (mb) : 1218.8056640625\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1084519\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                      protective\n",
      "['niezmiennie']         0.000043\n",
      "['nr']                  0.000090\n",
      "['nr', 'nr']            0.000003\n",
      "['ciasto']              0.000703\n",
      "['ciasto', 'ciasto']    0.000072\n",
      "...                          ...\n",
      "#ﬁtnessgirl             0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ              0.000043\n",
      "#ｔｈｏｕｇｈｔｓ               0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                 0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą    0.000003\n",
      "\n",
      "[1140164 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 28/37 [52:20<18:14, 121.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6713 ms\n",
      " Frequent sequences count : 2118072\n",
      " Max memory (mb) : 1098.7607421875\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 2118072\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                             generous\n",
      "['zainspirowany']            0.000015\n",
      "['wczorajszym']              0.000058\n",
      "['wczorajszym', 'poście']    0.000002\n",
      "['wczorajszym', 'naprawdę']  0.000002\n",
      "['wczorajszym', 'przepis']   0.000002\n",
      "...                               ...\n",
      "#ﬁtfam                       0.000003\n",
      "#ﬁtnessgirl                  0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                   0.000014\n",
      "#ｔｈｏｕｇｈｔｓ                    0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                      0.000003\n",
      "\n",
      "[2169568 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 29/37 [54:06<15:34, 116.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6836 ms\n",
      " Frequent sequences count : 2115434\n",
      " Max memory (mb) : 1239.5400390625\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 2115434\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                              thrifty\n",
      "['zainspirowany']            0.000014\n",
      "['wczorajszym']              0.000055\n",
      "['wczorajszym', 'poście']    0.000002\n",
      "['wczorajszym', 'naprawdę']  0.000002\n",
      "['wczorajszym', 'przepis']   0.000002\n",
      "...                               ...\n",
      "#ﬁtfam                       0.000003\n",
      "#ﬁtnessgirl                  0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                   0.000048\n",
      "#ｔｈｏｕｇｈｔｓ                    0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                      0.000003\n",
      "\n",
      "[2166705 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 30/37 [55:52<13:15, 113.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6126 ms\n",
      " Frequent sequences count : 1140295\n",
      " Max memory (mb) : 1208.818359375\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1140295\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            favourable\n",
      "['zainspirowany']             0.000013\n",
      "['wczorajszym']               0.000059\n",
      "['wczorajszym', 'poście']     0.000002\n",
      "['wczorajszym', 'przepis']    0.000002\n",
      "['odnośnie']                  0.000090\n",
      "...                                ...\n",
      "#ﬁtnessgirl                   0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                    0.000030\n",
      "#ｔｈｏｕｇｈｔｓ                     0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                       0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą          0.000003\n",
      "\n",
      "[1196592 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 31/37 [57:55<11:38, 116.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6179 ms\n",
      " Frequent sequences count : 1092718\n",
      " Max memory (mb) : 1244.75390625\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1092718\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            balanced\n",
      "['zainspirowany']           0.000013\n",
      "['wczorajszym']             0.000054\n",
      "['wczorajszym', 'poście']   0.000002\n",
      "['wczorajszym', 'przepis']  0.000002\n",
      "['odnośnie']                0.000094\n",
      "...                              ...\n",
      "#ﬁtnessgirl                 0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                  0.000043\n",
      "#ｔｈｏｕｇｈｔｓ                   0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                     0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą        0.000003\n",
      "\n",
      "[1149995 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 32/37 [59:59<09:54, 118.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6475 ms\n",
      " Frequent sequences count : 1187423\n",
      " Max memory (mb) : 1197.12744140625\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1187423\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            sensuality\n",
      "['zainspirowany']             0.000012\n",
      "['wczorajszym']               0.000052\n",
      "['wczorajszym', 'poście']     0.000002\n",
      "['wczorajszym', 'przepis']    0.000002\n",
      "['odnośnie']                  0.000085\n",
      "...                                ...\n",
      "#ﬁtnessgirl                   0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                    0.000041\n",
      "#ｔｈｏｕｇｈｔｓ                     0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                       0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą          0.000003\n",
      "\n",
      "[1246274 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 33/37 [1:02:06<08:05, 121.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 7809 ms\n",
      " Frequent sequences count : 2253739\n",
      " Max memory (mb) : 1122.96044921875\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 2253739\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                             intelligent\n",
      "['zainspirowany']               0.000013\n",
      "['wczorajszym']                 0.000054\n",
      "['wczorajszym', 'poście']       0.000002\n",
      "['wczorajszym', 'naprawdę']     0.000002\n",
      "['wczorajszym', 'przepis']      0.000002\n",
      "...                                  ...\n",
      "#ﬁtfam                          0.000003\n",
      "#ﬁtnessgirl                     0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                      0.000048\n",
      "#ｔｈｏｕｇｈｔｓ                       0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą            0.000003\n",
      "\n",
      "[2306337 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 34/37 [1:04:08<06:04, 121.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 7450 ms\n",
      " Frequent sequences count : 2141040\n",
      " Max memory (mb) : 1197.59521484375\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 2141040\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                                          believe\n",
      "['zainspirowany']                    1.351147e-05\n",
      "['wczorajszym']                      4.954207e-05\n",
      "['wczorajszym', 'poście']            1.981674e-06\n",
      "['wczorajszym', 'poście', 'innymi']  4.193610e-07\n",
      "['wczorajszym', 'naprawdę']          1.585339e-06\n",
      "...                                           ...\n",
      "#ﬁtnessgirl                          3.349455e-06\n",
      "#ａｅｓｔｈｅｔｉｃ                           4.689238e-05\n",
      "#ｔｈｏｕｇｈｔｓ                            3.349455e-06\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                              3.349455e-06\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą                 3.349455e-06\n",
      "\n",
      "[2193611 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 35/37 [1:05:54<03:53, 116.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6317 ms\n",
      " Frequent sequences count : 1156266\n",
      " Max memory (mb) : 1158.98095703125\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1156266\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            egocentric\n",
      "['zainspirowany']             0.000013\n",
      "['wczorajszym']               0.000051\n",
      "['wczorajszym', 'poście']     0.000002\n",
      "['wczorajszym', 'przepis']    0.000002\n",
      "['odnośnie']                  0.000094\n",
      "...                                ...\n",
      "#ﬁtnessgirl                   0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                    0.000042\n",
      "#ｔｈｏｕｇｈｔｓ                     0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                       0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą          0.000003\n",
      "\n",
      "[1213760 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 36/37 [1:08:01<01:59, 119.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 6562 ms\n",
      " Frequent sequences count : 1164640\n",
      " Max memory (mb) : 1242.158203125\n",
      " minsup = 5 sequences.\n",
      " Pattern count : 1164640\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            allocentric\n",
      "['zainspirowany']              0.000013\n",
      "['wczorajszym']                0.000051\n",
      "['wczorajszym', 'poście']      0.000002\n",
      "['wczorajszym', 'przepis']     0.000002\n",
      "['odnośnie']                   0.000090\n",
      "...                                 ...\n",
      "#ﬁtnessgirl                    0.000003\n",
      "#ａｅｓｔｈｅｔｉｃ                     0.000042\n",
      "#ｔｈｏｕｇｈｔｓ                      0.000003\n",
      "#𝐞𝐜𝐨𝐃𝐲𝐰                        0.000003\n",
      "#𝕠𝕔𝕙𝕛𝕒𝕜𝕗𝕒𝕛𝕟𝕚𝕖𝕓𝕪ć𝕞𝕒𝕞ą           0.000003\n",
      "\n",
      "[1221534 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [1:10:13<00:00, 113.89s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get the word count and create a dataframe, where columns are archetypes/traits, and rows are single words\n",
    "# Initialize a word DataFrame\n",
    "word_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over all of the traits/archetypes\n",
    "for trait in tqdm(trait_list):\n",
    "    # Test procedure for a single trait\n",
    "    subset_df = available_arch_df[available_arch_df[trait] != 0][trait]\n",
    "    subset_indices = [user_indices[idx] for idx in subset_df.index.values]\n",
    "\n",
    "    # Get all posts for the list of influencers\n",
    "    f = operator.itemgetter(*subset_indices)\n",
    "    sublist = list(f(posts))\n",
    "    post_list = []\n",
    "    for user in sublist:\n",
    "        for post in user:\n",
    "            post_list.append(\" \".join(post))\n",
    "\n",
    "    # SPMF - get the most frequent sequences\n",
    "    spmf = Spmf(\"PrefixSpan\", input_direct=post_list,\n",
    "                output_filename=\"output.txt\", arguments=[0.00025, 3], input_type=\"text\")\n",
    "    spmf.run()\n",
    "    spmf_df = spmf.to_pandas_dataframe(pickle=False)\n",
    "\n",
    "    # Normalize the data for all lengths\n",
    "    spmf_df.loc[spmf_df[\"pattern\"].map(len) == 3, \"sup\"] = spmf_df.loc[spmf_df[\"pattern\"].map(len) == 3].sup / spmf_df[spmf_df[\"pattern\"].map(len) == 3][\"sup\"].sum()\n",
    "    spmf_df.loc[spmf_df[\"pattern\"].map(len) == 2, \"sup\"] = spmf_df.loc[spmf_df[\"pattern\"].map(len) == 2].sup / spmf_df[spmf_df[\"pattern\"].map(len) == 2][\"sup\"].sum()\n",
    "    spmf_df.loc[spmf_df[\"pattern\"].map(len) == 1, \"sup\"] = spmf_df.loc[spmf_df[\"pattern\"].map(len) == 1].sup / spmf_df[spmf_df[\"pattern\"].map(len) == 1][\"sup\"].sum()\n",
    "\n",
    "    # Convert lists to tuples\n",
    "    spmf_df[\"pattern\"] = spmf_df[\"pattern\"].apply(lambda x: str(x))\n",
    "\n",
    "    sublist = list(f(hashtags))\n",
    "\n",
    "    # Counter to calculate each word occurrences\n",
    "    sublist = list(itertools.chain.from_iterable(sublist))\n",
    "    trait_ctr = Counter(sublist)\n",
    "    trait_total = sum(trait_ctr.values())\n",
    "    trait_ctr = {k: float(v / trait_total) for k, v in trait_ctr.items() if v >= 1}\n",
    "    trait_ctr = {trait: trait_ctr}\n",
    "\n",
    "    tmp_df = pd.DataFrame.from_dict(trait_ctr, orient=\"columns\")\n",
    "\n",
    "    # Change column name\n",
    "    spmf_df = spmf_df.rename(columns={\"sup\": trait})\n",
    "\n",
    "    # Change index to pattern\n",
    "    spmf_df = spmf_df.reset_index(drop=True)\n",
    "    spmf_df = spmf_df.set_index(\"pattern\")\n",
    "\n",
    "    # Append the hashtags\n",
    "    spmf_df = spmf_df.append(tmp_df)\n",
    "    print(spmf_df)\n",
    "\n",
    "    spmf_df = spmf_df.transpose()\n",
    "    spmf_df.to_pickle(f\"dfs/{trait}.pickle\")\n",
    "\n",
    "    # Append the dataframe to word_df\n",
    "    word_df = word_df.append(spmf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99c6bf53-495a-413a-a9bd-f036af42a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save word_df to file\n",
    "word_df.to_pickle(\"word_trait_array.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fab531b-f50c-4900-a7df-6c1364f4aa01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>['zainspirowany']</th>\n",
       "      <th>['wczorajszym']</th>\n",
       "      <th>['wczorajszym', 'poście']</th>\n",
       "      <th>['wczorajszym', 'poście', 'innymi']</th>\n",
       "      <th>['wczorajszym', 'naprawdę']</th>\n",
       "      <th>['wczorajszym', 'przepis']</th>\n",
       "      <th>['wczorajszym', 'innymi']</th>\n",
       "      <th>['odnośnie']</th>\n",
       "      <th>['odnośnie', 'życia']</th>\n",
       "      <th>['odnośnie', 'naprawdę']</th>\n",
       "      <th>...</th>\n",
       "      <th>['podkład', 'serii']</th>\n",
       "      <th>['konkursu', 'udział', 'nagrody']</th>\n",
       "      <th>['this', 'hope', 'you']</th>\n",
       "      <th>['zadowolony', 'temu']</th>\n",
       "      <th>['przeczytaniu', 'czytając']</th>\n",
       "      <th>['potoczą', 'czekam']</th>\n",
       "      <th>['help', 'and', 'with']</th>\n",
       "      <th>['noszenia', 'maseczek']</th>\n",
       "      <th>['best', 'much']</th>\n",
       "      <th>['care', 'it']</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>innocent</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.845665e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sage</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>3.600215e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explorer</th>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>3.269538e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outlaw</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>3.827676e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>magician</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>3.768889e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hero</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>3.792890e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lover</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>5.350438e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jester</th>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>3.576034e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everyman</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.506976e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caregiver</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>3.379707e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ruler</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.015145e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creator</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>3.704289e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dominant</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>submissive</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maximalist</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minimalist</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inspiring</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>systematic</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discovering</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conservative</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verifying</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overlooking</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sharpening</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>harmonic</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>empathic</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matter_of_fact</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>3.915640e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brave</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.479120e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>protective</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generous</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thrifty</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favourable</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balanced</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sensuality</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intelligent</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>believe</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.193610e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.193610e-07</td>\n",
       "      <td>4.193610e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.193610e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>egocentric</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allocentric</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37 rows × 4745869 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ['zainspirowany']  ['wczorajszym']  ['wczorajszym', 'poście']  \\\n",
       "innocent                 0.000013         0.000052                   0.000002   \n",
       "sage                     0.000012         0.000055                   0.000002   \n",
       "explorer                 0.000016         0.000059                   0.000002   \n",
       "outlaw                   0.000014         0.000057                   0.000003   \n",
       "magician                 0.000015         0.000059                   0.000003   \n",
       "hero                     0.000018         0.000074                   0.000003   \n",
       "lover                    0.000012         0.000046                   0.000003   \n",
       "jester                   0.000017         0.000058                   0.000003   \n",
       "everyman                 0.000010         0.000060                   0.000002   \n",
       "caregiver                0.000012         0.000052                   0.000002   \n",
       "ruler                    0.000015         0.000059                   0.000003   \n",
       "creator                  0.000014         0.000058                   0.000003   \n",
       "dominant                 0.000013         0.000053                   0.000002   \n",
       "submissive               0.000013         0.000050                   0.000002   \n",
       "maximalist               0.000013         0.000055                   0.000002   \n",
       "minimalist               0.000013         0.000054                   0.000002   \n",
       "inspiring                0.000012         0.000054                   0.000002   \n",
       "systematic               0.000013         0.000052                   0.000002   \n",
       "discovering              0.000012         0.000052                   0.000002   \n",
       "conservative             0.000014         0.000052                   0.000002   \n",
       "verifying                0.000013         0.000053                   0.000002   \n",
       "overlooking              0.000013         0.000053                   0.000000   \n",
       "sharpening               0.000013         0.000053                   0.000002   \n",
       "harmonic                 0.000013         0.000049                   0.000002   \n",
       "empathic                 0.000013         0.000051                   0.000002   \n",
       "matter_of_fact           0.000013         0.000051                   0.000002   \n",
       "brave                    0.000013         0.000056                   0.000002   \n",
       "protective               0.000011         0.000049                   0.000002   \n",
       "generous                 0.000015         0.000058                   0.000002   \n",
       "thrifty                  0.000014         0.000055                   0.000002   \n",
       "favourable               0.000013         0.000059                   0.000002   \n",
       "balanced                 0.000013         0.000054                   0.000002   \n",
       "sensuality               0.000012         0.000052                   0.000002   \n",
       "intelligent              0.000013         0.000054                   0.000002   \n",
       "believe                  0.000014         0.000050                   0.000002   \n",
       "egocentric               0.000013         0.000051                   0.000002   \n",
       "allocentric              0.000013         0.000051                   0.000002   \n",
       "\n",
       "                ['wczorajszym', 'poście', 'innymi']  \\\n",
       "innocent                               4.845665e-07   \n",
       "sage                                   3.600215e-07   \n",
       "explorer                               3.269538e-07   \n",
       "outlaw                                 3.827676e-07   \n",
       "magician                               3.768889e-07   \n",
       "hero                                   3.792890e-07   \n",
       "lover                                  5.350438e-07   \n",
       "jester                                 3.576034e-07   \n",
       "everyman                               4.506976e-07   \n",
       "caregiver                              3.379707e-07   \n",
       "ruler                                  4.015145e-07   \n",
       "creator                                3.704289e-07   \n",
       "dominant                               0.000000e+00   \n",
       "submissive                             0.000000e+00   \n",
       "maximalist                             0.000000e+00   \n",
       "minimalist                             0.000000e+00   \n",
       "inspiring                              0.000000e+00   \n",
       "systematic                             0.000000e+00   \n",
       "discovering                            0.000000e+00   \n",
       "conservative                           0.000000e+00   \n",
       "verifying                              0.000000e+00   \n",
       "overlooking                            0.000000e+00   \n",
       "sharpening                             0.000000e+00   \n",
       "harmonic                               0.000000e+00   \n",
       "empathic                               0.000000e+00   \n",
       "matter_of_fact                         3.915640e-07   \n",
       "brave                                  4.479120e-07   \n",
       "protective                             0.000000e+00   \n",
       "generous                               0.000000e+00   \n",
       "thrifty                                0.000000e+00   \n",
       "favourable                             0.000000e+00   \n",
       "balanced                               0.000000e+00   \n",
       "sensuality                             0.000000e+00   \n",
       "intelligent                            0.000000e+00   \n",
       "believe                                4.193610e-07   \n",
       "egocentric                             0.000000e+00   \n",
       "allocentric                            0.000000e+00   \n",
       "\n",
       "                ['wczorajszym', 'naprawdę']  ['wczorajszym', 'przepis']  \\\n",
       "innocent                           0.000002                    0.000002   \n",
       "sage                               0.000002                    0.000001   \n",
       "explorer                           0.000001                    0.000000   \n",
       "outlaw                             0.000002                    0.000000   \n",
       "magician                           0.000002                    0.000000   \n",
       "hero                               0.000002                    0.000000   \n",
       "lover                              0.000000                    0.000000   \n",
       "jester                             0.000002                    0.000000   \n",
       "everyman                           0.000002                    0.000002   \n",
       "caregiver                          0.000001                    0.000000   \n",
       "ruler                              0.000002                    0.000000   \n",
       "creator                            0.000002                    0.000000   \n",
       "dominant                           0.000000                    0.000002   \n",
       "submissive                         0.000000                    0.000002   \n",
       "maximalist                         0.000000                    0.000002   \n",
       "minimalist                         0.000000                    0.000002   \n",
       "inspiring                          0.000000                    0.000002   \n",
       "systematic                         0.000000                    0.000002   \n",
       "discovering                        0.000000                    0.000002   \n",
       "conservative                       0.000000                    0.000002   \n",
       "verifying                          0.000000                    0.000002   \n",
       "overlooking                        0.000000                    0.000002   \n",
       "sharpening                         0.000000                    0.000002   \n",
       "harmonic                           0.000000                    0.000002   \n",
       "empathic                           0.000000                    0.000002   \n",
       "matter_of_fact                     0.000001                    0.000002   \n",
       "brave                              0.000002                    0.000002   \n",
       "protective                         0.000000                    0.000002   \n",
       "generous                           0.000002                    0.000002   \n",
       "thrifty                            0.000002                    0.000002   \n",
       "favourable                         0.000000                    0.000002   \n",
       "balanced                           0.000000                    0.000002   \n",
       "sensuality                         0.000000                    0.000002   \n",
       "intelligent                        0.000002                    0.000002   \n",
       "believe                            0.000002                    0.000002   \n",
       "egocentric                         0.000000                    0.000002   \n",
       "allocentric                        0.000000                    0.000002   \n",
       "\n",
       "                ['wczorajszym', 'innymi']  ['odnośnie']  \\\n",
       "innocent                         0.000002      0.000081   \n",
       "sage                             0.000002      0.000113   \n",
       "explorer                         0.000002      0.000088   \n",
       "outlaw                           0.000002      0.000096   \n",
       "magician                         0.000002      0.000100   \n",
       "hero                             0.000002      0.000081   \n",
       "lover                            0.000002      0.000085   \n",
       "jester                           0.000002      0.000092   \n",
       "everyman                         0.000002      0.000093   \n",
       "caregiver                        0.000002      0.000106   \n",
       "ruler                            0.000002      0.000100   \n",
       "creator                          0.000002      0.000091   \n",
       "dominant                         0.000000      0.000096   \n",
       "submissive                       0.000000      0.000094   \n",
       "maximalist                       0.000000      0.000091   \n",
       "minimalist                       0.000000      0.000096   \n",
       "inspiring                        0.000000      0.000093   \n",
       "systematic                       0.000000      0.000085   \n",
       "discovering                      0.000000      0.000092   \n",
       "conservative                     0.000000      0.000097   \n",
       "verifying                        0.000000      0.000091   \n",
       "overlooking                      0.000000      0.000099   \n",
       "sharpening                       0.000000      0.000098   \n",
       "harmonic                         0.000000      0.000092   \n",
       "empathic                         0.000000      0.000093   \n",
       "matter_of_fact                   0.000001      0.000088   \n",
       "brave                            0.000002      0.000096   \n",
       "protective                       0.000000      0.000090   \n",
       "generous                         0.000000      0.000075   \n",
       "thrifty                          0.000000      0.000086   \n",
       "favourable                       0.000000      0.000090   \n",
       "balanced                         0.000000      0.000094   \n",
       "sensuality                       0.000000      0.000085   \n",
       "intelligent                      0.000000      0.000081   \n",
       "believe                          0.000002      0.000095   \n",
       "egocentric                       0.000000      0.000094   \n",
       "allocentric                      0.000000      0.000090   \n",
       "\n",
       "                ['odnośnie', 'życia']  ['odnośnie', 'naprawdę']  ...  \\\n",
       "innocent                     0.000002                  0.000003  ...   \n",
       "sage                         0.000003                  0.000002  ...   \n",
       "explorer                     0.000002                  0.000003  ...   \n",
       "outlaw                       0.000003                  0.000003  ...   \n",
       "magician                     0.000003                  0.000003  ...   \n",
       "hero                         0.000003                  0.000002  ...   \n",
       "lover                        0.000003                  0.000003  ...   \n",
       "jester                       0.000003                  0.000003  ...   \n",
       "everyman                     0.000003                  0.000003  ...   \n",
       "caregiver                    0.000003                  0.000003  ...   \n",
       "ruler                        0.000003                  0.000003  ...   \n",
       "creator                      0.000003                  0.000003  ...   \n",
       "dominant                     0.000003                  0.000003  ...   \n",
       "submissive                   0.000003                  0.000003  ...   \n",
       "maximalist                   0.000003                  0.000003  ...   \n",
       "minimalist                   0.000003                  0.000003  ...   \n",
       "inspiring                    0.000003                  0.000003  ...   \n",
       "systematic                   0.000003                  0.000000  ...   \n",
       "discovering                  0.000003                  0.000002  ...   \n",
       "conservative                 0.000003                  0.000003  ...   \n",
       "verifying                    0.000003                  0.000003  ...   \n",
       "overlooking                  0.000003                  0.000003  ...   \n",
       "sharpening                   0.000003                  0.000003  ...   \n",
       "harmonic                     0.000003                  0.000003  ...   \n",
       "empathic                     0.000003                  0.000003  ...   \n",
       "matter_of_fact               0.000002                  0.000002  ...   \n",
       "brave                        0.000002                  0.000002  ...   \n",
       "protective                   0.000002                  0.000000  ...   \n",
       "generous                     0.000002                  0.000000  ...   \n",
       "thrifty                      0.000002                  0.000002  ...   \n",
       "favourable                   0.000003                  0.000000  ...   \n",
       "balanced                     0.000003                  0.000003  ...   \n",
       "sensuality                   0.000003                  0.000000  ...   \n",
       "intelligent                  0.000002                  0.000000  ...   \n",
       "believe                      0.000003                  0.000002  ...   \n",
       "egocentric                   0.000003                  0.000003  ...   \n",
       "allocentric                  0.000003                  0.000003  ...   \n",
       "\n",
       "                ['podkład', 'serii']  ['konkursu', 'udział', 'nagrody']  \\\n",
       "innocent                    0.000000                       0.000000e+00   \n",
       "sage                        0.000000                       0.000000e+00   \n",
       "explorer                    0.000000                       0.000000e+00   \n",
       "outlaw                      0.000000                       0.000000e+00   \n",
       "magician                    0.000000                       0.000000e+00   \n",
       "hero                        0.000000                       0.000000e+00   \n",
       "lover                       0.000000                       0.000000e+00   \n",
       "jester                      0.000000                       0.000000e+00   \n",
       "everyman                    0.000000                       0.000000e+00   \n",
       "caregiver                   0.000000                       0.000000e+00   \n",
       "ruler                       0.000000                       0.000000e+00   \n",
       "creator                     0.000000                       0.000000e+00   \n",
       "dominant                    0.000000                       0.000000e+00   \n",
       "submissive                  0.000000                       0.000000e+00   \n",
       "maximalist                  0.000000                       0.000000e+00   \n",
       "minimalist                  0.000000                       0.000000e+00   \n",
       "inspiring                   0.000000                       0.000000e+00   \n",
       "systematic                  0.000000                       0.000000e+00   \n",
       "discovering                 0.000000                       0.000000e+00   \n",
       "conservative                0.000000                       0.000000e+00   \n",
       "verifying                   0.000000                       0.000000e+00   \n",
       "overlooking                 0.000000                       0.000000e+00   \n",
       "sharpening                  0.000000                       0.000000e+00   \n",
       "harmonic                    0.000000                       0.000000e+00   \n",
       "empathic                    0.000000                       0.000000e+00   \n",
       "matter_of_fact              0.000000                       0.000000e+00   \n",
       "brave                       0.000000                       0.000000e+00   \n",
       "protective                  0.000000                       0.000000e+00   \n",
       "generous                    0.000000                       0.000000e+00   \n",
       "thrifty                     0.000000                       0.000000e+00   \n",
       "favourable                  0.000000                       0.000000e+00   \n",
       "balanced                    0.000000                       0.000000e+00   \n",
       "sensuality                  0.000000                       0.000000e+00   \n",
       "intelligent                 0.000000                       0.000000e+00   \n",
       "believe                     0.000002                       4.193610e-07   \n",
       "egocentric                  0.000000                       0.000000e+00   \n",
       "allocentric                 0.000000                       0.000000e+00   \n",
       "\n",
       "                ['this', 'hope', 'you']  ['zadowolony', 'temu']  \\\n",
       "innocent                   0.000000e+00                0.000000   \n",
       "sage                       0.000000e+00                0.000000   \n",
       "explorer                   0.000000e+00                0.000000   \n",
       "outlaw                     0.000000e+00                0.000000   \n",
       "magician                   0.000000e+00                0.000000   \n",
       "hero                       0.000000e+00                0.000000   \n",
       "lover                      0.000000e+00                0.000000   \n",
       "jester                     0.000000e+00                0.000000   \n",
       "everyman                   0.000000e+00                0.000000   \n",
       "caregiver                  0.000000e+00                0.000000   \n",
       "ruler                      0.000000e+00                0.000000   \n",
       "creator                    0.000000e+00                0.000000   \n",
       "dominant                   0.000000e+00                0.000000   \n",
       "submissive                 0.000000e+00                0.000000   \n",
       "maximalist                 0.000000e+00                0.000000   \n",
       "minimalist                 0.000000e+00                0.000000   \n",
       "inspiring                  0.000000e+00                0.000000   \n",
       "systematic                 0.000000e+00                0.000000   \n",
       "discovering                0.000000e+00                0.000000   \n",
       "conservative               0.000000e+00                0.000000   \n",
       "verifying                  0.000000e+00                0.000000   \n",
       "overlooking                0.000000e+00                0.000000   \n",
       "sharpening                 0.000000e+00                0.000000   \n",
       "harmonic                   0.000000e+00                0.000000   \n",
       "empathic                   0.000000e+00                0.000000   \n",
       "matter_of_fact             0.000000e+00                0.000000   \n",
       "brave                      0.000000e+00                0.000000   \n",
       "protective                 0.000000e+00                0.000000   \n",
       "generous                   0.000000e+00                0.000000   \n",
       "thrifty                    0.000000e+00                0.000000   \n",
       "favourable                 0.000000e+00                0.000000   \n",
       "balanced                   0.000000e+00                0.000000   \n",
       "sensuality                 0.000000e+00                0.000000   \n",
       "intelligent                0.000000e+00                0.000000   \n",
       "believe                    4.193610e-07                0.000002   \n",
       "egocentric                 0.000000e+00                0.000000   \n",
       "allocentric                0.000000e+00                0.000000   \n",
       "\n",
       "                ['przeczytaniu', 'czytając']  ['potoczą', 'czekam']  \\\n",
       "innocent                            0.000000               0.000000   \n",
       "sage                                0.000000               0.000000   \n",
       "explorer                            0.000000               0.000000   \n",
       "outlaw                              0.000000               0.000000   \n",
       "magician                            0.000000               0.000000   \n",
       "hero                                0.000000               0.000000   \n",
       "lover                               0.000000               0.000000   \n",
       "jester                              0.000000               0.000000   \n",
       "everyman                            0.000000               0.000000   \n",
       "caregiver                           0.000000               0.000000   \n",
       "ruler                               0.000000               0.000000   \n",
       "creator                             0.000000               0.000000   \n",
       "dominant                            0.000000               0.000000   \n",
       "submissive                          0.000000               0.000000   \n",
       "maximalist                          0.000000               0.000000   \n",
       "minimalist                          0.000000               0.000000   \n",
       "inspiring                           0.000000               0.000000   \n",
       "systematic                          0.000000               0.000000   \n",
       "discovering                         0.000000               0.000000   \n",
       "conservative                        0.000000               0.000000   \n",
       "verifying                           0.000000               0.000000   \n",
       "overlooking                         0.000000               0.000000   \n",
       "sharpening                          0.000000               0.000000   \n",
       "harmonic                            0.000000               0.000000   \n",
       "empathic                            0.000000               0.000000   \n",
       "matter_of_fact                      0.000000               0.000000   \n",
       "brave                               0.000000               0.000000   \n",
       "protective                          0.000000               0.000000   \n",
       "generous                            0.000000               0.000000   \n",
       "thrifty                             0.000000               0.000000   \n",
       "favourable                          0.000000               0.000000   \n",
       "balanced                            0.000000               0.000000   \n",
       "sensuality                          0.000000               0.000000   \n",
       "intelligent                         0.000000               0.000000   \n",
       "believe                             0.000002               0.000002   \n",
       "egocentric                          0.000000               0.000000   \n",
       "allocentric                         0.000000               0.000000   \n",
       "\n",
       "                ['help', 'and', 'with']  ['noszenia', 'maseczek']  \\\n",
       "innocent                   0.000000e+00                  0.000000   \n",
       "sage                       0.000000e+00                  0.000000   \n",
       "explorer                   0.000000e+00                  0.000000   \n",
       "outlaw                     0.000000e+00                  0.000000   \n",
       "magician                   0.000000e+00                  0.000000   \n",
       "hero                       0.000000e+00                  0.000000   \n",
       "lover                      0.000000e+00                  0.000000   \n",
       "jester                     0.000000e+00                  0.000000   \n",
       "everyman                   0.000000e+00                  0.000000   \n",
       "caregiver                  0.000000e+00                  0.000000   \n",
       "ruler                      0.000000e+00                  0.000000   \n",
       "creator                    0.000000e+00                  0.000000   \n",
       "dominant                   0.000000e+00                  0.000000   \n",
       "submissive                 0.000000e+00                  0.000000   \n",
       "maximalist                 0.000000e+00                  0.000000   \n",
       "minimalist                 0.000000e+00                  0.000000   \n",
       "inspiring                  0.000000e+00                  0.000000   \n",
       "systematic                 0.000000e+00                  0.000000   \n",
       "discovering                0.000000e+00                  0.000000   \n",
       "conservative               0.000000e+00                  0.000000   \n",
       "verifying                  0.000000e+00                  0.000000   \n",
       "overlooking                0.000000e+00                  0.000000   \n",
       "sharpening                 0.000000e+00                  0.000000   \n",
       "harmonic                   0.000000e+00                  0.000000   \n",
       "empathic                   0.000000e+00                  0.000000   \n",
       "matter_of_fact             0.000000e+00                  0.000000   \n",
       "brave                      0.000000e+00                  0.000000   \n",
       "protective                 0.000000e+00                  0.000000   \n",
       "generous                   0.000000e+00                  0.000000   \n",
       "thrifty                    0.000000e+00                  0.000000   \n",
       "favourable                 0.000000e+00                  0.000000   \n",
       "balanced                   0.000000e+00                  0.000000   \n",
       "sensuality                 0.000000e+00                  0.000000   \n",
       "intelligent                0.000000e+00                  0.000000   \n",
       "believe                    4.193610e-07                  0.000002   \n",
       "egocentric                 0.000000e+00                  0.000000   \n",
       "allocentric                0.000000e+00                  0.000000   \n",
       "\n",
       "                ['best', 'much']  ['care', 'it']  \n",
       "innocent                0.000000        0.000000  \n",
       "sage                    0.000000        0.000000  \n",
       "explorer                0.000000        0.000000  \n",
       "outlaw                  0.000000        0.000000  \n",
       "magician                0.000000        0.000000  \n",
       "hero                    0.000000        0.000000  \n",
       "lover                   0.000000        0.000000  \n",
       "jester                  0.000000        0.000000  \n",
       "everyman                0.000000        0.000000  \n",
       "caregiver               0.000000        0.000000  \n",
       "ruler                   0.000000        0.000000  \n",
       "creator                 0.000000        0.000000  \n",
       "dominant                0.000000        0.000000  \n",
       "submissive              0.000000        0.000000  \n",
       "maximalist              0.000000        0.000000  \n",
       "minimalist              0.000000        0.000000  \n",
       "inspiring               0.000000        0.000000  \n",
       "systematic              0.000000        0.000000  \n",
       "discovering             0.000000        0.000000  \n",
       "conservative            0.000000        0.000000  \n",
       "verifying               0.000000        0.000000  \n",
       "overlooking             0.000000        0.000000  \n",
       "sharpening              0.000000        0.000000  \n",
       "harmonic                0.000000        0.000000  \n",
       "empathic                0.000000        0.000000  \n",
       "matter_of_fact          0.000000        0.000000  \n",
       "brave                   0.000000        0.000000  \n",
       "protective              0.000000        0.000000  \n",
       "generous                0.000000        0.000000  \n",
       "thrifty                 0.000000        0.000000  \n",
       "favourable              0.000000        0.000000  \n",
       "balanced                0.000000        0.000000  \n",
       "sensuality              0.000000        0.000000  \n",
       "intelligent             0.000000        0.000000  \n",
       "believe                 0.000002        0.000002  \n",
       "egocentric              0.000000        0.000000  \n",
       "allocentric             0.000000        0.000000  \n",
       "\n",
       "[37 rows x 4745869 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the resulting DataFrame\n",
    "word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cf89527-553f-4c26-8880-64e2b86e84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all NaN with 0.0\n",
    "word_df = word_df.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21c13154-632f-413b-aac8-b228bd4e119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save non-NaN word_df to file\n",
    "word_df.to_pickle(\"word_trait_array_no_nan.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8cefb12-2f38-45da-af10-11aa81a4f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a pickle\n",
    "import pickle\n",
    "\n",
    "with open(\"influencer_index_map.pickle\", \"wb\") as f:\n",
    "    pickle.dump(user_indices, f)\n",
    "    \n",
    "word_df.to_pickle(\"word_frequency_table.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "679da2ad-180c-4e09-95e7-1ff94de16431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for creating an output vector for dot product calculation\n",
    "# Word map - to easily create output vectors\n",
    "word_map = word_df.columns.tolist()\n",
    "\n",
    "def get_trait_dot_product(post_text: str, word_map: list, word_dataframe: pd.DataFrame) -> list:\n",
    "    # Filter out the text\n",
    "    filtered_post = remove_stopwords(clean_up_text(post_text))\n",
    "    \n",
    "    filtered_post = [\" \".join(pst) for pst in filtered_post]\n",
    "    filtered_post.extend(extract_hashtags(post_text))\n",
    "    \n",
    "    # Create a vector for dot product vector\n",
    "    post_vector = [0] * len(word_map)\n",
    "    \n",
    "    # Calculate word occurrences\n",
    "    spmf = Spmf(\"PrefixSpan\", input_direct=filtered_post,\n",
    "                output_filename=\"output.txt\", arguments=[0.00025, 3], input_type=\"text\")\n",
    "    spmf.run()\n",
    "    spmf_df = spmf.to_pandas_dataframe(pickle=False)\n",
    "    \n",
    "    for idx, row in tqdm(spmf_df.iterrows()):\n",
    "        phrase = str(row[\"pattern\"])\n",
    "        freq = int(row[\"sup\"])\n",
    "        try:\n",
    "            post_vector[word_map.index(phrase)] = freq\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Calculate dot product for a given text\n",
    "    word_dot = word_dataframe.dot(post_vector)\n",
    "    return word_dot.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a38b7660-1342-4b4a-afdc-d15d0c8734c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 2 ms\n",
      " Frequent sequences count : 85\n",
      " Max memory (mb) : 8.991737365722656\n",
      " minsup = 1 sequences.\n",
      " Pattern count : 85\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:07, 12.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00016620354740696497, 0.00012749299512918806, 0.0001690490664915492, 0.00014584830353522058, 0.0001594157228390828, 0.00015536049182692842, 0.00018260170915199766, 0.00016443368012853234, 0.00016633483706746642, 0.00010936498436992098, 0.00015111084902183367, 0.00016939103921402557, 0.00017278599281551573, 0.0001591613036986155, 0.00017426378862227474, 0.00016502322112468682, 0.0001605453602390891, 0.0001723152627698697, 0.0001679472645589285, 0.00016507620641309755, 0.00016276248092296895, 0.00015667617021652205, 0.00016640781007321942, 0.00016311498103788345, 0.00015501571392168522, 0.00015649754213106778, 0.00016995963458678562, 0.00016629475070394366, 0.00018173183166301588, 0.00018088433398864235, 0.00015980665583775904, 0.00016073035875016072, 0.00016210352180292368, 0.00014350258416762895, 0.00015313004569130333, 0.00017351924885445163, 0.00015830908779736435]\n"
     ]
    }
   ],
   "source": [
    "# Test the trait dot_product\n",
    "print(get_trait_dot_product(\"Cześć reasda  asdasda         #hello #man\", word_map, word_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e3a8543-ded4-4807-8999-d9c68c4910ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for calculating the dot product of trait <-> influencer relation\n",
    "def get_influencer_dot_product(trait_output: list, influencer_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    return influencer_dataframe.dot(trait_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2189d6d6-5ecd-4898-a47d-d849e61db033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for calculating the similarity\n",
    "def calculate_similarity(post_text: str, \n",
    "                         word_map: list, \n",
    "                         word_dataframe: pd.DataFrame,\n",
    "                         influencer_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Calculate word-trait dot product\n",
    "    post_result = get_trait_dot_product(post_text, word_map, word_dataframe)\n",
    "    \n",
    "    # Calculate trate-influencer dot-product\n",
    "    return get_influencer_dot_product(post_result, influencer_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae883b94-1542-4a70-9368-d97bc5690edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/media/maciek/HDD_Linux/Praca_magisterska/instagram_analysis/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 14 ms\n",
      " Frequent sequences count : 2445\n",
      " Max memory (mb) : 10.53741455078125\n",
      " minsup = 1 sequences.\n",
      " Pattern count : 2445\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2445it [03:20, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum similarity:\n",
      "User: muzykujemy\n",
      "Similarity score: 6.420715563113335e-05\n"
     ]
    }
   ],
   "source": [
    "# Test the method\n",
    "sim_df = calculate_similarity(\"\"\"Jak to jest być skrybą, dobrze? \n",
    "A, wie pan, moim zdaniem to nie ma tak, że dobrze, albo że niedobrze. \n",
    "Gdybym miał powiedzieć, co cenię w życiu najbardziej, powiedziałbym, że ludzi. \n",
    "Ludzi, którzy podali mi pomocną dłoń, kiedy sobie nie radziłem, kiedy byłem sam, i co ciekawe, to właśnie przypadkowe spotkania wpływają na nasze życie. \n",
    "Chodzi o to, że kiedy wyznaje się pewne wartości, nawet pozornie uniwersalne, bywa, że nie znajduje się zrozumienia, \n",
    "które by tak rzec, które pomaga się nam rozwijać. \n",
    "Ja miałem szczęście, by tak rzec, ponieważ je znalazłem, i dziękuję życiu! \n",
    "Dziękuję mu; życie to śpiew, życie to taniec, życie to miłość! \n",
    "Wielu ludzi pyta mnie o to samo: ale jak ty to robisz, skąd czerpiesz tę radość? \n",
    "A ja odpowiadam, że to proste! To umiłowanie życia. \n",
    "To właśnie ono sprawia, że dzisiaj na przykład buduję maszyny, a jutro – kto wie? \n",
    "Dlaczego by nie – oddam się pracy społecznej i będę, ot, choćby, sadzić... doć— m-marchew...\"\"\", word_map, word_df, available_arch_df)\n",
    "print(\"Maximum similarity:\\n\"\n",
    "        f\"User: {sim_df.idxmax()}\\n\"\n",
    "        f\"Similarity score: {sim_df.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac64c951-ea72-47ab-9b15-ce071441cbb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751cdee-37a1-4f5e-9734-bac89a4517e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
