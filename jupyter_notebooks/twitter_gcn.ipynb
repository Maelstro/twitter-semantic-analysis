{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolutional networks for Tweet archetype classification\n",
    "# Maciej W√≥jcik\n",
    "\n",
    "# Dependencies\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "def save_to_pickle(obj, file_name):\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def fcn_stub(stub):\n",
    "    return stub\n",
    "\n",
    "# Create a stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Function for stemming and lemmatization\n",
    "def stem_and_lemmatize(text:str) -> str:\n",
    "    \"\"\"Stems and lemmatizes a given text.\"\"\"\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess_texts(text_list: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Processes text to remove all unwanted words and symbols.\"\"\"\n",
    "\n",
    "    # Lowercase the tweets\n",
    "    text_list['processed_tweet'] = text_list['tweet_text'].str.lower()\n",
    "\n",
    "    # Regex patterns\n",
    "    url_pattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    user_pattern       = '@[^\\s]+'\n",
    "    alpha_pattern      = \"[^a-zA-Z]\"\n",
    "    sequence_pattern   = r\"(.)\\1\\1+\"\n",
    "    seq_replace_pattern = r\"\\1\\1\"\n",
    "\n",
    "    # Remove URLs from the tweet text\n",
    "    text_list['processed_tweet'] = [re.sub(url_pattern, ' ', str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "    # Remove username from the tweet text\n",
    "    text_list['processed_tweet'] = [re.sub(user_pattern, ' ', str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "    # Remove all non-alphanumeric symbols\n",
    "    text_list['processed_tweet'] = [re.sub(alpha_pattern, ' ', str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "    # Replace all 3 or more consecutive letters with 2 letters\n",
    "    text_list['processed_tweet'] = [re.sub(sequence_pattern, seq_replace_pattern, str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "\n",
    "    \n",
    "    full_tweet_list = []\n",
    "    for x in text_list['processed_tweet']:\n",
    "        full_tweet = ''\n",
    "        for word in x.split():\n",
    "            word = stem_and_lemmatize(word)\n",
    "            full_tweet += (word + ' ')\n",
    "        full_tweet_list.append(full_tweet)\n",
    "\n",
    "    text_list['processed_tweet'] = full_tweet_list\n",
    "\n",
    "    return text_list\n",
    "\n",
    "def filter_tokens(tokens):\n",
    "    tokens1 = []\n",
    "    for token in tokens:\n",
    "        if (token not in [\".\",\",\",\";\",\"&\",\"'s\", \":\", \"?\", \"!\",\"(\",\")\",\\\n",
    "            \"'\",\"'m\",\"'no\",\"***\",\"--\",\"...\",\"[\",\"]\", \" \"]):\n",
    "            tokens1.append(token)\n",
    "    return tokens1\n",
    "\n",
    "def word_word_edges(p_ij):\n",
    "    word_word = []\n",
    "    cols = list(p_ij.columns)\n",
    "    cols = [str(w) for w in cols]\n",
    "    \n",
    "    for w1, w2 in tqdm(combinations(cols, 2), total=nCr(len(cols), 2)):\n",
    "        if (p_ij.loc[w1,w2] > 0):\n",
    "            word_word.append((w1,w2,{\"weight\":p_ij.loc[w1,w2]}))\n",
    "    return word_word\n",
    "\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return int(f(n)/(f(r)*f(n-r)))\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>username</th>\n",
       "      <th>created_at</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>archetype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5f9f1c36b38e10f823bf2cef</td>\n",
       "      <td>@eliostruyf So exciting, have fun!¬†üòä</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-30 18:23:50.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5f9f1c36b38e10f823bf2ce7</td>\n",
       "      <td>These Brick-O-Lanterns are certainly all treat...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-31 09:00:28.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5f9f1c36b38e10f823bf2d0a</td>\n",
       "      <td>@dentistescabri Nous prenons la s√©curit√© de no...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-30 12:07:58.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5f9f1c36b38e10f823bf2cf5</td>\n",
       "      <td>@Jasmin80212446 üòçüéÑü•∞</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-30 16:35:39.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5f9f1c36b38e10f823bf2d07</td>\n",
       "      <td>@ashleydrixey Sounds like a perfect fit for th...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-30 13:09:14.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       _id  \\\n",
       "0           0  5f9f1c36b38e10f823bf2cef   \n",
       "1           1  5f9f1c36b38e10f823bf2ce7   \n",
       "2           2  5f9f1c36b38e10f823bf2d0a   \n",
       "3           3  5f9f1c36b38e10f823bf2cf5   \n",
       "4           4  5f9f1c36b38e10f823bf2d07   \n",
       "\n",
       "                                          tweet_text    username  \\\n",
       "0               @eliostruyf So exciting, have fun!¬†üòä  LEGO_Group   \n",
       "1  These Brick-O-Lanterns are certainly all treat...  LEGO_Group   \n",
       "2  @dentistescabri Nous prenons la s√©curit√© de no...  LEGO_Group   \n",
       "3                                @Jasmin80212446 üòçüéÑü•∞  LEGO_Group   \n",
       "4  @ashleydrixey Sounds like a perfect fit for th...  LEGO_Group   \n",
       "\n",
       "                created_at timestamp archetype  \n",
       "0  2020-10-30 18:23:50.000       NaN    artist  \n",
       "1  2020-10-31 09:00:28.000       NaN    artist  \n",
       "2  2020-10-30 12:07:58.000       NaN    artist  \n",
       "3  2020-10-30 16:35:39.000       NaN    artist  \n",
       "4  2020-10-30 13:09:14.000       NaN    artist  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and process dataset\n",
    "text_df = pd.read_csv('twitter_database.csv')\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text and drop empty fields\n",
    "text_df = preprocess_texts(text_df)\n",
    "text_df = text_df.groupby('archetype').head(1000)\n",
    "save_to_pickle(text_df, \"unprocessed_tweets_df.pickle\")\n",
    "print(len(text_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maelstro/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>aa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aarchi</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aayush</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbi</th>\n",
       "      <th>...</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zu</th>\n",
       "      <th>zukunft</th>\n",
       "      <th>zukunftssicherung</th>\n",
       "      <th>zum</th>\n",
       "      <th>zur</th>\n",
       "      <th>zwei</th>\n",
       "      <th>zyciora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.199264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.177881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.209317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.292803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.221512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 11727 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          a   aa  aah    aarchi     aaron  aayush   ab   abandon  abbey  abbi  \\\n",
       "0  0.199264  0.0  0.0  0.000000  0.000000     0.0  0.0  0.000000    0.0   0.0   \n",
       "1  0.177881  0.0  0.0  0.000000  0.000000     0.0  0.0  0.000000    0.0   0.0   \n",
       "2  0.209317  0.0  0.0  0.000000  0.002159     0.0  0.0  0.000000    0.0   0.0   \n",
       "3  0.292803  0.0  0.0  0.000000  0.000857     0.0  0.0  0.007835    0.0   0.0   \n",
       "4  0.221512  0.0  0.0  0.001365  0.000000     0.0  0.0  0.000000    0.0   0.0   \n",
       "\n",
       "   ...     zombi      zone      zoom   zu  zukunft  zukunftssicherung  zum  \\\n",
       "0  ...  0.000000  0.004686  0.000000  0.0      0.0                0.0  0.0   \n",
       "1  ...  0.000000  0.000000  0.000000  0.0      0.0                0.0  0.0   \n",
       "2  ...  0.000000  0.000000  0.000000  0.0      0.0                0.0  0.0   \n",
       "3  ...  0.001306  0.000939  0.000939  0.0      0.0                0.0  0.0   \n",
       "4  ...  0.002345  0.000000  0.000000  0.0      0.0                0.0  0.0   \n",
       "\n",
       "   zur  zwei  zyciora  \n",
       "0  0.0   0.0      0.0  \n",
       "1  0.0   0.0      0.0  \n",
       "2  0.0   0.0      0.0  \n",
       "3  0.0   0.0      0.0  \n",
       "4  0.0   0.0      0.0  \n",
       "\n",
       "[5 rows x 11727 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the words\n",
    "df_ta = pd.DataFrame(columns=[\"processed_tweet\", \"archetype\"])\n",
    "for arch in text_df[\"archetype\"].unique():\n",
    "    dummy = pd.DataFrame(columns=[\"processed_tweet\", \"archetype\"])\n",
    "    dummy[\"processed_tweet\"] = text_df[text_df[\"archetype\"] == arch].groupby(\"archetype\").apply(lambda x: (\" \".join(x[\"processed_tweet\"])).lower())\n",
    "    dummy[\"archetype\"] = arch\n",
    "    df_ta = pd.concat([df_ta, dummy], ignore_index=True)\n",
    "\n",
    "# Tokenize the dataframe\n",
    "df_ta['processed_tweet'] = df_ta['processed_tweet'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: filter_tokens(x))\n",
    "\n",
    "# Data vectorization\n",
    "vectorizer = TfidfVectorizer(input=\"content\", max_features=None, tokenizer=fcn_stub, preprocessor=fcn_stub)\n",
    "vectorizer.fit(df_ta['processed_tweet'])\n",
    "df_tfidf = vectorizer.transform(df_ta['processed_tweet'])\n",
    "df_tfidf = df_tfidf.toarray()\n",
    "\n",
    "# Get feature names\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocab = np.array(vocab)\n",
    "\n",
    "df_tfidf = pd.DataFrame(df_tfidf, columns=vocab)\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:09<00:00,  1.31it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11727/11727 [00:03<00:00, 3876.73it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11727/11727 [00:05<00:00, 2322.67it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11727/11727 [00:43<00:00, 267.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate PMI between words\n",
    "names = vocab\n",
    "name_idx = OrderedDict((name, 0) for name in names)\n",
    "word_to_index = OrderedDict((name, index) for index, name in enumerate(names))\n",
    "\n",
    "# Get the co-occurrences\n",
    "occurrences = np.zeros((len(names), len(names)), dtype=np.int32)\n",
    "windows_count = 0\n",
    "window = 10             # Sliding window size, for calculation PMI between words \n",
    "for l in tqdm(df_ta['processed_tweet'], total=len(df_ta['processed_tweet'])):\n",
    "    for i in range(len(l) - window):\n",
    "        windows_count += 1\n",
    "        d = set(l[i:(i+window)])\n",
    "        for w in d:\n",
    "            name_idx[w] += 1\n",
    "        for w1, w2 in combinations(d, 2):\n",
    "            i1 = word_to_index[w1]\n",
    "            i2 = word_to_index[w2]\n",
    "            \n",
    "            occurrences[i1][i2] = 1\n",
    "            occurrences[i2][i1] = 1\n",
    "            \n",
    "# Convert the occurences to PMI\n",
    "pmi_per_word = pd.DataFrame(occurrences, index=names, columns=names) / windows_count\n",
    "pmi_index = pd.Series(name_idx, index=name_idx.keys()) / windows_count\n",
    "\n",
    "# Free memory\n",
    "del occurrences\n",
    "del name_idx\n",
    "\n",
    "for col in tqdm(pmi_per_word.columns):\n",
    "    pmi_per_word[col] = pmi_per_word[col]/pmi_index[col]\n",
    "\n",
    "for row in tqdm(pmi_per_word.index):\n",
    "    pmi_per_word.loc[row, :] = pmi_per_word.loc[row, :] / pmi_index[row]\n",
    "    \n",
    "pmi_per_word = pmi_per_word + 1E-9\n",
    "for col in tqdm(pmi_per_word.columns):\n",
    "    pmi_per_word[col] = pmi_per_word[col].apply(lambda x: math.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:01<00:00,  9.07it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68755401/68755401 [07:06<00:00, 161123.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build a graph\n",
    "graph = nx.Graph()\n",
    "graph.add_nodes_from(df_tfidf.index)\n",
    "graph.add_nodes_from(vocab)\n",
    "\n",
    "# Build document-word edges\n",
    "document_word = [(doc,w,{\"weight\":df_tfidf.loc[doc,w]}) for doc in tqdm(df_tfidf.index, total=len(df_tfidf.index))\\\n",
    "                     for w in df_tfidf.columns]\n",
    "\n",
    "word_word = word_word_edges(pmi_per_word)\n",
    "graph.add_edges_from(document_word)\n",
    "graph.add_edges_from(word_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export every needed structure\n",
    "save_to_pickle(graph, \"text_graph.pickle\")\n",
    "save_to_pickle(df_ta, \"tweet_archetype_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph loaded.\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "with open('text_graph.pickle', \"rb\") as f:\n",
    "    graph = pickle.load(f)\n",
    "print(\"Graph loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11739/11739 [00:00<00:00, 1106174.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# GCN - implementation and training\n",
    "# Create A matrix and hat_A\n",
    "A = nx.to_numpy_matrix(graph, weight=\"weight\")\n",
    "A = A + np.eye(graph.number_of_nodes())\n",
    "\n",
    "degs = []\n",
    "for deg in tqdm(graph.degree(weight=None)):\n",
    "    if deg == 0:\n",
    "        degs.append(0)\n",
    "    else:\n",
    "        degs.append(deg[1]**(-0.5))\n",
    "degs = np.diag(degs)\n",
    "X = np.eye(graph.number_of_nodes())\n",
    "hat_A = np.matmul(np.matmul(degs, A), degs)\n",
    "inp = X  # Net input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 1496.94it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 6043.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing test indices: [235, 529, 803, 877, 757, 680, 711, 641, 844, 345, 685, 862, 92, 321, 247, 931, 698, 398, 33, 967, 947, 712, 390, 54, 762, 11, 676, 220, 719, 253, 102, 849, 705, 388, 888, 548, 258, 194, 653, 837, 909, 581, 266, 319, 150, 709, 556, 873, 268, 148, 51, 284, 690, 828, 941, 280, 897, 679, 567, 218, 186, 161, 219, 386, 182, 453, 983, 28, 571, 650, 442, 410, 107, 928, 996, 768, 976, 248, 75, 279, 568, 596, 720, 91, 933, 254, 240, 93, 945, 379, 501, 361, 522, 534, 106, 422, 260, 100, 800, 747, 1374, 1622, 1329, 1567, 1560, 1181, 1941, 1405, 1241, 1642, 1964, 1141, 1239, 1449, 1972, 1936, 1827, 1664, 1554, 1890, 1627, 1284, 1468, 1363, 1386, 1419, 1129, 1871, 1661, 1190, 1215, 1985, 1926, 1764, 1131, 1029, 1780, 1399, 1228, 1942, 1426, 1321, 1596, 1182, 1874, 1309, 1464, 1651, 1845, 1743, 1316, 1920, 1364, 1169, 1240, 1080, 1342, 1759, 1783, 1407, 1992, 1725, 1794, 1577, 1417, 1684, 1657, 1621, 1950, 1753, 1490, 1204, 1572, 1740, 1456, 1733, 1028, 1853, 1320, 1816, 1128, 1549, 1516, 1441, 1618, 1582, 1036, 1328, 1882, 1126, 1051, 1979, 1980, 1180, 1903, 1072, 1611, 1782, 1509, 1443, 2551, 2799, 2395, 2711, 2793, 2153, 2151, 2191, 2430, 2205, 2349, 2583, 2691, 2734, 2353, 2216, 2162, 2979, 2319, 2447, 2288, 2724, 2920, 2698, 2828, 2112, 2605, 2424, 2408, 2170, 2419, 2985, 2221, 2791, 2028, 2930, 2684, 2218, 2653, 2104, 2394, 2230, 2222, 2317, 2305, 2338, 2233, 2406, 2937, 2617, 2609, 2956, 2090, 2452, 2214, 2437, 2645, 2239, 2963, 2034, 2740, 2255, 2548, 2522, 2611, 2055, 2045, 2283, 2413, 2891, 2019, 2672, 2872, 2025, 2630, 2629, 2992, 2297, 2184, 2286, 2261, 2359, 2328, 2391, 2958, 2968, 2592, 2663, 2190, 2614, 2679, 2167, 2029, 2446, 2642, 2129, 2464, 2106, 2195, 2081, 3514, 3475, 3678, 3557, 3127, 3089, 3195, 3019, 3957, 3712, 3960, 3112, 3180, 3596, 3982, 3315, 3926, 3247, 3552, 3117, 3737, 3897, 3694, 3879, 3179, 3116, 3123, 3805, 3198, 3672, 3119, 3324, 3330, 3503, 3702, 3200, 3605, 3689, 3471, 3972, 3065, 3883, 3872, 3333, 3912, 3845, 3206, 3769, 3344, 3171, 3492, 3313, 3720, 3680, 3097, 3133, 3144, 3331, 3725, 3181, 3016, 3285, 3384, 3624, 3018, 3350, 3303, 3322, 3704, 3191, 3857, 3970, 3225, 3785, 3981, 3716, 3589, 3602, 3211, 3079, 3050, 3202, 3464, 3663, 3650, 3721, 3272, 3168, 3891, 3372, 3802, 3728, 3378, 3178, 3409, 3698, 3572, 3392, 3049, 3281, 4402, 4115, 4091, 4498, 4692, 4567, 4537, 4139, 4532, 4087, 4378, 4799, 4109, 4519, 4852, 4576, 4061, 4963, 4439, 4755, 4531, 4664, 4599, 4048, 4411, 4896, 4163, 4079, 4987, 4404, 4088, 4483, 4464, 4611, 4064, 4172, 4574, 4908, 4006, 4569, 4358, 4433, 4130, 4137, 4991, 4842, 4392, 4278, 4671, 4353, 4962, 4857, 4011, 4403, 4757, 4471, 4203, 4583, 4024, 4369, 4431, 4501, 4428, 4636, 4239, 4229, 4575, 4063, 4502, 4733, 4551, 4642, 4336, 4789, 4608, 4699, 4881, 4493, 4148, 4158, 4303, 4223, 4129, 4250, 4687, 4883, 4288, 4982, 4753, 4481, 4233, 4021, 4779, 4463, 4409, 4198, 4614, 4660, 4176, 4333, 5120, 5241, 5754, 5557, 5561, 5199, 5682, 5669, 5439, 5762, 5830, 5920, 5786, 5061, 5655, 5699, 5995, 5510, 5871, 5104, 5339, 5368, 5025, 5767, 5267, 5627, 5192, 5521, 5888, 5252, 5929, 5930, 5380, 5923, 5536, 5184, 5114, 5086, 5127, 5406, 5953, 5188, 5169, 5581, 5876, 5266, 5748, 5056, 5421, 5244, 5759, 5756, 5357, 5306, 5909, 5703, 5313, 5066, 5879, 5531, 5725, 5890, 5402, 5626, 5712, 5788, 5497, 5424, 5658, 5289, 5952, 5586, 5695, 5005, 5760, 5348, 5937, 5193, 5847, 5436, 5145, 5173, 5766, 5470, 5746, 5737, 5633, 5367, 5141, 5055, 5792, 5646, 5223, 5254, 5143, 5429, 5245, 5749, 5942, 5768, 6882, 6496, 6190, 6521, 6397, 6973, 6446, 6864, 6713, 6263, 6418, 6736, 6656, 6431, 6399, 6715, 6519, 6711, 6782, 6640, 6671, 6234, 6546, 6232, 6849, 6077, 6384, 6141, 6603, 6647, 6876, 6551, 6530, 6808, 6838, 6620, 6106, 6744, 6540, 6313, 6463, 6400, 6208, 6592, 6661, 6800, 6257, 6005, 6718, 6929, 6567, 6503, 6374, 6275, 6595, 6090, 6997, 6082, 6925, 6854, 6076, 6458, 6771, 6750, 6287, 6006, 6497, 6448, 6588, 6956, 6683, 6932, 6591, 6003, 6934, 6419, 6067, 6487, 6812, 6581, 6250, 6941, 6363, 6378, 6844, 6202, 6633, 6486, 6600, 6466, 6817, 6949, 6964, 6976, 6869, 6921, 6256, 6862, 6662, 6033, 7088, 7069, 7774, 7643, 7517, 7139, 7213, 7650, 7056, 7553, 7524, 7675, 7330, 7013, 7230, 7924, 7567, 7592, 7795, 7866, 7316, 7066, 7603, 7417, 7478, 7365, 7671, 7328, 7711, 7492, 7404, 7928, 7463, 7444, 7262, 7618, 7534, 7949, 7682, 7722, 7025, 7454, 7942, 7977, 7815, 7776, 7564, 7796, 7820, 7805, 7063, 7907, 7560, 7397, 7732, 7544, 7799, 7718, 7629, 7736, 7503, 7831, 7044, 7134, 7657, 7651, 7172, 7026, 7236, 7569, 7611, 7943, 7223, 7351, 7555, 7525, 7893, 7241, 7363, 7254, 7571, 7396, 7300, 7613, 7731, 7837, 7798, 7368, 7335, 7327, 7217, 7009, 7565, 7398, 7556, 7357, 7152, 7313, 7347, 7652, 8470, 8245, 8342, 8108, 8544, 8135, 8585, 8163, 8285, 8161, 8500, 8783, 8323, 8415, 8801, 8613, 8942, 8399, 8440, 8575, 8790, 8730, 8865, 8219, 8880, 8804, 8928, 8357, 8138, 8099, 8733, 8007, 8447, 8923, 8468, 8913, 8823, 8775, 8811, 8538, 8411, 8434, 8370, 8719, 8883, 8427, 8545, 8302, 8509, 8238, 8150, 8325, 8521, 8488, 8930, 8472, 8576, 8120, 8479, 8435, 8796, 8917, 8145, 8955, 8452, 8750, 8979, 8670, 8871, 8287, 8122, 8293, 8886, 8936, 8022, 8741, 8475, 8687, 8757, 8290, 8892, 8350, 8364, 8294, 8158, 8486, 8030, 8354, 8172, 8622, 8683, 8107, 8676, 8986, 8168, 8092, 8528, 8890, 8096, 8170, 9725, 9229, 9020, 9470, 9001, 9340, 9038, 9312, 9291, 9790, 9317, 9055, 9373, 9730, 9473, 9630, 9051, 9614, 9530, 9256, 9754, 9185, 9404, 9705, 9868, 9139, 9717, 9742, 9383, 9238, 9912, 9954, 9348, 9511, 9425, 9681, 9770, 9922, 9018, 9321, 9806, 9558, 9663, 9198, 9801, 9418, 9772, 9600, 9808, 9631, 9998, 9981, 9457, 9352, 9620, 9828, 9824, 9800, 9370, 9791, 9970, 9698, 9410, 9095, 9316, 9342, 9891, 9405, 9295, 9697, 9101, 9943, 9855, 9302, 9531, 9406, 9909, 9045, 9079, 9807, 9890, 9675, 9685, 9850, 9566, 9398, 9930, 9512, 9261, 9775, 9691, 9897, 9439, 9233, 9960, 9422, 9360, 9595, 9652, 9409, 10087, 10024, 10375, 10088, 10630, 10746, 10833, 10106, 10315, 10523, 10553, 10764, 10551, 10694, 10542, 10978, 10515, 10034, 10215, 10507, 10337, 10094, 10732, 10533, 10071, 10396, 10877, 10222, 10279, 10185, 10485, 10490, 10349, 10979, 10125, 10202, 10067, 10920, 10158, 10109, 10855, 10133, 10823, 10333, 10002, 10883, 10031, 10821, 10642, 10163, 10126, 10847, 10407, 10976, 10891, 10677, 10102, 10907, 10908, 10814, 10509, 10070, 10298, 10339, 10329, 10204, 10981, 10357, 10940, 10951, 10767, 10618, 10741, 10190, 10739, 10961, 10539, 10612, 10563, 10725, 10830, 10759, 10294, 10080, 10625, 10044, 10191, 10198, 10792, 10835, 10142, 10837, 10178, 10576, 10139, 10079, 10552, 10884, 10820, 10328, 11031, 11849, 11972, 11647, 11537, 11190, 11526, 11887, 11615, 11415, 11455, 11011, 11771, 11851, 11374, 11922, 11938, 11786, 11104, 11834, 11946, 11553, 11492, 11038, 11139, 11640, 11476, 11685, 11447, 11463, 11084, 11797, 11590, 11783, 11529, 11961, 11508, 11443, 11705, 11572, 11116, 11355, 11368, 11518, 11605, 11179, 11759, 11563, 11532, 11725, 11243, 11308, 11616, 11808, 11660, 11431, 11119, 11639, 11063, 11288, 11255, 11096, 11032, 11842, 11136, 11546, 11510, 11372, 11703, 11769, 11394, 11684, 11653, 11459, 11016, 11276, 11403, 11050, 11724, 11440, 11316, 11788, 11576, 11973, 11191, 11905, 11036, 11377, 11232, 11332, 11186, 11272, 11414, 11992, 11755, 11574, 11388, 11424, 11748, 11777]\n",
      "Finished selecting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the tweet pickle\n",
    "with open('unprocessed_tweets_df.pickle', \"rb\") as f:\n",
    "    df_tweet = pickle.load(f)\n",
    "\n",
    "archetype_dict = {'archetype': \n",
    "                  {'artist': 0,\n",
    "                  'caregiver': 1,\n",
    "                  'everyman': 2,\n",
    "                  'explorer': 3,\n",
    "                  'guru': 4,\n",
    "                  'hero': 5,\n",
    "                  'innocent': 6,\n",
    "                  'jester': 7,\n",
    "                  'magician': 8,\n",
    "                  'rebel': 9,\n",
    "                  'ruler': 10,\n",
    "                  'seducer':11}\n",
    "                 }\n",
    "\n",
    "df_tweet = df_tweet.replace(archetype_dict)\n",
    "df_tweet = df_tweet.reset_index()\n",
    "\n",
    "# Split the testing dataset\n",
    "test_indices = []\n",
    "for arch in tqdm(df_tweet[\"archetype\"].unique()):\n",
    "    tmp = df_tweet[df_tweet[\"archetype\"] == arch]\n",
    "    if len(tmp) >= 4:\n",
    "        test_indices.extend(list(np.random.choice(tmp.index, size=round(0.1*len(tmp)), replace=False)))\n",
    "print(f\"Finished processing test indices: {test_indices}\")\n",
    "\n",
    "selected = []\n",
    "for i in tqdm(range(len(df_ta))):\n",
    "    if i not in test_indices:\n",
    "        selected.append(i)\n",
    "print(\"Finished selecting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-c827cfd72680>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inp_selected = torch.tensor(inp_selected, device=torch.device('cuda'))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 11849 is out of bounds for axis 0 with size 11739",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c827cfd72680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minp_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlabels_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"archetype\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0minp_not_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0minp_not_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_not_selected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlabels_not_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"archetype\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 11849 is out of bounds for axis 0 with size 11739"
     ]
    }
   ],
   "source": [
    "# Save test indices and seleced ones\n",
    "save_to_pickle(test_indices, \"test_indices.pickle\")\n",
    "save_to_pickle(selected, \"selected.pickle\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Operations on selected inputs\n",
    "inp_selected = inp[selected]\n",
    "inp_selected = torch.from_numpy(inp_selected).float()\n",
    "inp_selected = torch.tensor(inp_selected, device=torch.device('cuda'))\n",
    "labels_selected = [l for idx, l in enumerate(df_tweet[\"archetype\"]) if idx in selected]\n",
    "inp_not_selected = inp[test_indices]\n",
    "inp_not_selected = torch.from_numpy(inp_not_selected).float()\n",
    "labels_not_selected = [l for idx, l in enumerate(df_tweet[\"archetype\"]) if idx not in selected]\n",
    "inp = torch.from_numpy(inp).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, X_size, A_hat, args, bias=True): # X_size = num features\n",
    "        super(GCN, self).__init__()\n",
    "        self.A_hat = torch.tensor(A_hat, requires_grad=False, device=device).float()\n",
    "        self.weight = nn.parameter.Parameter(torch.FloatTensor(X_size, args['hidden_size_1']))\n",
    "        var = 2./(self.weight.size(1)+self.weight.size(0))\n",
    "        self.weight.data.normal_(0,var)\n",
    "        self.weight2 = nn.parameter.Parameter(torch.FloatTensor(args['hidden_size_1'], args['hidden_size_2']))\n",
    "        var2 = 2./(self.weight2.size(1)+self.weight2.size(0))\n",
    "        self.weight2.data.normal_(0,var2)\n",
    "        if bias:\n",
    "            self.bias = nn.parameter.Parameter(torch.FloatTensor(args['hidden_size_1']))\n",
    "            self.bias.data.normal_(0,var)\n",
    "            self.bias2 = nn.parameter.Parameter(torch.FloatTensor(args['hidden_size_2']))\n",
    "            self.bias2.data.normal_(0,var2)\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.fc1 = nn.Linear(args['hidden_size_2'], args['num_classes'])\n",
    "        \n",
    "    def forward(self, X): ### 2-layer GCN architecture\n",
    "        X = torch.mm(X, self.weight)\n",
    "        if self.bias is not None:\n",
    "            X = (X + self.bias)\n",
    "        X = F.relu(torch.mm(self.A_hat, X))\n",
    "        X = torch.mm(X, self.weight2)\n",
    "        if self.bias2 is not None:\n",
    "            X = (X + self.bias2)\n",
    "        X = F.relu(torch.mm(self.A_hat, X))\n",
    "        return self.fc1(X)\n",
    "    \n",
    "def evaluate(output, labels_e):\n",
    "    _, labels = output.max(1); labels = labels.numpy()\n",
    "    return sum([(e-1) for e in labels_e] == labels)/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define additional arguments\n",
    "args = {\n",
    "    'hidden_size_2': 130, \n",
    "    'num_classes': 12, \n",
    "    'hidden_size_1': 330\n",
    "}\n",
    "\n",
    "net = GCN(X.shape[1], hat_A, args).to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1000,2000,3000,4000,5000,6000], gamma=0.77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_per_epoch, accuracy_per_epoch = [], []\n",
    "evaluation_trained = []\n",
    "best_pred = 0.0\n",
    "import os\n",
    "\n",
    "for e in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        inp_selected = inp_selected.to(device)\n",
    "        output = net(inp_selected)\n",
    "        loss = criterion(output[selected], torch.tensor(labels_selected).long())\n",
    "        losses_per_epoch.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if e % 50 == 0:\n",
    "            ### Evaluate other untrained nodes and check accuracy of labelling\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_labels = net(inp_selected)\n",
    "                trained_accuracy = evaluate(output[selected], labels_selected); \n",
    "                #untrained_accuracy = evaluate(pred_labels[test_indices], labels_not_selected)\n",
    "            evaluation_trained.append((e, trained_accuracy))\n",
    "            #evaluation_untrained.append((e, untrained_accuracy))\n",
    "            print(\"[Epoch %d]: Evaluation accuracy of trained nodes: %.7f\" % (e, trained_accuracy))\n",
    "            #print(\"[Epoch %d]: Evaluation accuracy of test nodes: %.7f\" % (e, untrained_accuracy))\n",
    "            print(\"Labels of trained nodes: \\n\", output[selected].max(1)[1])\n",
    "            net.train()\n",
    "            if trained_accuracy > best_pred:\n",
    "                best_pred = trained_accuracy\n",
    "                torch.save({\n",
    "                    'epoch': e + 1,\\\n",
    "                    'state_dict': net.state_dict(),\\\n",
    "                    'best_acc': trained_accuracy,\\\n",
    "                    'optimizer' : optimizer.state_dict(),\\\n",
    "                    'scheduler' : scheduler.state_dict(),\\\n",
    "                }, os.path.join(\"./data/\" ,\\\n",
    "                    \"test_model_best_%d.pth.tar\" % e))\n",
    "        if (e % 250) == 0:\n",
    "            save_to_pickle(losses_per_epoch, \"test_losses_per_epoch_%d.pkl\" % e)\n",
    "            #save_as_pickle(\"test_accuracy_per_epoch_%d.pkl\" % args.model_no, evaluation_untrained)\n",
    "            torch.save({\n",
    "                    'epoch': e + 1,\\\n",
    "                    'state_dict': net.state_dict(),\\\n",
    "                    'best_acc': trained_accuracy,\\\n",
    "                    'optimizer' : optimizer.state_dict(),\\\n",
    "                    'scheduler' : scheduler.state_dict(),\\\n",
    "                }, os.path.join(\"./data/\",\\\n",
    "                    \"test_checkpoint_%d.pth.tar\" % e))\n",
    "        scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
