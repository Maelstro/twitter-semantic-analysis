{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spmf import Spmf\n",
    "import pandas as pd\n",
    "from text_cleaner import *\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "archetype_list = ['artist',\n",
    "                 'caregiver',\n",
    "                 'everyman',\n",
    "                 'explorer',\n",
    "                 'guru',\n",
    "                 'hero',\n",
    "                 'innocent',\n",
    "                 'jester',\n",
    "                 'magician',\n",
    "                 'rebel',\n",
    "                 'ruler',\n",
    "                 'seducer']\n",
    "\n",
    "parameter_val = [\n",
    "    0.0005,\n",
    "    0.0007,\n",
    "    0.0005,\n",
    "    0.0003,\n",
    "    0.0003,\n",
    "    0.0008,\n",
    "    0.0006,\n",
    "    0.0004,\n",
    "    0.0003,\n",
    "    0.0008,\n",
    "    0.0004,\n",
    "    0.001\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>username</th>\n",
       "      <th>created_at</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>archetype</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cdc</td>\n",
       "      <td>@AndruEdwards The hard work has paid off, this...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-11-01 19:32:05.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[hard, work, paid, awesome]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cdd</td>\n",
       "      <td>@soosupersam A great way to surprise your love...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-11-01 19:09:40.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[great, way, surprise, loved, one]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cde</td>\n",
       "      <td>You can now just bring the fun home, and reliv...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-11-01 14:00:36.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[bring, fun, home, relive, favorite, childhood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cdf</td>\n",
       "      <td>@at_knb Happy birthday to the master builder! ...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-31 17:16:57.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[happy, birthday, master, builder, hope, magic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5f9f1c36b38e10f823bf2ce2</td>\n",
       "      <td>@Ranchie This is the way! ðŸ˜€</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-31 15:16:26.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[way]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  5f9f1c36b38e10f823bf2cdc   \n",
       "1  5f9f1c36b38e10f823bf2cdd   \n",
       "2  5f9f1c36b38e10f823bf2cde   \n",
       "3  5f9f1c36b38e10f823bf2cdf   \n",
       "6  5f9f1c36b38e10f823bf2ce2   \n",
       "\n",
       "                                          tweet_text    username  \\\n",
       "0  @AndruEdwards The hard work has paid off, this...  LEGO_Group   \n",
       "1  @soosupersam A great way to surprise your love...  LEGO_Group   \n",
       "2  You can now just bring the fun home, and reliv...  LEGO_Group   \n",
       "3  @at_knb Happy birthday to the master builder! ...  LEGO_Group   \n",
       "6                        @Ranchie This is the way! ðŸ˜€  LEGO_Group   \n",
       "\n",
       "                created_at timestamp archetype  \\\n",
       "0  2020-11-01 19:32:05.000       NaN    artist   \n",
       "1  2020-11-01 19:09:40.000       NaN    artist   \n",
       "2  2020-11-01 14:00:36.000       NaN    artist   \n",
       "3  2020-10-31 17:16:57.000       NaN    artist   \n",
       "6  2020-10-31 15:16:26.000       NaN    artist   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0                        [hard, work, paid, awesome]  \n",
       "1                 [great, way, surprise, loved, one]  \n",
       "2  [bring, fun, home, relive, favorite, childhood...  \n",
       "3  [happy, birthday, master, builder, hope, magic...  \n",
       "6                                              [way]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Twitter dataset\n",
    "twitter_df = pd.read_csv('tweets_06_03_2021.csv', index_col=0)\n",
    "\n",
    "# Print the head of the loaded dataset\n",
    "twitter_df.head()\n",
    "\n",
    "# Clean-up the texts\n",
    "twitter_df[\"cleaned_text\"] = twitter_df[\"tweet_text\"].apply(lambda x: clean_up_text(x))\n",
    "\n",
    "# Tokenize the cleaned texts\n",
    "twitter_df[\"cleaned_text\"] = twitter_df[\"cleaned_text\"].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Remove the stopwords\n",
    "twitter_df[\"cleaned_text\"] = twitter_df[\"cleaned_text\"].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# Drop the rows with empty 'cleaned_text' field\n",
    "twitter_df = twitter_df.drop(twitter_df[twitter_df['cleaned_text'].map(len) < 1].index)\n",
    "\n",
    "# Print the new head of the dataset\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 266 ms\n",
      " Frequent sequences count : 57106\n",
      " Max memory (mb) : 90.71826171875\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 57106\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                           pattern       sup\n",
      "0                           [able]  0.004080\n",
      "1                     [able, able]  0.000368\n",
      "2             [able, able, advise]  0.000070\n",
      "3               [able, able, look]  0.000070\n",
      "4        [able, able, information]  0.000070\n",
      "...                            ...       ...\n",
      "57101                  [mysteries]  0.000053\n",
      "57102  [mysteries, nintendoswitch]  0.000053\n",
      "57103                    [venture]  0.000070\n",
      "57104                       [woah]  0.000053\n",
      "57105                        [yay]  0.000053\n",
      "\n",
      "[57106 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 213 ms\n",
      " Frequent sequences count : 64677\n",
      " Max memory (mb) : 153.17578125\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 64677\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                        pattern       sup\n",
      "0                    [abortion]  0.000356\n",
      "1          [abortion, abortion]  0.000108\n",
      "2             [abortion, human]  0.000108\n",
      "3      [abortion, human, right]  0.000093\n",
      "4      [abortion, human, could]  0.000046\n",
      "...                         ...       ...\n",
      "64672                 [swiftly]  0.000046\n",
      "64673              [systematic]  0.000046\n",
      "64674                     [wow]  0.000077\n",
      "64675              [wow, great]  0.000046\n",
      "64676               [wow, shot]  0.000046\n",
      "\n",
      "[64677 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 229 ms\n",
      " Frequent sequences count : 83808\n",
      " Max memory (mb) : 102.51385498046875\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 83808\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                       pattern       sup\n",
      "0                    [provide]  0.000298\n",
      "1            [provide, please]  0.000036\n",
      "2           [provide, support]  0.000048\n",
      "3       [provide, support, dm]  0.000048\n",
      "4      [provide, support, via]  0.000048\n",
      "...                        ...       ...\n",
      "83803            [md, updates]  0.000036\n",
      "83804                  [vroom]  0.000036\n",
      "83805                   [whoa]  0.000048\n",
      "83806           [whoa, sierra]  0.000036\n",
      "83807                   [woah]  0.000048\n",
      "\n",
      "[83808 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 345 ms\n",
      " Frequent sequences count : 76232\n",
      " Max memory (mb) : 106.03048706054688\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 76232\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                  pattern       sup\n",
      "0                  [wolf]  0.000052\n",
      "1                  [moon]  0.001600\n",
      "2            [moon, moon]  0.000262\n",
      "3      [moon, moon, moon]  0.000079\n",
      "4      [moon, moon, full]  0.000039\n",
      "...                   ...       ...\n",
      "76227              [ease]  0.000039\n",
      "76228             [tulsa]  0.000039\n",
      "76229      [tulsa, black]  0.000039\n",
      "76230         [hyperloop]  0.000039\n",
      "76231          [newshour]  0.000039\n",
      "\n",
      "[76232 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 392 ms\n",
      " Frequent sequences count : 65344\n",
      " Max memory (mb) : 242.96351623535156\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 65344\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                pattern       sup\n",
      "0               [aaron]  0.000061\n",
      "1               [threw]  0.000046\n",
      "2                 [nfl]  0.000245\n",
      "3            [nfl, one]  0.000046\n",
      "4                 [bus]  0.000077\n",
      "...                 ...       ...\n",
      "65339    [walt, disney]  0.000046\n",
      "65340            [woah]  0.000061\n",
      "65341  [woah, captured]  0.000046\n",
      "65342             [wow]  0.000046\n",
      "65343          [losers]  0.000061\n",
      "\n",
      "[65344 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 183 ms\n",
      " Frequent sequences count : 69097\n",
      " Max memory (mb) : 133.40966796875\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 69097\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                         pattern       sup\n",
      "0                         [able]  0.000666\n",
      "1                    [able, get]  0.000101\n",
      "2                [able, account]  0.000043\n",
      "3                [able, twitter]  0.000043\n",
      "4                  [able, going]  0.000043\n",
      "...                          ...       ...\n",
      "69092  [unsere, kollegen, ihnen]  0.000058\n",
      "69093   [unsere, kollegen, sich]  0.000043\n",
      "69094             [unsere, sich]  0.000043\n",
      "69095      [unsere, sich, ihnen]  0.000043\n",
      "69096                     [wise]  0.000043\n",
      "\n",
      "[69097 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 224 ms\n",
      " Frequent sequences count : 82318\n",
      " Max memory (mb) : 82.91009521484375\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 82318\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                             pattern       sup\n",
      "0                             [able]  0.000680\n",
      "1                       [able, feel]  0.000049\n",
      "2                 [able, feel, free]  0.000049\n",
      "3                       [able, like]  0.000036\n",
      "4                       [able, send]  0.000097\n",
      "...                              ...       ...\n",
      "82313                       [toetal]  0.000036\n",
      "82314             [toetal, pleasure]  0.000036\n",
      "82315  [toetal, pleasure, christmas]  0.000036\n",
      "82316            [toetal, christmas]  0.000036\n",
      "82317               [wholeheartedly]  0.000061\n",
      "\n",
      "[82318 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 215 ms\n",
      " Frequent sequences count : 70391\n",
      " Max memory (mb) : 84.5087890625\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 70391\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                    pattern       sup\n",
      "0                [balloons]  0.000043\n",
      "1                  [pounds]  0.000057\n",
      "2                  [single]  0.000256\n",
      "3            [single, vote]  0.000043\n",
      "4                    [week]  0.000739\n",
      "...                     ...       ...\n",
      "70386          [yikes, let]  0.000071\n",
      "70387  [yikes, let, please]  0.000071\n",
      "70388      [yikes, let, dm]  0.000071\n",
      "70389      [yikes, let, us]  0.000071\n",
      "70390      [yikes, let, ka]  0.000043\n",
      "\n",
      "[70391 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 508 ms\n",
      " Frequent sequences count : 217215\n",
      " Max memory (mb) : 158.01280212402344\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 217215\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                   pattern       sup\n",
      "0                    [abc]  0.000046\n",
      "1             [technology]  0.000069\n",
      "2       [technology, like]  0.000014\n",
      "3         [technology, us]  0.000018\n",
      "4             [friendship]  0.000051\n",
      "...                    ...       ...\n",
      "217210         [visionary]  0.000014\n",
      "217211          [whispers]  0.000014\n",
      "217212              [woah]  0.000028\n",
      "217213               [woo]  0.000018\n",
      "217214              [yess]  0.000055\n",
      "\n",
      "[217215 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 144 ms\n",
      " Frequent sequences count : 28773\n",
      " Max memory (mb) : 129.92578125\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 28773\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                     pattern       sup\n",
      "0                     [able]  0.003510\n",
      "1             [able, please]  0.000174\n",
      "2         [able, please, dm]  0.000104\n",
      "3         [able, please, us]  0.000104\n",
      "4               [able, help]  0.001112\n",
      "...                      ...       ...\n",
      "28768             [whipping]  0.000104\n",
      "28769  [whipping, delicious]  0.000104\n",
      "28770               [woohoo]  0.000139\n",
      "28771         [woohoo, hear]  0.000104\n",
      "28772                 [woow]  0.000104\n",
      "\n",
      "[28773 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 272 ms\n",
      " Frequent sequences count : 75168\n",
      " Max memory (mb) : 142.1627426147461\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 75168\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                     pattern       sup\n",
      "0                 [continue]  0.000452\n",
      "1            [continue, new]  0.000053\n",
      "2         [continue, behind]  0.000040\n",
      "3      [continue, customers]  0.000040\n",
      "4          [continue, serve]  0.000040\n",
      "...                      ...       ...\n",
      "75163                [coast]  0.000040\n",
      "75164                 [woah]  0.000053\n",
      "75165     [worldkindnessday]  0.000040\n",
      "75166                 [yeah]  0.000040\n",
      "75167                [yikes]  0.000040\n",
      "\n",
      "[75168 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 180 ms\n",
      " Frequent sequences count : 56563\n",
      " Max memory (mb) : 138.45556640625\n",
      " minsup = 3 sequences.\n",
      " Pattern count : 56563\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                            pattern       sup\n",
      "0                            [able]  0.000566\n",
      "1                     [able, place]  0.000053\n",
      "2              [able, place, order]  0.000053\n",
      "3                     [able, order]  0.000141\n",
      "4              [able, order, order]  0.000071\n",
      "...                             ...       ...\n",
      "56558  [emerald, cut, tiffanyandco]  0.000053\n",
      "56559        [emerald, cut, carats]  0.000053\n",
      "56560                    [produced]  0.000053\n",
      "56561                 [experiences]  0.000053\n",
      "56562                         [yay]  0.000071\n",
      "\n",
      "[56563 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "for archetype, param in zip(archetype_list, parameter_val):\n",
    "    # Extract all the tweets for the archetype\n",
    "    tmp_df = twitter_df.cleaned_text[twitter_df[\"archetype\"] == archetype]\n",
    "\n",
    "    # Reset the index of the subset\n",
    "    tmp_df = tmp_df.reset_index(drop=True)\n",
    "\n",
    "    # Print the head of the subset\n",
    "    tmp_df.head()\n",
    "    \n",
    "    # Convert the word lists to full sentences, detokenization\n",
    "    tmp_df = pd.concat([tmp_df, tmp_df.apply(lambda x: \" \".join(x))], axis=1)\n",
    "    tmp_df.columns.values[1] = \"full_sentence\"\n",
    "\n",
    "    # Drop duplicates\n",
    "    tmp_df.sort_values(\"full_sentence\", inplace = True) \n",
    "    tmp_df.drop_duplicates(subset=\"full_sentence\", keep=False, inplace=True)\n",
    "    \n",
    "    text_list = tmp_df[\"full_sentence\"].tolist()\n",
    "\n",
    "    spmf = Spmf(\"PrefixSpan\", input_direct=text_list,\n",
    "                    output_filename=f\"sequence_files_df_only/output_{archetype}.txt\", arguments=[param, 3], input_type=\"text\")\n",
    "    spmf.run()\n",
    "\n",
    "    spmf = spmf.to_pandas_dataframe()\n",
    "\n",
    "    spmf[\"sup\"] = spmf[\"sup\"] / len(spmf)\n",
    "    print(spmf)\n",
    "    spmf.to_csv(f\"sequence_files_df_only/output_{archetype}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create an AGDS with the calculated values\n",
    "agds_layer = {}\n",
    "\n",
    "for archetype in archetype_list:\n",
    "    tmp_df = pd.read_csv(f\"sequence_files_df_only/output_{archetype}.csv\", index_col=0)\n",
    "    \n",
    "    for _, row in tmp_df.iterrows():\n",
    "        key = tuple(row[\"pattern\"])\n",
    "        if key not in agds_layer.keys():\n",
    "            agds_layer[key] = {archetype: row[\"sup\"]}\n",
    "        else:\n",
    "            agds_layer[key].update({archetype: row[\"sup\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "import pickle\n",
    "with open(\"sequence_files_df_only/agds_minsup_3.pickle\", \"wb\") as f:\n",
    "    pickle.dump(agds_layer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGDS representation size:  41943136\n",
      "Matrix representation size:  53337937\n",
      "AGDS needs ~ 1.27 times less space than standard matrix representation.\n"
     ]
    }
   ],
   "source": [
    "# Check the memory usage\n",
    "import os\n",
    "from sys import getsizeof\n",
    "mem_csvs = sum(os.path.getsize(os.path.join('sequence_files_df_only', f)) for f in os.listdir('sequence_files_df_only') if f.endswith(\".csv\"))\n",
    "\n",
    "print(\"AGDS representation size: \", getsizeof(agds_layer))\n",
    "print(\"Matrix representation size: \", mem_csvs)\n",
    "print(\"AGDS needs ~\", round(mem_csvs/getsizeof(agds_layer), 2), \"times less space than standard matrix representation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class for Tweet end-to-end processing\n",
    "# From tokenization to class assignment\n",
    "class SingleTweet(object):\n",
    "    def __init__(self, text):\n",
    "        self._raw_text = text\n",
    "        self._cleaned_text = self._clean_text()\n",
    "        self._all_terms = self._tokenize_and_permute()\n",
    "        self._class_description = {\n",
    "        \"artist\": 0.0,\n",
    "        \"caregiver\": 0.0,\n",
    "        \"everyman\": 0.0,\n",
    "        \"explorer\": 0.0,\n",
    "        \"guru\": 0.0,\n",
    "        \"hero\": 0.0,\n",
    "        \"innocent\": 0.0,\n",
    "        \"jester\": 0.0,\n",
    "        \"magician\": 0.0,\n",
    "        \"rebel\": 0.0,\n",
    "        \"ruler\": 0.0,\n",
    "        \"seducer\": 0.0\n",
    "        } \n",
    "        self._key_list = list(self._class_description)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self._class_description}\"\n",
    "        \n",
    "    def _clean_text(self):\n",
    "        return clean_up_text(self._raw_text)\n",
    "        \n",
    "    def _tokenize_and_permute(self):\n",
    "        terms = []\n",
    "        # Tokenize the Tweet\n",
    "        words = nltk.word_tokenize(self._cleaned_text)\n",
    "        terms = np.unique(words).tolist()\n",
    "        \n",
    "        # Get all two-word and three-word phrases\n",
    "        terms.append(itertools.permutations(words, 2))\n",
    "        terms.append(itertools.permutations(words, 3))\n",
    "        return terms\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "    def classify_tweet(self, structure):\n",
    "        for term in self._all_terms:\n",
    "            tuple_term = tuple(term)\n",
    "            if tuple_term in structure.keys():\n",
    "                for k, v in structure[tuple_term]:\n",
    "                    self._class_description[k] += v\n",
    "        val_list = list(self._class_description.values())\n",
    "        outs = self._softmax(val_list)\n",
    "        \n",
    "        return self._key_list[np.argmax(outs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the implementation on Twitter set (accuracy can be verified)\n",
    "twitter_df = pd.read_csv(\"tweets_06_03_2021.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109053it [08:28, 214.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AGDS: 0.11295425160243185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the AGDS accuracy\n",
    "import operator\n",
    "\n",
    "acc = 0\n",
    "for _, row in tqdm(twitter_df.iterrows()):\n",
    "    result = SingleTweet(row.tweet_text).classify_tweet(agds_layer)\n",
    "    if result == row.archetype:\n",
    "        acc += 1\n",
    "\n",
    "# Show the real accuracy\n",
    "print(f\"Accuracy of AGDS: {acc / len(twitter_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
