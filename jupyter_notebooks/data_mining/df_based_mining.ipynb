{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spmf import Spmf\n",
    "import pandas as pd\n",
    "from text_cleaner import *\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "archetype_list = ['artist',\n",
    "                 'caregiver',\n",
    "                 'everyman',\n",
    "                 'explorer',\n",
    "                 'guru',\n",
    "                 'hero',\n",
    "                 'innocent',\n",
    "                 'jester',\n",
    "                 'magician',\n",
    "                 'rebel',\n",
    "                 'ruler',\n",
    "                 'seducer']\n",
    "\n",
    "parameter_val = [\n",
    "    0.0007,\n",
    "    0.001,\n",
    "    0.0007,\n",
    "    0.0005,\n",
    "    0.0004,\n",
    "    0.001,\n",
    "    0.0007,\n",
    "    0.0005,\n",
    "    0.0004,\n",
    "    0.001,\n",
    "    0.0005,\n",
    "    0.0015\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>username</th>\n",
       "      <th>created_at</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>archetype</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cdc</td>\n",
       "      <td>@AndruEdwards The hard work has paid off, this...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-11-01 19:32:05.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[hard, work, paid, awesome]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cdd</td>\n",
       "      <td>@soosupersam A great way to surprise your love...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-11-01 19:09:40.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[great, way, surprise, loved, one]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cde</td>\n",
       "      <td>You can now just bring the fun home, and reliv...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-11-01 14:00:36.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[bring, fun, home, relive, favorite, childhood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cdf</td>\n",
       "      <td>@at_knb Happy birthday to the master builder! ...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-31 17:16:57.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[happy, birthday, master, builder, hope, magic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5f9f1c36b38e10f823bf2ce2</td>\n",
       "      <td>@Ranchie This is the way! ðŸ˜€</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-31 15:16:26.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[way]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  5f9f1c36b38e10f823bf2cdc   \n",
       "1  5f9f1c36b38e10f823bf2cdd   \n",
       "2  5f9f1c36b38e10f823bf2cde   \n",
       "3  5f9f1c36b38e10f823bf2cdf   \n",
       "6  5f9f1c36b38e10f823bf2ce2   \n",
       "\n",
       "                                          tweet_text    username  \\\n",
       "0  @AndruEdwards The hard work has paid off, this...  LEGO_Group   \n",
       "1  @soosupersam A great way to surprise your love...  LEGO_Group   \n",
       "2  You can now just bring the fun home, and reliv...  LEGO_Group   \n",
       "3  @at_knb Happy birthday to the master builder! ...  LEGO_Group   \n",
       "6                        @Ranchie This is the way! ðŸ˜€  LEGO_Group   \n",
       "\n",
       "                created_at timestamp archetype  \\\n",
       "0  2020-11-01 19:32:05.000       NaN    artist   \n",
       "1  2020-11-01 19:09:40.000       NaN    artist   \n",
       "2  2020-11-01 14:00:36.000       NaN    artist   \n",
       "3  2020-10-31 17:16:57.000       NaN    artist   \n",
       "6  2020-10-31 15:16:26.000       NaN    artist   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0                        [hard, work, paid, awesome]  \n",
       "1                 [great, way, surprise, loved, one]  \n",
       "2  [bring, fun, home, relive, favorite, childhood...  \n",
       "3  [happy, birthday, master, builder, hope, magic...  \n",
       "6                                              [way]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Twitter dataset\n",
    "twitter_df = pd.read_csv('tweets_06_03_2021.csv', index_col=0)\n",
    "\n",
    "# Print the head of the loaded dataset\n",
    "twitter_df.head()\n",
    "\n",
    "# Clean-up the texts\n",
    "twitter_df[\"cleaned_text\"] = twitter_df[\"tweet_text\"].apply(lambda x: clean_up_text(x))\n",
    "\n",
    "# Tokenize the cleaned texts\n",
    "twitter_df[\"cleaned_text\"] = twitter_df[\"cleaned_text\"].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Remove the stopwords\n",
    "twitter_df[\"cleaned_text\"] = twitter_df[\"cleaned_text\"].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# Drop the rows with empty 'cleaned_text' field\n",
    "twitter_df = twitter_df.drop(twitter_df[twitter_df['cleaned_text'].map(len) < 1].index)\n",
    "\n",
    "# Print the new head of the dataset\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 237 ms\n",
      " Frequent sequences count : 28796\n",
      " Max memory (mb) : 82.10414123535156\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 28796\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                         pattern       sup\n",
      "0                         [able]  0.008091\n",
      "1                   [able, able]  0.000729\n",
      "2           [able, able, advise]  0.000139\n",
      "3             [able, able, look]  0.000139\n",
      "4      [able, able, information]  0.000139\n",
      "...                          ...       ...\n",
      "28791                    [grabs]  0.000208\n",
      "28792             [grabs, today]  0.000174\n",
      "28793                 [giveaway]  0.000139\n",
      "28794                  [surreal]  0.000139\n",
      "28795                  [venture]  0.000139\n",
      "\n",
      "[28796 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 174 ms\n",
      " Frequent sequences count : 33688\n",
      " Max memory (mb) : 144.650390625\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 33688\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                        pattern       sup\n",
      "0                    [abortion]  0.000683\n",
      "1          [abortion, abortion]  0.000208\n",
      "2             [abortion, human]  0.000208\n",
      "3      [abortion, human, right]  0.000178\n",
      "4             [abortion, right]  0.000178\n",
      "...                         ...       ...\n",
      "33683            [truly, claim]  0.000119\n",
      "33684            [truly, thank]  0.000119\n",
      "33685        [truly, thank, bb]  0.000119\n",
      "33686                 [worries]  0.000119\n",
      "33687                     [wow]  0.000148\n",
      "\n",
      "[33688 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 205 ms\n",
      " Frequent sequences count : 50054\n",
      " Max memory (mb) : 98.43435668945312\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 50054\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                       pattern       sup\n",
      "0                    [provide]  0.000499\n",
      "1           [provide, support]  0.000080\n",
      "2       [provide, support, dm]  0.000080\n",
      "3      [provide, support, via]  0.000080\n",
      "4       [provide, support, us]  0.000080\n",
      "...                        ...       ...\n",
      "50049                    [tis]  0.000100\n",
      "50050            [tis, season]  0.000100\n",
      "50051             [underrated]  0.000080\n",
      "50052                   [whoa]  0.000080\n",
      "50053                   [woah]  0.000080\n",
      "\n",
      "[50054 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 299 ms\n",
      " Frequent sequences count : 45217\n",
      " Max memory (mb) : 103.92999267578125\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 45217\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                  pattern       sup\n",
      "0                  [wolf]  0.000088\n",
      "1                  [moon]  0.002698\n",
      "2            [moon, moon]  0.000442\n",
      "3      [moon, moon, moon]  0.000133\n",
      "4           [moon, earth]  0.000221\n",
      "...                   ...       ...\n",
      "45212              [vest]  0.000088\n",
      "45213               [mum]  0.000088\n",
      "45214      [spacewalkers]  0.000111\n",
      "45215        [splashdown]  0.000133\n",
      "45216               [tis]  0.000088\n",
      "\n",
      "[45217 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 431 ms\n",
      " Frequent sequences count : 37965\n",
      " Max memory (mb) : 196.7289047241211\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 37965\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                  pattern       sup\n",
      "0                 [aaron]  0.000105\n",
      "1                   [nfl]  0.000421\n",
      "2                   [bus]  0.000132\n",
      "3              [throwing]  0.000105\n",
      "4                [passes]  0.000105\n",
      "...                   ...       ...\n",
      "37960           [spanish]  0.000105\n",
      "37961  [worldwildlifeday]  0.000105\n",
      "37962             [edged]  0.000105\n",
      "37963              [woah]  0.000105\n",
      "37964            [losers]  0.000105\n",
      "\n",
      "[37965 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 200 ms\n",
      " Frequent sequences count : 40636\n",
      " Max memory (mb) : 122.43408203125\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 40636\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                         pattern       sup\n",
      "0                         [able]  0.001132\n",
      "1                    [able, get]  0.000172\n",
      "2                   [able, help]  0.000172\n",
      "3            [able, information]  0.000123\n",
      "4                  [able, order]  0.000148\n",
      "...                          ...       ...\n",
      "40631      [ihre, das, kollegen]  0.000098\n",
      "40632                   [unsere]  0.000098\n",
      "40633            [unsere, ihnen]  0.000098\n",
      "40634         [unsere, kollegen]  0.000098\n",
      "40635  [unsere, kollegen, ihnen]  0.000098\n",
      "\n",
      "[40636 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 175 ms\n",
      " Frequent sequences count : 50295\n",
      " Max memory (mb) : 71.72604370117188\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 50295\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                   pattern       sup\n",
      "0                   [able]  0.001113\n",
      "1             [able, feel]  0.000080\n",
      "2       [able, feel, free]  0.000080\n",
      "3             [able, send]  0.000159\n",
      "4         [able, send, us]  0.000080\n",
      "...                    ...       ...\n",
      "50290   [tagging, suggest]  0.000080\n",
      "50291     [tagging, helps]  0.000080\n",
      "50292           [unilever]  0.000159\n",
      "50293  [unilever, website]  0.000159\n",
      "50294     [wholeheartedly]  0.000099\n",
      "\n",
      "[50295 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 203 ms\n",
      " Frequent sequences count : 44939\n",
      " Max memory (mb) : 83.66732788085938\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 44939\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                    pattern       sup\n",
      "0                  [pounds]  0.000089\n",
      "1                  [single]  0.000401\n",
      "2                    [week]  0.001157\n",
      "3           [week, cartoon]  0.000267\n",
      "4               [week, get]  0.000134\n",
      "...                     ...       ...\n",
      "44934  [yikes, make, right]  0.000089\n",
      "44935          [yikes, let]  0.000111\n",
      "44936  [yikes, let, please]  0.000111\n",
      "44937      [yikes, let, dm]  0.000111\n",
      "44938      [yikes, let, us]  0.000111\n",
      "\n",
      "[44939 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 422 ms\n",
      " Frequent sequences count : 136950\n",
      " Max memory (mb) : 157.86660766601562\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 136950\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                 pattern       sup\n",
      "0                  [abc]  0.000073\n",
      "1           [technology]  0.000110\n",
      "2       [technology, us]  0.000029\n",
      "3           [friendship]  0.000080\n",
      "4                 [like]  0.004191\n",
      "...                  ...       ...\n",
      "136945     [tis, season]  0.000051\n",
      "136946              [uh]  0.000044\n",
      "136947            [woah]  0.000044\n",
      "136948             [woo]  0.000029\n",
      "136949            [yess]  0.000088\n",
      "\n",
      "[136950 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 129 ms\n",
      " Frequent sequences count : 14673\n",
      " Max memory (mb) : 111.9140625\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 14673\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                      pattern       sup\n",
      "0                      [able]  0.006883\n",
      "1              [able, please]  0.000341\n",
      "2                [able, help]  0.002181\n",
      "3              [able, advise]  0.000818\n",
      "4      [able, advise, thanks]  0.000613\n",
      "...                       ...       ...\n",
      "14668                   [thx]  0.001090\n",
      "14669               [thx, gx]  0.000409\n",
      "14670  [thx, gx, bankbalance]  0.000341\n",
      "14671      [thx, bankbalance]  0.000818\n",
      "14672                [woohoo]  0.000273\n",
      "\n",
      "[14673 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 247 ms\n",
      " Frequent sequences count : 42939\n",
      " Max memory (mb) : 98.66696166992188\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 42939\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                      pattern       sup\n",
      "0                  [continue]  0.000792\n",
      "1             [continue, new]  0.000093\n",
      "2                      [make]  0.003656\n",
      "3               [make, world]  0.000093\n",
      "4              [make, better]  0.000140\n",
      "...                       ...       ...\n",
      "42934  [clarifying, services]  0.000093\n",
      "42935                   [tis]  0.000093\n",
      "42936           [tis, season]  0.000093\n",
      "42937               [flights]  0.000093\n",
      "42938                  [woah]  0.000093\n",
      "\n",
      "[42939 rows x 2 columns]\n",
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 166 ms\n",
      " Frequent sequences count : 37308\n",
      " Max memory (mb) : 120.77685546875\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 37308\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                     pattern       sup\n",
      "0                     [able]  0.000858\n",
      "1              [able, order]  0.000214\n",
      "2       [able, order, order]  0.000107\n",
      "3      [able, order, please]  0.000134\n",
      "4          [able, order, us]  0.000107\n",
      "...                      ...       ...\n",
      "37303              [finding]  0.000134\n",
      "37304         [finding, see]  0.000107\n",
      "37305               [photos]  0.000107\n",
      "37306              [updates]  0.000107\n",
      "37307                  [yay]  0.000107\n",
      "\n",
      "[37308 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "for archetype, param in zip(archetype_list, parameter_val):\n",
    "    # Extract all the tweets for the archetype\n",
    "    tmp_df = twitter_df.cleaned_text[twitter_df[\"archetype\"] == archetype]\n",
    "\n",
    "    # Reset the index of the subset\n",
    "    tmp_df = tmp_df.reset_index(drop=True)\n",
    "\n",
    "    # Print the head of the subset\n",
    "    tmp_df.head()\n",
    "    \n",
    "    # Convert the word lists to full sentences, detokenization\n",
    "    tmp_df = pd.concat([tmp_df, tmp_df.apply(lambda x: \" \".join(x))], axis=1)\n",
    "    tmp_df.columns.values[1] = \"full_sentence\"\n",
    "\n",
    "    # Drop duplicates\n",
    "    tmp_df.sort_values(\"full_sentence\", inplace = True) \n",
    "    tmp_df.drop_duplicates(subset=\"full_sentence\", keep=False, inplace=True)\n",
    "    \n",
    "    text_list = tmp_df[\"full_sentence\"].tolist()\n",
    "\n",
    "    spmf = Spmf(\"PrefixSpan\", input_direct=text_list,\n",
    "                    output_filename=f\"sequence_files_minsup_4/output_{archetype}.txt\", arguments=[param, 3], input_type=\"text\")\n",
    "    spmf.run()\n",
    "\n",
    "    spmf = spmf.to_pandas_dataframe()\n",
    "\n",
    "    spmf[\"sup\"] = spmf[\"sup\"] / len(spmf)\n",
    "    print(spmf)\n",
    "    spmf.to_csv(f\"sequence_files_minsup_4/output_{archetype}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:48<00:00,  4.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# Now, create an AGDS with the calculated values\n",
    "import ast\n",
    "agds_layer = {}\n",
    "\n",
    "for archetype in tqdm(archetype_list):\n",
    "    tmp_df = pd.read_csv(f\"sequence_files_minsup_4/output_{archetype}.csv\", index_col=0)\n",
    "    tmp_df['pattern'] = tmp_df['pattern'].apply(lambda x: ast.literal_eval(x))\n",
    "    for _, row in tmp_df.iterrows():\n",
    "        \n",
    "        key = tuple(row[\"pattern\"])\n",
    "        if key not in agds_layer.keys():\n",
    "            agds_layer[key] = {archetype: row[\"sup\"]}\n",
    "        else:\n",
    "            agds_layer[key].update({archetype: row[\"sup\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "import pickle\n",
    "with open(\"sequence_files_minsup_4/agds_minsup_4.pickle\", \"wb\") as f:\n",
    "    pickle.dump(agds_layer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGDS representation size:  20971608\n",
      "Matrix representation size:  31407470\n",
      "AGDS needs ~ 1.5 times less space than standard matrix representation.\n"
     ]
    }
   ],
   "source": [
    "# Check the memory usage\n",
    "import os\n",
    "from sys import getsizeof\n",
    "mem_csvs = sum(os.path.getsize(os.path.join('sequence_files_minsup_4', f)) for f in os.listdir('sequence_files_minsup_4') if f.endswith(\".csv\"))\n",
    "\n",
    "print(\"AGDS representation size: \", getsizeof(agds_layer))\n",
    "print(\"Matrix representation size: \", mem_csvs)\n",
    "print(\"AGDS needs ~\", round(mem_csvs/getsizeof(agds_layer), 2), \"times less space than standard matrix representation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class for Tweet end-to-end processing\n",
    "# From tokenization to class assignment\n",
    "class SingleTweet(object):\n",
    "    def __init__(self, text):\n",
    "        self._raw_text = text\n",
    "        self._cleaned_text = self._clean_text()\n",
    "        self._all_terms = self._tokenize_and_permute()\n",
    "        self._class_description = {\n",
    "        \"artist\": 0.0,\n",
    "        \"caregiver\": 0.0,\n",
    "        \"everyman\": 0.0,\n",
    "        \"explorer\": 0.0,\n",
    "        \"guru\": 0.0,\n",
    "        \"hero\": 0.0,\n",
    "        \"innocent\": 0.0,\n",
    "        \"jester\": 0.0,\n",
    "        \"magician\": 0.0,\n",
    "        \"rebel\": 0.0,\n",
    "        \"ruler\": 0.0,\n",
    "        \"seducer\": 0.0\n",
    "        } \n",
    "        self._key_list = list(self._class_description)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self._class_description}\"\n",
    "        \n",
    "    def _clean_text(self):\n",
    "        return clean_up_text(self._raw_text)\n",
    "        \n",
    "    def _tokenize_and_permute(self):\n",
    "        terms = []\n",
    "        # Tokenize the Tweet\n",
    "        words = nltk.word_tokenize(self._cleaned_text)\n",
    "        terms = np.unique(words).tolist()\n",
    "        \n",
    "        # Get all two-word and three-word phrases\n",
    "        terms.append(itertools.permutations(words, 2))\n",
    "        terms.append(itertools.permutations(words, 3))\n",
    "        return terms\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "    def classify_tweet(self, structure):\n",
    "        for term in self._all_terms:\n",
    "            tuple_term = tuple(term)\n",
    "            if tuple_term in structure.keys():\n",
    "                for k, v in structure[tuple_term].items():\n",
    "                    self._class_description[k] += v\n",
    "        val_list = list(self._class_description.values())\n",
    "        outs = self._softmax(val_list)\n",
    "        \n",
    "        return self._key_list[np.argmax(outs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the implementation on Twitter set (accuracy can be verified)\n",
    "twitter_df = pd.read_csv(\"tweets_06_03_2021.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current accuracy: 0.12117044006125462: : 109053it [07:14, 250.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AGDS: 0.12117044006125462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the AGDS accuracy\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "acc = 0\n",
    "total_cnt = len(twitter_df)\n",
    "dataset = tqdm(twitter_df.iterrows())\n",
    "\n",
    "for _, row in dataset:\n",
    "    result = SingleTweet(row.tweet_text).classify_tweet(agds_layer)\n",
    "    if result == row.archetype:\n",
    "        acc += 1\n",
    "    dataset.set_description(f\"Current accuracy: {acc / total_cnt}\")\n",
    "\n",
    "# Show the real accuracy\n",
    "print(f\"Accuracy of AGDS: {acc / len(twitter_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
