{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolutional networks for Tweet archetype classification\n",
    "# Maciej W√≥jcik\n",
    "\n",
    "# Dependencies\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "def save_to_pickle(obj, file_name):\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def fcn_stub(stub):\n",
    "    return stub\n",
    "\n",
    "# Create a stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Function for stemming and lemmatization\n",
    "def stem_and_lemmatize(text:str) -> str:\n",
    "    \"\"\"Stems and lemmatizes a given text.\"\"\"\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess_texts(text_list: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Processes text to remove all unwanted words and symbols.\"\"\"\n",
    "\n",
    "    # Lowercase the tweets\n",
    "    text_list['processed_tweet'] = text_list['tweet_text'].str.lower()\n",
    "\n",
    "    # Regex patterns\n",
    "    url_pattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    user_pattern       = '@[^\\s]+'\n",
    "    alpha_pattern      = \"[^a-zA-Z]\"\n",
    "    sequence_pattern   = r\"(.)\\1\\1+\"\n",
    "    seq_replace_pattern = r\"\\1\\1\"\n",
    "\n",
    "    # Remove URLs from the tweet text\n",
    "    text_list['processed_tweet'] = [re.sub(url_pattern, ' ', str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "    # Remove username from the tweet text\n",
    "    text_list['processed_tweet'] = [re.sub(user_pattern, ' ', str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "    # Remove all non-alphanumeric symbols\n",
    "    text_list['processed_tweet'] = [re.sub(alpha_pattern, ' ', str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "    # Replace all 3 or more consecutive letters with 2 letters\n",
    "    text_list['processed_tweet'] = [re.sub(sequence_pattern, seq_replace_pattern, str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "\n",
    "    \n",
    "    full_tweet_list = []\n",
    "    for x in text_list['processed_tweet']:\n",
    "        full_tweet = ''\n",
    "        for word in x.split():\n",
    "            word = stem_and_lemmatize(word)\n",
    "            full_tweet += (word + ' ')\n",
    "        full_tweet_list.append(full_tweet)\n",
    "\n",
    "    text_list['processed_tweet'] = full_tweet_list\n",
    "\n",
    "    return text_list\n",
    "\n",
    "def filter_tokens(tokens):\n",
    "    tokens1 = []\n",
    "    for token in tokens:\n",
    "        if (token not in [\".\",\",\",\";\",\"&\",\"'s\", \":\", \"?\", \"!\",\"(\",\")\",\\\n",
    "            \"'\",\"'m\",\"'no\",\"***\",\"--\",\"...\",\"[\",\"]\", \" \"]):\n",
    "            tokens1.append(token)\n",
    "    return tokens1\n",
    "\n",
    "def word_word_edges(p_ij):\n",
    "    word_word = []\n",
    "    cols = list(p_ij.columns)\n",
    "    cols = [str(w) for w in cols]\n",
    "    \n",
    "    for w1, w2 in tqdm(combinations(cols, 2), total=nCr(len(cols), 2)):\n",
    "        if (p_ij.loc[w1,w2] > 0):\n",
    "            word_word.append((w1,w2,{\"weight\":p_ij.loc[w1,w2]}))\n",
    "    return word_word\n",
    "\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return int(f(n)/(f(r)*f(n-r)))\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>username</th>\n",
       "      <th>created_at</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>archetype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5f9f1c36b38e10f823bf2cef</td>\n",
       "      <td>@eliostruyf So exciting, have fun!¬†üòä</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-30 18:23:50.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5f9f1c36b38e10f823bf2ce7</td>\n",
       "      <td>These Brick-O-Lanterns are certainly all treat...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-31 09:00:28.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5f9f1c36b38e10f823bf2d0a</td>\n",
       "      <td>@dentistescabri Nous prenons la s√©curit√© de no...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-30 12:07:58.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5f9f1c36b38e10f823bf2cf5</td>\n",
       "      <td>@Jasmin80212446 üòçüéÑü•∞</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-30 16:35:39.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5f9f1c36b38e10f823bf2d07</td>\n",
       "      <td>@ashleydrixey Sounds like a perfect fit for th...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-30 13:09:14.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       _id  \\\n",
       "0           0  5f9f1c36b38e10f823bf2cef   \n",
       "1           1  5f9f1c36b38e10f823bf2ce7   \n",
       "2           2  5f9f1c36b38e10f823bf2d0a   \n",
       "3           3  5f9f1c36b38e10f823bf2cf5   \n",
       "4           4  5f9f1c36b38e10f823bf2d07   \n",
       "\n",
       "                                          tweet_text    username  \\\n",
       "0               @eliostruyf So exciting, have fun!¬†üòä  LEGO_Group   \n",
       "1  These Brick-O-Lanterns are certainly all treat...  LEGO_Group   \n",
       "2  @dentistescabri Nous prenons la s√©curit√© de no...  LEGO_Group   \n",
       "3                                @Jasmin80212446 üòçüéÑü•∞  LEGO_Group   \n",
       "4  @ashleydrixey Sounds like a perfect fit for th...  LEGO_Group   \n",
       "\n",
       "                created_at timestamp archetype  \n",
       "0  2020-10-30 18:23:50.000       NaN    artist  \n",
       "1  2020-10-31 09:00:28.000       NaN    artist  \n",
       "2  2020-10-30 12:07:58.000       NaN    artist  \n",
       "3  2020-10-30 16:35:39.000       NaN    artist  \n",
       "4  2020-10-30 13:09:14.000       NaN    artist  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and process dataset\n",
    "text_df = pd.read_csv('twitter_database.csv')\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text and drop empty fields\n",
    "text_df = preprocess_texts(text_df)\n",
    "text_df = text_df.groupby('archetype').head(1000)\n",
    "save_to_pickle(text_df, \"unprocessed_tweets_df.pickle\")\n",
    "print(len(text_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maelstro/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>aa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aarchi</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aayush</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbi</th>\n",
       "      <th>...</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zu</th>\n",
       "      <th>zukunft</th>\n",
       "      <th>zukunftssicherung</th>\n",
       "      <th>zum</th>\n",
       "      <th>zur</th>\n",
       "      <th>zwei</th>\n",
       "      <th>zyciora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.199264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.177881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.209317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.292803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.221512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 11727 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          a   aa  aah    aarchi     aaron  aayush   ab   abandon  abbey  abbi  \\\n",
       "0  0.199264  0.0  0.0  0.000000  0.000000     0.0  0.0  0.000000    0.0   0.0   \n",
       "1  0.177881  0.0  0.0  0.000000  0.000000     0.0  0.0  0.000000    0.0   0.0   \n",
       "2  0.209317  0.0  0.0  0.000000  0.002159     0.0  0.0  0.000000    0.0   0.0   \n",
       "3  0.292803  0.0  0.0  0.000000  0.000857     0.0  0.0  0.007835    0.0   0.0   \n",
       "4  0.221512  0.0  0.0  0.001365  0.000000     0.0  0.0  0.000000    0.0   0.0   \n",
       "\n",
       "   ...     zombi      zone      zoom   zu  zukunft  zukunftssicherung  zum  \\\n",
       "0  ...  0.000000  0.004686  0.000000  0.0      0.0                0.0  0.0   \n",
       "1  ...  0.000000  0.000000  0.000000  0.0      0.0                0.0  0.0   \n",
       "2  ...  0.000000  0.000000  0.000000  0.0      0.0                0.0  0.0   \n",
       "3  ...  0.001306  0.000939  0.000939  0.0      0.0                0.0  0.0   \n",
       "4  ...  0.002345  0.000000  0.000000  0.0      0.0                0.0  0.0   \n",
       "\n",
       "   zur  zwei  zyciora  \n",
       "0  0.0   0.0      0.0  \n",
       "1  0.0   0.0      0.0  \n",
       "2  0.0   0.0      0.0  \n",
       "3  0.0   0.0      0.0  \n",
       "4  0.0   0.0      0.0  \n",
       "\n",
       "[5 rows x 11727 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the words\n",
    "df_ta = pd.DataFrame(columns=[\"processed_tweet\", \"archetype\"])\n",
    "for arch in text_df[\"archetype\"].unique():\n",
    "    dummy = pd.DataFrame(columns=[\"processed_tweet\", \"archetype\"])\n",
    "    dummy[\"processed_tweet\"] = text_df[text_df[\"archetype\"] == arch].groupby(\"archetype\").apply(lambda x: (\" \".join(x[\"processed_tweet\"])).lower())\n",
    "    dummy[\"archetype\"] = arch\n",
    "    df_ta = pd.concat([df_ta, dummy], ignore_index=True)\n",
    "\n",
    "# Tokenize the dataframe\n",
    "df_ta['processed_tweet'] = df_ta['processed_tweet'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: filter_tokens(x))\n",
    "\n",
    "# Data vectorization\n",
    "vectorizer = TfidfVectorizer(input=\"content\", max_features=None, tokenizer=fcn_stub, preprocessor=fcn_stub)\n",
    "vectorizer.fit(df_ta['processed_tweet'])\n",
    "df_tfidf = vectorizer.transform(df_ta['processed_tweet'])\n",
    "df_tfidf = df_tfidf.toarray()\n",
    "\n",
    "# Get feature names\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocab = np.array(vocab)\n",
    "\n",
    "df_tfidf = pd.DataFrame(df_tfidf, columns=vocab)\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:09<00:00,  1.31it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11727/11727 [00:03<00:00, 3876.73it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11727/11727 [00:05<00:00, 2322.67it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11727/11727 [00:43<00:00, 267.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate PMI between words\n",
    "names = vocab\n",
    "name_idx = OrderedDict((name, 0) for name in names)\n",
    "word_to_index = OrderedDict((name, index) for index, name in enumerate(names))\n",
    "\n",
    "# Get the co-occurrences\n",
    "occurrences = np.zeros((len(names), len(names)), dtype=np.int32)\n",
    "windows_count = 0\n",
    "window = 10             # Sliding window size, for calculation PMI between words \n",
    "for l in tqdm(df_ta['processed_tweet'], total=len(df_ta['processed_tweet'])):\n",
    "    for i in range(len(l) - window):\n",
    "        windows_count += 1\n",
    "        d = set(l[i:(i+window)])\n",
    "        for w in d:\n",
    "            name_idx[w] += 1\n",
    "        for w1, w2 in combinations(d, 2):\n",
    "            i1 = word_to_index[w1]\n",
    "            i2 = word_to_index[w2]\n",
    "            \n",
    "            occurrences[i1][i2] = 1\n",
    "            occurrences[i2][i1] = 1\n",
    "            \n",
    "# Convert the occurences to PMI\n",
    "pmi_per_word = pd.DataFrame(occurrences, index=names, columns=names) / windows_count\n",
    "pmi_index = pd.Series(name_idx, index=name_idx.keys()) / windows_count\n",
    "\n",
    "# Free memory\n",
    "del occurrences\n",
    "del name_idx\n",
    "\n",
    "for col in tqdm(pmi_per_word.columns):\n",
    "    pmi_per_word[col] = pmi_per_word[col]/pmi_index[col]\n",
    "\n",
    "for row in tqdm(pmi_per_word.index):\n",
    "    pmi_per_word.loc[row, :] = pmi_per_word.loc[row, :] / pmi_index[row]\n",
    "    \n",
    "pmi_per_word = pmi_per_word + 1E-9\n",
    "for col in tqdm(pmi_per_word.columns):\n",
    "    pmi_per_word[col] = pmi_per_word[col].apply(lambda x: math.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:01<00:00,  9.07it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68755401/68755401 [07:06<00:00, 161123.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build a graph\n",
    "graph = nx.Graph()\n",
    "graph.add_nodes_from(df_tfidf.index)\n",
    "graph.add_nodes_from(vocab)\n",
    "\n",
    "# Build document-word edges\n",
    "document_word = [(doc,w,{\"weight\":df_tfidf.loc[doc,w]}) for doc in tqdm(df_tfidf.index, total=len(df_tfidf.index))\\\n",
    "                     for w in df_tfidf.columns]\n",
    "\n",
    "word_word = word_word_edges(pmi_per_word)\n",
    "graph.add_edges_from(document_word)\n",
    "graph.add_edges_from(word_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export every needed structure\n",
    "save_to_pickle(graph, \"text_graph.pickle\")\n",
    "save_to_pickle(df_ta, \"tweet_archetype_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph loaded.\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "with open('text_graph.pickle', \"rb\") as f:\n",
    "    graph = pickle.load(f)\n",
    "print(\"Graph loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11739/11739 [00:00<00:00, 1212344.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# GCN - implementation and training\n",
    "# Create A matrix and hat_A\n",
    "A = nx.to_numpy_matrix(graph, weight=\"weight\")\n",
    "A = A + np.eye(graph.number_of_nodes())\n",
    "\n",
    "degs = []\n",
    "for deg in tqdm(graph.degree(weight=None)):\n",
    "    if deg == 0:\n",
    "        degs.append(0)\n",
    "    else:\n",
    "        degs.append(deg[1]**(-0.5))\n",
    "degs = np.diag(degs)\n",
    "X = np.eye(graph.number_of_nodes())\n",
    "hat_A = np.matmul(np.matmul(degs, A), degs)\n",
    "inp = X  # Net input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 1357.01it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 5919.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing test indices: [74, 276, 107, 77, 401, 277, 251, 60, 430, 64, 249, 397, 38, 364, 279, 150, 682, 53, 735, 325, 373, 82, 659, 889, 178, 317, 623, 293, 925, 261, 318, 284, 597, 181, 697, 742, 470, 760, 479, 780, 749, 36, 784, 964, 974, 591, 468, 437, 492, 907, 176, 956, 687, 242, 126, 639, 387, 834, 835, 606, 485, 624, 647, 572, 620, 368, 848, 575, 13, 265, 714, 63, 928, 149, 795, 840, 455, 350, 340, 102, 940, 695, 666, 290, 844, 573, 405, 862, 264, 998, 708, 415, 577, 151, 797, 6, 333, 754, 203, 534, 1220, 1170, 1649, 1056, 1322, 1400, 1114, 1871, 1023, 1415, 1043, 1492, 1353, 1570, 1629, 1088, 1812, 1384, 1793, 1759, 1115, 1943, 1729, 1228, 1364, 1351, 1836, 1069, 1905, 1583, 1440, 1889, 1464, 1621, 1097, 1965, 1226, 1388, 1222, 1772, 1631, 1435, 1514, 1398, 1008, 1489, 1540, 1021, 1936, 1925, 1562, 1561, 1206, 1816, 1521, 1880, 1994, 1268, 1212, 1834, 1070, 1252, 1896, 1238, 1337, 1901, 1437, 1617, 1754, 1102, 1169, 1090, 1289, 1153, 1959, 1062, 1283, 1452, 1541, 1182, 1698, 1037, 1141, 1865, 1481, 1985, 1071, 1133, 1483, 1231, 1072, 1930, 1465, 1463, 1001, 1605, 1563, 1422, 1038, 1861, 2583, 2758, 2519, 2943, 2038, 2772, 2080, 2438, 2484, 2916, 2368, 2367, 2736, 2862, 2447, 2216, 2565, 2305, 2221, 2531, 2496, 2779, 2281, 2230, 2201, 2327, 2380, 2606, 2106, 2234, 2639, 2215, 2354, 2343, 2310, 2292, 2144, 2441, 2319, 2044, 2308, 2003, 2932, 2445, 2431, 2202, 2398, 2993, 2184, 2058, 2063, 2737, 2220, 2735, 2890, 2393, 2190, 2123, 2995, 2387, 2282, 2812, 2832, 2904, 2388, 2973, 2607, 2571, 2245, 2544, 2108, 2271, 2499, 2340, 2885, 2720, 2770, 2137, 2632, 2334, 2072, 2978, 2251, 2117, 2162, 2096, 2814, 2986, 2402, 2060, 2808, 2260, 2477, 2497, 2400, 2183, 2295, 2507, 2200, 2265, 3205, 3831, 3873, 3206, 3893, 3758, 3742, 3734, 3238, 3902, 3492, 3469, 3292, 3646, 3972, 3227, 3364, 3700, 3466, 3086, 3251, 3818, 3180, 3917, 3926, 3961, 3153, 3570, 3482, 3312, 3980, 3059, 3686, 3746, 3582, 3344, 3641, 3541, 3892, 3871, 3193, 3725, 3955, 3827, 3613, 3204, 3496, 3029, 3300, 3659, 3329, 3588, 3761, 3801, 3967, 3245, 3696, 3833, 3566, 3094, 3275, 3535, 3412, 3395, 3134, 3743, 3346, 3432, 3242, 3293, 3444, 3177, 3343, 3710, 3402, 3331, 3309, 3218, 3333, 3363, 3768, 3763, 3460, 3077, 3603, 3277, 3660, 3201, 3592, 3465, 3174, 3308, 3716, 3291, 3176, 3576, 3844, 3216, 3124, 3336, 4435, 4441, 4659, 4236, 4862, 4156, 4661, 4375, 4784, 4960, 4272, 4332, 4275, 4051, 4046, 4562, 4164, 4410, 4870, 4513, 4934, 4212, 4349, 4687, 4200, 4889, 4931, 4343, 4628, 4949, 4573, 4765, 4511, 4171, 4600, 4054, 4348, 4360, 4434, 4487, 4422, 4184, 4407, 4922, 4418, 4760, 4147, 4510, 4450, 4634, 4193, 4686, 4580, 4752, 4412, 4772, 4834, 4484, 4946, 4131, 4392, 4724, 4350, 4173, 4747, 4001, 4281, 4875, 4927, 4766, 4390, 4553, 4981, 4475, 4355, 4077, 4941, 4124, 4499, 4621, 4215, 4306, 4878, 4020, 4465, 4890, 4745, 4867, 4715, 4497, 4398, 4692, 4705, 4872, 4500, 4603, 4988, 4344, 4068, 4005, 5451, 5793, 5156, 5610, 5143, 5355, 5817, 5167, 5444, 5266, 5232, 5500, 5446, 5738, 5460, 5123, 5062, 5671, 5139, 5057, 5890, 5385, 5726, 5675, 5121, 5063, 5126, 5747, 5004, 5393, 5639, 5367, 5058, 5312, 5772, 5085, 5314, 5968, 5304, 5939, 5905, 5238, 5208, 5416, 5457, 5532, 5294, 5342, 5822, 5733, 5594, 5724, 5140, 5334, 5453, 5479, 5144, 5612, 5755, 5282, 5038, 5235, 5417, 5818, 5792, 5656, 5283, 5729, 5226, 5956, 5340, 5452, 5317, 5971, 5972, 5338, 5315, 5609, 5823, 5015, 5176, 5975, 5600, 5861, 5388, 5324, 5844, 5552, 5328, 5273, 5626, 5278, 5809, 5608, 5734, 5725, 5053, 5499, 5173, 5698, 6473, 6212, 6637, 6785, 6691, 6170, 6843, 6079, 6898, 6845, 6405, 6587, 6185, 6453, 6977, 6411, 6956, 6738, 6003, 6135, 6059, 6734, 6725, 6667, 6007, 6126, 6933, 6077, 6779, 6277, 6313, 6215, 6881, 6947, 6892, 6115, 6747, 6426, 6744, 6198, 6524, 6298, 6752, 6431, 6935, 6202, 6076, 6952, 6435, 6107, 6025, 6541, 6161, 6870, 6926, 6580, 6258, 6291, 6271, 6380, 6739, 6768, 6605, 6030, 6939, 6606, 6839, 6516, 6813, 6512, 6312, 6840, 6684, 6902, 6289, 6789, 6967, 6062, 6279, 6357, 6270, 6057, 6283, 6378, 6454, 6728, 6299, 6960, 6683, 6047, 6535, 6730, 6503, 6832, 6670, 6129, 6116, 6395, 6934, 6325, 7752, 7760, 7680, 7011, 7190, 7238, 7102, 7189, 7809, 7466, 7553, 7398, 7470, 7647, 7866, 7807, 7587, 7610, 7463, 7402, 7186, 7247, 7843, 7885, 7816, 7561, 7493, 7832, 7833, 7535, 7934, 7820, 7788, 7327, 7999, 7810, 7867, 7690, 7445, 7815, 7221, 7307, 7002, 7078, 7251, 7556, 7565, 7634, 7532, 7502, 7675, 7513, 7020, 7876, 7856, 7444, 7157, 7507, 7987, 7051, 7254, 7596, 7174, 7401, 7771, 7848, 7545, 7273, 7087, 7562, 7997, 7363, 7745, 7036, 7968, 7892, 7726, 7546, 7289, 7000, 7901, 7384, 7424, 7660, 7600, 7171, 7544, 7554, 7381, 7797, 7537, 7520, 7800, 7474, 7028, 7432, 7558, 7134, 7830, 7085, 8981, 8556, 8633, 8723, 8338, 8232, 8099, 8563, 8102, 8333, 8368, 8670, 8309, 8189, 8053, 8280, 8040, 8244, 8221, 8248, 8254, 8100, 8214, 8766, 8987, 8705, 8725, 8867, 8320, 8313, 8404, 8408, 8319, 8829, 8729, 8365, 8339, 8388, 8108, 8610, 8324, 8476, 8892, 8426, 8070, 8260, 8202, 8087, 8443, 8848, 8162, 8888, 8485, 8438, 8440, 8655, 8462, 8465, 8074, 8006, 8360, 8413, 8968, 8179, 8392, 8470, 8611, 8135, 8727, 8204, 8342, 8982, 8106, 8035, 8819, 8469, 8346, 8119, 8048, 8534, 8488, 8239, 8557, 8923, 8952, 8486, 8142, 8446, 8565, 8842, 8951, 8409, 8269, 8972, 8508, 8054, 8225, 8728, 8306, 8104, 9264, 9351, 9302, 9705, 9604, 9176, 9700, 9504, 9890, 9134, 9808, 9227, 9772, 9236, 9532, 9373, 9564, 9685, 9534, 9576, 9909, 9001, 9764, 9003, 9698, 9245, 9284, 9809, 9377, 9109, 9202, 9148, 9831, 9086, 9085, 9573, 9346, 9143, 9443, 9475, 9132, 9826, 9283, 9903, 9238, 9462, 9655, 9354, 9915, 9198, 9311, 9817, 9934, 9105, 9738, 9763, 9081, 9672, 9627, 9033, 9381, 9734, 9493, 9371, 9538, 9448, 9593, 9675, 9669, 9722, 9701, 9068, 9907, 9811, 9293, 9419, 9853, 9303, 9595, 9980, 9295, 9953, 9642, 9185, 9278, 9115, 9309, 9256, 9645, 9782, 9301, 9471, 9870, 9405, 9874, 9633, 9773, 9820, 9417, 9994, 10402, 10281, 10113, 10131, 10350, 10171, 10301, 10297, 10023, 10791, 10409, 10211, 10242, 10662, 10692, 10510, 10289, 10497, 10392, 10467, 10958, 10460, 10045, 10330, 10868, 10807, 10576, 10678, 10154, 10586, 10526, 10445, 10885, 10581, 10993, 10728, 10277, 10389, 10880, 10559, 10715, 10239, 10348, 10999, 10026, 10515, 10263, 10980, 10920, 10410, 10179, 10660, 10712, 10457, 10989, 10796, 10264, 10998, 10645, 10721, 10349, 10285, 10778, 10175, 10018, 10748, 10258, 10454, 10792, 10578, 10620, 10229, 10968, 10652, 10536, 10082, 10736, 10775, 10181, 10148, 10141, 10276, 10955, 10224, 10861, 10145, 10883, 10035, 10766, 10010, 10503, 10114, 10613, 10079, 10014, 10996, 10074, 10354, 10971, 10933, 11618, 11675, 11742, 11205, 11572, 11894, 11582, 11535, 11563, 11609, 11178, 11850, 11196, 11241, 11320, 11657, 11845, 11496, 11590, 11329, 11307, 11967, 11173, 11472, 11604, 11130, 11484, 11411, 11498, 11157, 11424, 11412, 11175, 11679, 11324, 11139, 11233, 11964, 11963, 11573, 11776, 11354, 11867, 11459, 11781, 11265, 11994, 11462, 11514, 11730, 11692, 11745, 11669, 11814, 11272, 11672, 11136, 11304, 11571, 11540, 11525, 11378, 11550, 11447, 11754, 11587, 11761, 11473, 11437, 11971, 11036, 11739, 11626, 11791, 11707, 11722, 11471, 11905, 11203, 11416, 11189, 11057, 11296, 11180, 11361, 11918, 11006, 11341, 11641, 11245, 11920, 11031, 11336, 11519, 11533, 11695, 11160, 11321, 11779, 11980]\n",
      "Finished selecting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the tweet pickle\n",
    "with open('unprocessed_tweets_df.pickle', \"rb\") as f:\n",
    "    df_tweet = pickle.load(f)\n",
    "\n",
    "archetype_dict = {'archetype': \n",
    "                  {'artist': 0,\n",
    "                  'caregiver': 1,\n",
    "                  'everyman': 2,\n",
    "                  'explorer': 3,\n",
    "                  'guru': 4,\n",
    "                  'hero': 5,\n",
    "                  'innocent': 6,\n",
    "                  'jester': 7,\n",
    "                  'magician': 8,\n",
    "                  'rebel': 9,\n",
    "                  'ruler': 10,\n",
    "                  'seducer':11}\n",
    "                 }\n",
    "\n",
    "df_tweet = df_tweet.replace(archetype_dict)\n",
    "df_tweet = df_tweet.reset_index()\n",
    "\n",
    "# Split the testing dataset\n",
    "test_indices = []\n",
    "for arch in tqdm(df_tweet[\"archetype\"].unique()):\n",
    "    tmp = df_tweet[df_tweet[\"archetype\"] == arch]\n",
    "    if len(tmp) >= 4:\n",
    "        test_indices.extend(list(np.random.choice(tmp.index, size=round(0.1*len(tmp)), replace=False)))\n",
    "print(f\"Finished processing test indices: {test_indices}\")\n",
    "\n",
    "selected = []\n",
    "for i in tqdm(range(len(df_ta))):\n",
    "    if i not in test_indices:\n",
    "        selected.append(i)\n",
    "print(\"Finished selecting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-93-c827cfd72680>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inp_selected = torch.tensor(inp_selected, device=torch.device('cuda'))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 11742 is out of bounds for axis 0 with size 11739",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-c827cfd72680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minp_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlabels_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"archetype\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0minp_not_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0minp_not_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_not_selected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlabels_not_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"archetype\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 11742 is out of bounds for axis 0 with size 11739"
     ]
    }
   ],
   "source": [
    "# Save test indices and seleced ones\n",
    "save_to_pickle(test_indices, \"test_indices.pickle\")\n",
    "save_to_pickle(selected, \"selected.pickle\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Operations on selected inputs\n",
    "inp_selected = inp[selected]\n",
    "inp_selected = torch.from_numpy(inp_selected).float()\n",
    "inp_selected = torch.tensor(inp_selected, device=torch.device('cuda'))\n",
    "labels_selected = [l for idx, l in enumerate(df_tweet[\"archetype\"]) if idx in selected]\n",
    "inp_not_selected = inp[test_indices]\n",
    "inp_not_selected = torch.from_numpy(inp_not_selected).float()\n",
    "labels_not_selected = [l for idx, l in enumerate(df_tweet[\"archetype\"]) if idx not in selected]\n",
    "inp = torch.from_numpy(inp).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, X_size, A_hat, args, bias=True): # X_size = num features\n",
    "        super(GCN, self).__init__()\n",
    "        self.A_hat = torch.tensor(A_hat, requires_grad=False, device=device).float()\n",
    "        self.weight = nn.parameter.Parameter(torch.FloatTensor(X_size, args['hidden_size_1']))\n",
    "        var = 2./(self.weight.size(1)+self.weight.size(0))\n",
    "        self.weight.data.normal_(0,var)\n",
    "        self.weight2 = nn.parameter.Parameter(torch.FloatTensor(args['hidden_size_1'], args['hidden_size_2']))\n",
    "        var2 = 2./(self.weight2.size(1)+self.weight2.size(0))\n",
    "        self.weight2.data.normal_(0,var2)\n",
    "        if bias:\n",
    "            self.bias = nn.parameter.Parameter(torch.FloatTensor(args['hidden_size_1']))\n",
    "            self.bias.data.normal_(0,var)\n",
    "            self.bias2 = nn.parameter.Parameter(torch.FloatTensor(args['hidden_size_2']))\n",
    "            self.bias2.data.normal_(0,var2)\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.fc1 = nn.Linear(args['hidden_size_2'], args['num_classes'])\n",
    "        \n",
    "    def forward(self, X): ### 2-layer GCN architecture\n",
    "        X = torch.mm(X, self.weight)\n",
    "        if self.bias is not None:\n",
    "            X = (X + self.bias)\n",
    "        X = F.relu(torch.mm(self.A_hat, X))\n",
    "        X = torch.mm(X, self.weight2)\n",
    "        if self.bias2 is not None:\n",
    "            X = (X + self.bias2)\n",
    "        X = F.relu(torch.mm(self.A_hat, X))\n",
    "        return self.fc1(X)\n",
    "    \n",
    "def evaluate(output, labels_e):\n",
    "    _, labels = output.max(1); labels = labels.numpy()\n",
    "    return sum([(e-1) for e in labels_e] == labels)/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define additional arguments\n",
    "args = {\n",
    "    'hidden_size_2': 130, \n",
    "    'num_classes': 12, \n",
    "    'hidden_size_1': 330\n",
    "}\n",
    "\n",
    "net = GCN(X.shape[1], hat_A, args).to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1000,2000,3000,4000,5000,6000], gamma=0.77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 dim 1 must match mat2 dim 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-fe26c26e4e58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0minp_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp_selected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_selected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_selected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlosses_per_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-4f79205d558e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias2\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
     ]
    }
   ],
   "source": [
    "losses_per_epoch, accuracy_per_epoch = [], []\n",
    "evaluation_trained = []\n",
    "best_pred = 0.0\n",
    "import os\n",
    "\n",
    "for e in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        inp_selected = inp_selected.to(device)\n",
    "        output = net(inp_selected)\n",
    "        loss = criterion(output[selected], torch.tensor(labels_selected).long())\n",
    "        losses_per_epoch.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if e % 50 == 0:\n",
    "            ### Evaluate other untrained nodes and check accuracy of labelling\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_labels = net(inp_selected)\n",
    "                trained_accuracy = evaluate(output[selected], labels_selected); \n",
    "                #untrained_accuracy = evaluate(pred_labels[test_indices], labels_not_selected)\n",
    "            evaluation_trained.append((e, trained_accuracy))\n",
    "            #evaluation_untrained.append((e, untrained_accuracy))\n",
    "            print(\"[Epoch %d]: Evaluation accuracy of trained nodes: %.7f\" % (e, trained_accuracy))\n",
    "            #print(\"[Epoch %d]: Evaluation accuracy of test nodes: %.7f\" % (e, untrained_accuracy))\n",
    "            print(\"Labels of trained nodes: \\n\", output[selected].max(1)[1])\n",
    "            net.train()\n",
    "            if trained_accuracy > best_pred:\n",
    "                best_pred = trained_accuracy\n",
    "                torch.save({\n",
    "                    'epoch': e + 1,\\\n",
    "                    'state_dict': net.state_dict(),\\\n",
    "                    'best_acc': trained_accuracy,\\\n",
    "                    'optimizer' : optimizer.state_dict(),\\\n",
    "                    'scheduler' : scheduler.state_dict(),\\\n",
    "                }, os.path.join(\"./data/\" ,\\\n",
    "                    \"test_model_best_%d.pth.tar\" % e))\n",
    "        if (e % 250) == 0:\n",
    "            save_to_pickle(losses_per_epoch, \"test_losses_per_epoch_%d.pkl\" % e)\n",
    "            #save_as_pickle(\"test_accuracy_per_epoch_%d.pkl\" % args.model_no, evaluation_untrained)\n",
    "            torch.save({\n",
    "                    'epoch': e + 1,\\\n",
    "                    'state_dict': net.state_dict(),\\\n",
    "                    'best_acc': trained_accuracy,\\\n",
    "                    'optimizer' : optimizer.state_dict(),\\\n",
    "                    'scheduler' : scheduler.state_dict(),\\\n",
    "                }, os.path.join(\"./data/\",\\\n",
    "                    \"test_checkpoint_%d.pth.tar\" % e))\n",
    "        scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
