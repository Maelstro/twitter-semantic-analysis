{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGDS - Twitter\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from itertools import combinations\n",
    "import math\n",
    "\n",
    "def filter_tokens(tokens):\n",
    "    tokens1 = []\n",
    "    for token in tokens:\n",
    "        if (token not in [\".\",\",\",\";\",\"&\",\"'s\", \":\", \"?\", \"!\",\"(\",\")\",\\\n",
    "            \"'\",\"'m\",\"'no\",\"***\",\"--\",\"...\",\"[\",\"]\", \" \"]):\n",
    "            tokens1.append(token)\n",
    "    return tokens1\n",
    "\n",
    "def fcn_stub(stub):\n",
    "    return stub\n",
    "\n",
    "def word_word_edges(p_ij):\n",
    "    word_word = []\n",
    "    cols = list(p_ij.columns)\n",
    "    cols = [str(w) for w in cols]\n",
    "    \n",
    "    for w1, w2 in tqdm(combinations(cols, 2), total=nCr(len(cols), 2)):\n",
    "        if (p_ij.loc[w1,w2] > 0):\n",
    "            word_word.append((w1,w2,{\"weight\":p_ij.loc[w1,w2]}))\n",
    "    return word_word\n",
    "\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return int(f(n)/(f(r)*f(n-r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>username</th>\n",
       "      <th>created_at</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>archetype</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cef</td>\n",
       "      <td>@eliostruyf So exciting, have fun!¬†üòä</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-30 18:23:50.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>['excit']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5f9f1c36b38e10f823bf2ce7</td>\n",
       "      <td>These Brick-O-Lanterns are certainly all treat...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-31 09:00:28.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>['brick', 'lantern', 'certain', 'treat', 'tric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5f9f1c36b38e10f823bf2d0a</td>\n",
       "      <td>@dentistescabri Nous prenons la s√©curit√© de no...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-30 12:07:58.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>['nous', 'prenon', 'curit', 'fan', 'rieux', 'p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cf5</td>\n",
       "      <td>@Jasmin80212446 üòçüéÑü•∞</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-30 16:35:39.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5f9f1c36b38e10f823bf2d07</td>\n",
       "      <td>@ashleydrixey Sounds like a perfect fit for th...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-30 13:09:14.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>['sound', 'like', 'perfect', 'aspir', 'young',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  5f9f1c36b38e10f823bf2cef   \n",
       "1  5f9f1c36b38e10f823bf2ce7   \n",
       "2  5f9f1c36b38e10f823bf2d0a   \n",
       "3  5f9f1c36b38e10f823bf2cf5   \n",
       "4  5f9f1c36b38e10f823bf2d07   \n",
       "\n",
       "                                          tweet_text    username  \\\n",
       "0               @eliostruyf So exciting, have fun!¬†üòä  LEGO_Group   \n",
       "1  These Brick-O-Lanterns are certainly all treat...  LEGO_Group   \n",
       "2  @dentistescabri Nous prenons la s√©curit√© de no...  LEGO_Group   \n",
       "3                                @Jasmin80212446 üòçüéÑü•∞  LEGO_Group   \n",
       "4  @ashleydrixey Sounds like a perfect fit for th...  LEGO_Group   \n",
       "\n",
       "                created_at timestamp archetype  \\\n",
       "0  2020-10-30 18:23:50.000       NaN    artist   \n",
       "1  2020-10-31 09:00:28.000       NaN    artist   \n",
       "2  2020-10-30 12:07:58.000       NaN    artist   \n",
       "3  2020-10-30 16:35:39.000       NaN    artist   \n",
       "4  2020-10-30 13:09:14.000       NaN    artist   \n",
       "\n",
       "                                     processed_tweet  \n",
       "0                                          ['excit']  \n",
       "1  ['brick', 'lantern', 'certain', 'treat', 'tric...  \n",
       "2  ['nous', 'prenon', 'curit', 'fan', 'rieux', 'p...  \n",
       "3                                                 []  \n",
       "4  ['sound', 'like', 'perfect', 'aspir', 'young',...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read .csv with processed tweets\n",
    "with open('twitter_database.csv', \"r\") as f:\n",
    "    df = pd.read_csv(f, index_col=0)\n",
    "    \n",
    "# Show head of DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace string processed_tweet with list-like\n",
    "df['processed_tweet'] = df['processed_tweet'].apply(lambda x: x.lstrip(\"[\").rstrip(\"]\").split(\",\"))\n",
    "\n",
    "# Drop tweets that have no words after processing\n",
    "df = df.drop(df[df['processed_tweet'].map(len) < 2].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataframe head after processing\n",
    "df.head()\n",
    "\n",
    "# Extract 1000 tweets from each archetype\n",
    "df = df.groupby('archetype').head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maelstro/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>''</th>\n",
       "      <th>'aah</th>\n",
       "      <th>'aarchi</th>\n",
       "      <th>'aaron</th>\n",
       "      <th>'aarzel</th>\n",
       "      <th>'aayush</th>\n",
       "      <th>'abandon</th>\n",
       "      <th>'abbey</th>\n",
       "      <th>'abbi</th>\n",
       "      <th>'abbiamo</th>\n",
       "      <th>...</th>\n",
       "      <th>'zulu</th>\n",
       "      <th>'zwei</th>\n",
       "      <th>'zyciora</th>\n",
       "      <th>``</th>\n",
       "      <th>gon</th>\n",
       "      <th>got</th>\n",
       "      <th>na</th>\n",
       "      <th>ta</th>\n",
       "      <th>wan</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.706564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.706564</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.706169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.706169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.706256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.706256</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.706619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.706619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.706460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.706460</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 11199 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ''  'aah   'aarchi    'aaron  'aarzel  'aayush  'abandon  'abbey  \\\n",
       "0  0.706564   0.0  0.000000  0.000000      0.0      0.0  0.000000     0.0   \n",
       "1  0.706169   0.0  0.000000  0.000000      0.0      0.0  0.000000     0.0   \n",
       "2  0.706256   0.0  0.000000  0.000310      0.0      0.0  0.000000     0.0   \n",
       "3  0.706619   0.0  0.000000  0.000116      0.0      0.0  0.000939     0.0   \n",
       "4  0.706460   0.0  0.000178  0.000000      0.0      0.0  0.000135     0.0   \n",
       "\n",
       "   'abbi  'abbiamo  ...     'zulu  'zwei  'zyciora        ``       gon  \\\n",
       "0    0.0       0.0  ...  0.000000    0.0       0.0  0.706564  0.000265   \n",
       "1    0.0       0.0  ...  0.000000    0.0       0.0  0.706169  0.000000   \n",
       "2    0.0       0.0  ...  0.000000    0.0       0.0  0.706256  0.000678   \n",
       "3    0.0       0.0  ...  0.000206    0.0       0.0  0.706619  0.000000   \n",
       "4    0.0       0.0  ...  0.000000    0.0       0.0  0.706460  0.000110   \n",
       "\n",
       "        got        na        ta       wan       was  \n",
       "0  0.000000  0.000443  0.000000  0.000292  0.000214  \n",
       "1  0.000142  0.000097  0.000142  0.000127  0.000000  \n",
       "2  0.000000  0.000568  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.000000  0.000092  0.000000  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 11199 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the words\n",
    "df_ta = pd.DataFrame(columns=[\"processed_tweet\", \"archetype\"])\n",
    "for arch in df[\"archetype\"].unique():\n",
    "    dummy = pd.DataFrame(columns=[\"processed_tweet\", \"archetype\"])\n",
    "    dummy[\"processed_tweet\"] = df[df[\"archetype\"] == arch].groupby(\"archetype\").apply(lambda x: (' '.join(str(txt) for txt in x[\"processed_tweet\"])))\n",
    "    dummy[\"archetype\"] = arch\n",
    "    df_ta = pd.concat([df_ta, dummy], ignore_index=True)\n",
    "\n",
    "# Tokenize the dataframe\n",
    "df_ta['processed_tweet'] = df_ta['processed_tweet'].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: filter_tokens(x))\n",
    "\n",
    "# Data vectorization\n",
    "vectorizer = TfidfVectorizer(input=\"content\", max_features=None, tokenizer=fcn_stub, preprocessor=fcn_stub)\n",
    "vectorizer.fit(df_ta['processed_tweet'])\n",
    "df_tfidf = vectorizer.transform(df_ta['processed_tweet'])\n",
    "df_tfidf = df_tfidf.toarray()\n",
    "\n",
    "# Get feature names\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocab = np.array(vocab)\n",
    "\n",
    "df_tfidf = pd.DataFrame(df_tfidf, columns=vocab)\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:04<00:00,  2.97it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11199/11199 [00:02<00:00, 3962.75it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11199/11199 [00:04<00:00, 2382.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11199/11199 [00:37<00:00, 296.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate PMI between words\n",
    "names = vocab\n",
    "name_idx = OrderedDict((name, 0) for name in names)\n",
    "word_to_index = OrderedDict((name, index) for index, name in enumerate(names))\n",
    "\n",
    "# Get the co-occurrences\n",
    "occurrences = np.zeros((len(names), len(names)), dtype=np.int32)\n",
    "windows_count = 0\n",
    "window = 10             # Sliding window size, for calculation PMI between words \n",
    "for l in tqdm(df_ta['processed_tweet'], total=len(df_ta['processed_tweet'])):\n",
    "    for i in range(len(l) - window):\n",
    "        windows_count += 1\n",
    "        d = set(l[i:(i+window)])\n",
    "        for w in d:\n",
    "            name_idx[w] += 1\n",
    "        for w1, w2 in combinations(d, 2):\n",
    "            i1 = word_to_index[w1]\n",
    "            i2 = word_to_index[w2]\n",
    "            \n",
    "            occurrences[i1][i2] = 1\n",
    "            occurrences[i2][i1] = 1\n",
    "            \n",
    "# Convert the occurences to PMI\n",
    "pmi_per_word = pd.DataFrame(occurrences, index=names, columns=names) / windows_count\n",
    "pmi_index = pd.Series(name_idx, index=name_idx.keys()) / windows_count\n",
    "\n",
    "# Free memory\n",
    "del occurrences\n",
    "del name_idx\n",
    "\n",
    "for col in tqdm(pmi_per_word.columns):\n",
    "    pmi_per_word[col] = pmi_per_word[col]/pmi_index[col]\n",
    "\n",
    "for row in tqdm(pmi_per_word.index):\n",
    "    pmi_per_word.loc[row, :] = pmi_per_word.loc[row, :] / pmi_index[row]\n",
    "    \n",
    "pmi_per_word = pmi_per_word + 1E-9\n",
    "for col in tqdm(pmi_per_word.columns):\n",
    "    pmi_per_word[col] = pmi_per_word[col].apply(lambda x: math.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:01<00:00,  9.42it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62703201/62703201 [06:39<00:00, 156856.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build a graph\n",
    "graph = nx.Graph()\n",
    "graph.add_nodes_from(df_tfidf.index)\n",
    "graph.add_nodes_from(vocab)\n",
    "\n",
    "# Build document-word edges\n",
    "document_word = [(doc,w,{\"weight\":df_tfidf.loc[doc,w]}) for doc in tqdm(df_tfidf.index, total=len(df_tfidf.index))\\\n",
    "                     for w in df_tfidf.columns]\n",
    "\n",
    "word_word = word_word_edges(pmi_per_word)\n",
    "graph.add_edges_from(document_word)\n",
    "graph.add_edges_from(word_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling current objects, in case of kernel crash\n",
    "import pickle\n",
    "\n",
    "def pickle_object(obj, file_name):\n",
    "    with open(f'agds_nx/{file_name}.pickle', \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "# Pickle graph, TF-IDF vectorizer and extracted DataFrame\n",
    "pickle_object(graph, \"graph\")\n",
    "pickle_object(df_tfidf, \"df_tfidf\")\n",
    "pickle_object(vectorizer, \"tfidf_vectorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "import matplotlib.pyplot as plt\n",
    "nx.draw(graph, with_labels=True, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
