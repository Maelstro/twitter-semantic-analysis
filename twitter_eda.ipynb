{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter - tweet analysis (date: 8.11.2020)\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans, DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used in the notebook\n",
    "def mongo_connect(server_name: str) -> MongoClient:\n",
    "    \"\"\"Creates connection to the MongoDB database with given server name.\"\"\"\n",
    "    client = MongoClient(server_name)\n",
    "    db = client.twitter_db\n",
    "    return db\n",
    "\n",
    "word_lemm = WordNetLemmatizer()\n",
    "stopwords_eng = stopwords.words('english')\n",
    "\n",
    "# Tweet preprocessing\n",
    "def preprocess_texts(text_list: pd.DataFrame):\n",
    "    \"\"\"Processes text to remove all unwanted words and symbols.\"\"\"\n",
    "\n",
    "    # Lowercase the tweets\n",
    "    text_list['processed_tweet'] = text_list['tweet_text'].str.lower()\n",
    "\n",
    "    # Regex patterns\n",
    "    url_pattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    user_pattern       = '@[^\\s]+'\n",
    "    alpha_pattern      = \"[^a-zA-Z0-9]\"\n",
    "    sequence_pattern   = r\"(.)\\1\\1+\"\n",
    "    seq_replace_pattern = r\"\\1\\1\"\n",
    "\n",
    "    # Remove URLs from the tweet text\n",
    "    text_list['processed_tweet'] = [re.sub(url_pattern, ' ', str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "    # Remove username from the tweet text\n",
    "    text_list['processed_tweet'] = [re.sub(user_pattern, ' ', str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "    # Remove all non-alphanumeric symbols\n",
    "    text_list['processed_tweet'] = [re.sub(alpha_pattern, ' ', str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "    # Replace all 3 or more consecutive letters with 2 letters\n",
    "    text_list['processed_tweet'] = [re.sub(sequence_pattern, seq_replace_pattern, str(x))\n",
    "                                    for x in text_list['processed_tweet']]\n",
    "\n",
    "    \n",
    "    full_tweet_list = []\n",
    "    for x in text_list['processed_tweet']:\n",
    "        full_tweet = ''\n",
    "        for word in x.split():\n",
    "            word = word_lemm.lemmatize(word)\n",
    "            full_tweet += (word + ' ')\n",
    "        full_tweet_list.append(full_tweet)\n",
    "\n",
    "    text_list['processed_tweet'] = full_tweet_list\n",
    "\n",
    "    return text_list\n",
    "\n",
    "def get_most_frequent_words(docs: list, most_frequent_count: int = 10) -> dict:\n",
    "    \"\"\"Gets the most frequent words from the dataset\"\"\"\n",
    "    word_count = {}\n",
    "    \n",
    "    for doc in docs:\n",
    "        for word in doc.lower().split():\n",
    "            if word not in word_count:\n",
    "                word_count[word] = 1\n",
    "            elif word in word_count:\n",
    "                word_count[word] += 1\n",
    "            \n",
    "    word_counter = collections.Counter(word_count)\n",
    "    \n",
    "    final_word_count = {}\n",
    "    \n",
    "    for word, count in word_counter.most_common(most_frequent_count):\n",
    "        final_word_count[word] = count\n",
    "    \n",
    "    final_word_count = pd.DataFrame({'Word': list(final_word_count.keys()), 'Word usage count': list(final_word_count.values())}, columns=['Word', 'Word usage count'])\n",
    "    return final_word_count\n",
    "\n",
    "def get_most_frequent_ngrams(docs: list, most_frequent_count: int = 10, ngram_count: int = 2) -> dict:\n",
    "    \"\"\"Gets the most frequent words from the dataset\"\"\" \n",
    "    final_ngram_count = {}\n",
    "    \n",
    "    for doc in docs:\n",
    "        ngram_counter = collections.Counter(nltk.ngrams(doc.split(), ngram_count))\n",
    "        for ngram, count in ngram_counter.most_common(most_frequent_count):\n",
    "            if ngram not in final_ngram_count:\n",
    "                final_ngram_count[ngram] = 1\n",
    "            else:\n",
    "                final_ngram_count[ngram] += int(count)\n",
    "    \n",
    "    final_ngram_count = pd.DataFrame({'Term': list(final_ngram_count.keys())[:most_frequent_count], \n",
    "                                      'Term usage count': sorted(list(final_ngram_count.values())[:most_frequent_count])[::-1]}, \n",
    "                                     columns=['Term', 'Term usage count'])\n",
    "    return final_ngram_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to local database\n",
    "db = mongo_connect('localhost')\n",
    "\n",
    "# Cursor for acquiring all posts\n",
    "cursor = db['artist'].find()\n",
    "\n",
    "artist_list = pd.DataFrame(list(cursor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>username</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cdc</td>\n",
       "      <td>@AndruEdwards The hard work has paid off, this...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-11-01 19:32:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cdd</td>\n",
       "      <td>@soosupersam A great way to surprise your love...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-11-01 19:09:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cde</td>\n",
       "      <td>You can now just bring the fun home, and reliv...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-11-01 14:00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cdf</td>\n",
       "      <td>@at_knb Happy birthday to the master builder! ...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-31 17:16:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5f9f1c36b38e10f823bf2ce0</td>\n",
       "      <td>@dizunatsu üòÄüòÄ</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-31 15:18:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  5f9f1c36b38e10f823bf2cdc   \n",
       "1  5f9f1c36b38e10f823bf2cdd   \n",
       "2  5f9f1c36b38e10f823bf2cde   \n",
       "3  5f9f1c36b38e10f823bf2cdf   \n",
       "4  5f9f1c36b38e10f823bf2ce0   \n",
       "\n",
       "                                          tweet_text    username  \\\n",
       "0  @AndruEdwards The hard work has paid off, this...  LEGO_Group   \n",
       "1  @soosupersam A great way to surprise your love...  LEGO_Group   \n",
       "2  You can now just bring the fun home, and reliv...  LEGO_Group   \n",
       "3  @at_knb Happy birthday to the master builder! ...  LEGO_Group   \n",
       "4                                      @dizunatsu üòÄüòÄ  LEGO_Group   \n",
       "\n",
       "           created_at  \n",
       "0 2020-11-01 19:32:05  \n",
       "1 2020-11-01 19:09:40  \n",
       "2 2020-11-01 14:00:36  \n",
       "3 2020-10-31 17:16:57  \n",
       "4 2020-10-31 15:18:50  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the array \n",
    "artist_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@soosupersam A great way to surprise your loved one! üéÅü•∞\n",
      "['the hard work ha paid off this is awesome ', 'a great way to surprise your loved one ', 'you can now just bring the fun home and relive your favorite childhood memory on sesame street ', 'happy birthday to the master builder we hope she had a magical day ', '', '', 'this is the way ', 'time to add a bit of legodots hocus pocus to your animal crossing island scan these code with your nooklink app to add to your collection acnh dotyourworld ', 'this is how you make all the ninja in the neighborhood jealous they look ninja tastic ', 'boo ', 'what a spooky ride ', 'these brick o lantern are certainly all treat and no trick get building this halloween rebuildtheworld ', '', 'u when we first saw the child set thisistheway legostarwars themandalorien thechild ', 'wow what a cool lego tower keep building and it ll be over your head in no time we can t wait to see what you use your imagination to create next rebuildtheworld ', 'the best way to spend a weekend happy building ', 'amazing job lewis he look proud of his work ', 'never too early to get into holiday spirit enjoy the build ', 'bonne construction ', 'so exciting have fun ', 'found your favorite already ', 'teacher are the brick that keep this world together ', 'you ve done both set justice awesome work ', 'what a talented little builder if she keep building like this the possibility are endless ', 'the trip of a lifetime have so much fun ', '', 'we know our fan are eager to start building the set wa officially released today so it should be on it way to you soon ', 'so exciting we hope you have a batty good time building the batwing ', 'we re always on the lookout for new idea did you know you can submit your own at ', 'max did a magical job he s a master builder in the making ', 'wearable art at it finest ', 'taking creativity to infinity and beyond ', 'we re glad you enjoyed the set so much we re wondering which set it replaced a your favorite ', 'we hope you have an awesome time building it ', 'a self taught poultry and building expert please give the young renaissance gentleman our best wish ', 'that is great to hear we hope you have a lovely time building it ', 'boo is loose in the lego super mario app go try out the new play feature available with the latest update ', 'thank you for making u aware of that we re looking into why that is ', 'hard to disagree with all that cuteness ', 'we are lucky to have the best fan in the world ', 'thanks for sharing this great picture we hope you enjoyed the build ', 'we re very sorry but once we retire a set we quickly sell off our remaining stock to make room for exciting new set it won t be possible to get that set from u anymore please give the young builder our best wish ', 'the safety of our young fan is very important to u if you have any concern in this regard or would like to chat about the step we take to make sure our lego brick are a safe a possible please get in touch with u here ', 'sound like a perfect fit for the aspiring young builder when he s a bit older of course ', 'what a perfect way to start your weekend ', 'that is a seriously heroic build are you sure alfred didn t help you out ', 'nous prenons la s curit de no fan tr s au s rieux et c est pourquoi tous le jouets lego sont test s et respectent ou d passent toutes le lois sur la s curit de jouets dans tous le pay o il sont vendus plus d info sur ', 'this adorable co pilot need no introduction starwars babyyoda ', 'viel spa beim bauen und entspannen ', 'aux derni re nouvelles oui mais restez connect s notre actualit sur notre site et sur ', 'oje bitte melde dich kurz bei unserem team auf und wir sehen un da an ', 'oh non nous somme d sol s pour cette exp rience pourriez vous nous envoyer un message priv nous serions ravis de vous aider ', 'it s possible you can purchase additional plate here we can t wait to see what you create ', 'we ll be happy to look into this for you please get in touch at or send u a private message ', 'the child is available for purchase right now in the uk at happy building ', 'quelle imagination bravo lui c est digne d un grand magicien ', 'pr te pour de aventures intergalactiques ce qu on peut voir ', 'can t wait to see it added to your already incredible collection happy building ', 'prepare for liftoff ', 'we hope you two had fun building ', 'double the awesome build ', 'we can t wait to see your build don t hesitate to share all your creation ', 'enjoy ', 'happy building ', 'coming very soon on october 30th ', 'we hope you enjoyed every minute of the build ', 'the cutest of the galaxy ', 'spooktacular build ', 'may the force be with you the child will be worth the wait ', 'what a brick tastic representation ', 'update your old office sound like the coolest office ever ', 'what a stellar gift happy building ', 'just because you cannot see it doesn t mean you can t believe it ', 'we re sorry to hear you re having these issue with your set if you give u a shout at our expert would be more than happy to help you figure out the solution ', 'grown ups need to play too ', 'we re working hard to make this amazing new set available again asap for our awesome fan we don t have an exact date for when it ll be back in stock but keep an eye on for the latest info ', 'amazing photo one of our favorite view ', 'coolest office ever ', 'it ll make an adorable addition ', 'we couldn t decide which photo of the child to post today so we ve used all of them thisistheway legostarwars starwars thechild ', 'send u a dm with your info we d love to have a look ', 'that s a lucky 7 year old happy birthday ', 'yes you can bet your millennium falcon and have faith in the force that we ll get more of the mo eisley cantina in keep an eye on the site here ', 'we don t have a part catalog but have a look at our online pick a brick wall ', 'a lego brick a day keep the stress away ', '10 10 would wear ', 'keep an eye on our website for announcement on new set and promotion ', 'bonne construction ', 'now that s one cool cat ', '', 'the instruction are available in the lego super mario app but you can download all the building instruction directly from a well happy building ', 'both are available to purchase separately from our brick amp piece site the design number are 35499 and 41124 ', 'love it ', 'oh no your upside down is definitely not supposed to fall apart when assembling please make sure the pillar are pressed firmly into the roof before connecting the two house page 114 116 book 1 it can also help to find some video of people building the set ', 'you re welcome ', 'amazing it seems that lily and james were meant to be ', 'we do too will you be decorating your wall with the fab four or with marilyn ', 'wow this look amazing ', 'send u a dm we d be happy to take a look at that order for you ', 'your donated brick will make some kid very happy rebuildtheworld ']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the texts\n",
    "artist_list_processed = preprocess_texts(artist_list)\n",
    "docs = artist_list_processed['processed_tweet'].tolist()\n",
    "\n",
    "print(artist_list['tweet_text'][1])\n",
    "print(docs[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Word  Word usage count\n",
      "0      the              1694\n",
      "1       to              1075\n",
      "2      you               961\n",
      "3        a               801\n",
      "4      and               710\n",
      "..     ...               ...\n",
      "95     way                51\n",
      "96    just                51\n",
      "97   sorry                51\n",
      "98  repair                51\n",
      "99   these                50\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "                Term  Term usage count\n",
      "0        (the, hard)               128\n",
      "1       (hard, work)               110\n",
      "2         (work, ha)                89\n",
      "3         (ha, paid)                36\n",
      "4        (paid, off)                32\n",
      "..               ...               ...\n",
      "95         (and, no)                 1\n",
      "96       (no, trick)                 1\n",
      "97      (trick, get)                 1\n",
      "98   (get, building)                 1\n",
      "99  (building, this)                 1\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "freq_list = get_most_frequent_words(docs, 100)\n",
    "print(freq_list)\n",
    "\n",
    "ngram_list = get_most_frequent_ngrams(docs, 100, 2)\n",
    "print(ngram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent words and bigrams for \"artist\" archetype:\n",
      "             Word  Word usage count\n",
      "0             the              1694\n",
      "1              to              1075\n",
      "2             you               961\n",
      "3               a               801\n",
      "4             and               710\n",
      "..            ...               ...\n",
      "995         ninja                 4\n",
      "996  legostarwars                 4\n",
      "997      thechild                 4\n",
      "998         proud                 4\n",
      "999       teacher                 4\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "                Term  Term usage count\n",
      "0        (the, hard)               128\n",
      "1       (hard, work)               110\n",
      "2         (work, ha)                89\n",
      "3         (ha, paid)                36\n",
      "4        (paid, off)                32\n",
      "..               ...               ...\n",
      "95         (and, no)                 1\n",
      "96       (no, trick)                 1\n",
      "97      (trick, get)                 1\n",
      "98   (get, building)                 1\n",
      "99  (building, this)                 1\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "The most frequent words and bigrams for \"caregiver\" archetype:\n",
      "         Word  Word usage count\n",
      "0          to               697\n",
      "1         the               556\n",
      "2         you               446\n",
      "3          we               435\n",
      "4           a               323\n",
      "..        ...               ...\n",
      "995        bw                 2\n",
      "996       fbk                 2\n",
      "997   alerted                 2\n",
      "998  upcoming                 2\n",
      "999     cause                 2\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "              Term  Term usage count\n",
      "0         (we, re)               105\n",
      "1      (re, sorry)               104\n",
      "2      (sorry, to)                94\n",
      "3       (to, hear)                89\n",
      "4    (hear, about)                81\n",
      "..             ...               ...\n",
      "95       (to, pas)                 2\n",
      "96     (pas, this)                 2\n",
      "97   (this, along)                 2\n",
      "98     (along, to)                 1\n",
      "99  (our, product)                 1\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "The most frequent words and bigrams for \"everyman\" archetype:\n",
      "           Word  Word usage count\n",
      "0            we               725\n",
      "1           you               685\n",
      "2           the               661\n",
      "3            to               622\n",
      "4             a               456\n",
      "..          ...               ...\n",
      "995  engagement                 2\n",
      "996        00am                 2\n",
      "997         sat                 2\n",
      "998          jr                 2\n",
      "999        brad                 2\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "                                 Term  Term usage count\n",
      "0                          (this, is)               112\n",
      "1                            (is, my)                79\n",
      "2                           (my, bud)                61\n",
      "3                        (bud, nacho)                39\n",
      "4                        (nacho, bud)                36\n",
      "..                                ...               ...\n",
      "95                      (reply, with)                 1\n",
      "96                      (with, using)                 1\n",
      "97        (using, vintagebudforvotes)                 1\n",
      "98  (vintagebudforvotes, sweepstakes)                 1\n",
      "99                 (sweepstakes, for)                 1\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "The most frequent words and bigrams for \"explorer\" archetype:\n",
      "           Word  Word usage count\n",
      "0           the              1285\n",
      "1             a               871\n",
      "2            to               825\n",
      "3            of               585\n",
      "4           and               576\n",
      "..          ...               ...\n",
      "995        deep                 4\n",
      "996     twitter                 4\n",
      "997  researcher                 4\n",
      "998   including                 4\n",
      "999         1st                 4\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "                   Term  Term usage count\n",
      "0           (a, maggie)               134\n",
      "1   (maggie, continues)                78\n",
      "2       (continues, to)                42\n",
      "3            (to, seek)                34\n",
      "4        (seek, amends)                30\n",
      "..                  ...               ...\n",
      "95        (the, crisis)                 2\n",
      "96      (crisis, reach)                 2\n",
      "97     (reach, boiling)                 2\n",
      "98     (boiling, point)                 2\n",
      "99      (point, fraser)                 2\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "The most frequent words and bigrams for \"guru\" archetype:\n",
      "          Word  Word usage count\n",
      "0          the              1315\n",
      "1           to               954\n",
      "2            a               722\n",
      "3          and               597\n",
      "4           of               580\n",
      "..         ...               ...\n",
      "995  continues                 4\n",
      "996       view                 4\n",
      "997  continued                 4\n",
      "998     spread                 4\n",
      "999       mask                 4\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "                     Term  Term usage count\n",
      "0          (crew, dragon)                54\n",
      "1    (dragon, spacecraft)                38\n",
      "2       (spacecraft, for)                13\n",
      "3              (for, the)                10\n",
      "4             (the, crew)                10\n",
      "..                    ...               ...\n",
      "95        (80, favorable)                 2\n",
      "96         (static, fire)                 2\n",
      "97           (fire, test)                 2\n",
      "98       (test, complete)                 1\n",
      "99  (complete, targeting)                 1\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "The most frequent words and bigrams for \"hero\" archetype:\n",
      "           Word  Word usage count\n",
      "0           the               583\n",
      "1            to               505\n",
      "2           you               341\n",
      "3           and               296\n",
      "4            we               289\n",
      "..          ...               ...\n",
      "995  evacuation                 2\n",
      "996        matt                 2\n",
      "997    guidance                 2\n",
      "998     greater                 2\n",
      "999       sleep                 2\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "             Term  Term usage count\n",
      "0     (hi, sorry)                52\n",
      "1    (sorry, you)                44\n",
      "2     (you, feel)                26\n",
      "3    (feel, that)                25\n",
      "4     (that, way)                22\n",
      "..            ...               ...\n",
      "95    (hagas, la)                 1\n",
      "96   (la, compra)                 1\n",
      "97  (compra, por)                 1\n",
      "98      (por, pc)                 1\n",
      "99   (hola, luis)                 1\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "The most frequent words and bigrams for \"innocent\" archetype:\n",
      "             Word  Word usage count\n",
      "0              to               614\n",
      "1             you               584\n",
      "2             the               583\n",
      "3              we               525\n",
      "4               a               484\n",
      "..            ...               ...\n",
      "995  manufacturer                 2\n",
      "996          lost                 2\n",
      "997         paige                 2\n",
      "998    california                 2\n",
      "999        camper                 2\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "                       Term  Term usage count\n",
      "0                  (we, re)               111\n",
      "1          (we, appreciate)               105\n",
      "2        (appreciate, your)               101\n",
      "3           (your, loyalty)                94\n",
      "4             (loyalty, in)                79\n",
      "..                      ...               ...\n",
      "95       (are, temporarily)                 1\n",
      "96  (temporarily, shifting)                 1\n",
      "97          (shifting, our)                 1\n",
      "98                (hi, jay)                 1\n",
      "99             (jay, could)                 1\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "The most frequent words and bigrams for \"jester\" archetype:\n",
      "        Word  Word usage count\n",
      "0        the               788\n",
      "1         to               746\n",
      "2        you               676\n",
      "3         we               663\n",
      "4          a               513\n",
      "..       ...               ...\n",
      "995  craving                 3\n",
      "996   cherry                 3\n",
      "997   garcia                 3\n",
      "998     sink                 3\n",
      "999      mia                 3\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "                Term  Term usage count\n",
      "0        (we, thank)                72\n",
      "1       (thank, the)                70\n",
      "2         (the, for)                44\n",
      "3       (for, their)                40\n",
      "4   (their, support)                31\n",
      "..               ...               ...\n",
      "95     (the, region)                 2\n",
      "96      (the, fight)                 2\n",
      "97      (fight, for)                 2\n",
      "98  (for, democracy)                 2\n",
      "99  (democracy, doe)                 2\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "The most frequent words and bigrams for \"magician\" archetype:\n",
      "            Word  Word usage count\n",
      "0            you               959\n",
      "1             to               857\n",
      "2            the               781\n",
      "3              a               717\n",
      "4             we               712\n",
      "..           ...               ...\n",
      "995         film                 3\n",
      "996       corner                 3\n",
      "997  frustration                 3\n",
      "998          far                 3\n",
      "999        becca                 3\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "                Term  Term usage count\n",
      "0        (so, sorry)               115\n",
      "1        (sorry, to)                85\n",
      "2         (to, hear)                80\n",
      "3       (hear, this)                62\n",
      "4   (this, happened)                52\n",
      "..               ...               ...\n",
      "95      (best, wish)                 1\n",
      "96     (wish, steve)                 1\n",
      "97        (steve, 2)                 1\n",
      "98            (2, 2)                 1\n",
      "99         (in, our)                 1\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "The most frequent words and bigrams for \"rebel\" archetype:\n",
      "        Word  Word usage count\n",
      "0        the               511\n",
      "1         to               465\n",
      "2          a               365\n",
      "3         we               322\n",
      "4        and               273\n",
      "..       ...               ...\n",
      "995  written                 2\n",
      "996  profile                 2\n",
      "997  started                 2\n",
      "998  billion                 2\n",
      "999    pound                 2\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "              Term  Term usage count\n",
      "0    (fiery, warm)                68\n",
      "1     (warm, tone)                65\n",
      "2       (tone, we)                52\n",
      "3         (we, re)                47\n",
      "4      (re, drawn)                39\n",
      "..             ...               ...\n",
      "95     (new, mini)                 1\n",
      "96  (palette, for)                 1\n",
      "97       (for, on)                 1\n",
      "98       (on, the)                 1\n",
      "99       (the, go)                 1\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "The most frequent words and bigrams for \"ruler\" archetype:\n",
      "         Word  Word usage count\n",
      "0         the               909\n",
      "1          to               742\n",
      "2           a               503\n",
      "3         you               453\n",
      "4         and               421\n",
      "..        ...               ...\n",
      "995     alike                 4\n",
      "996   scaling                 4\n",
      "997    income                 4\n",
      "998  stronger                 4\n",
      "999      amex                 4\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "                      Term  Term usage count\n",
      "0             (join, chef)                47\n",
      "1              (chef, and)                38\n",
      "2           (and, tonight)                28\n",
      "3            (tonight, at)                13\n",
      "4                  (at, 6)                11\n",
      "..                     ...               ...\n",
      "95             (2016, for)                 1\n",
      "96              (for, his)                 1\n",
      "97       (his, pioneering)                 1\n",
      "98  (pioneering, research)                 1\n",
      "99          (research, in)                 1\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "The most frequent words and bigrams for \"seducer\" archetype:\n",
      "                 Word  Word usage count\n",
      "0                 the               332\n",
      "1                  to               309\n",
      "2                  we               290\n",
      "3                   a               240\n",
      "4                 you               221\n",
      "..                ...               ...\n",
      "995             pecan                 1\n",
      "996            square                 1\n",
      "997         condition                 1\n",
      "998  welcomebackimola                 1\n",
      "999             track                 1\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "                   Term  Term usage count\n",
      "0       (this, holiday)                30\n",
      "1     (holiday, season)                27\n",
      "2     (season, immerse)                21\n",
      "3   (immerse, yourself)                21\n",
      "4        (yourself, in)                20\n",
      "..                  ...               ...\n",
      "95     (technique, new)                 2\n",
      "96           (new, for)                 2\n",
      "97       (jones, laser)                 2\n",
      "98         (laser, cut)                 2\n",
      "99           (cut, the)                 2\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline for getting most frequent words from the collection\n",
    "archetype_list = ['artist',\n",
    "                 'caregiver',\n",
    "                 'everyman',\n",
    "                 'explorer',\n",
    "                 'guru',\n",
    "                 'hero',\n",
    "                 'innocent',\n",
    "                 'jester',\n",
    "                 'magician',\n",
    "                 'rebel',\n",
    "                 'ruler',\n",
    "                 'seducer']\n",
    "\n",
    "for archetype in archetype_list:\n",
    "    # Create a cursor for acquiring all posts from the collection\n",
    "    cursor = db[archetype].find()\n",
    "    \n",
    "    # Create a DataFrame with all the tweet text\n",
    "    df_name = archetype + \"_list\"\n",
    "    globals()[df_name] = pd.DataFrame(list(cursor))\n",
    "    \n",
    "    # Preprocess the texts\n",
    "    globals()[df_name+\"_processed\"] = preprocess_texts(globals()[df_name])\n",
    "    \n",
    "    # Create a list out of processed tweets\n",
    "    docs = globals()[df_name+\"_processed\"]['processed_tweet'].tolist()\n",
    "    \n",
    "    # Get the list of the most frequent words\n",
    "    freq_list = get_most_frequent_words(docs, 1000)\n",
    "    ngram_list = get_most_frequent_ngrams(docs, 100, 2)\n",
    "    print(f'The most frequent words and bigrams for \"{archetype}\" archetype:')\n",
    "    print(freq_list)\n",
    "    print(ngram_list)\n",
    "    \n",
    "    # Save the most frequent word list to a Pickle\n",
    "    file_path = f'archetype_freq_15112020/{archetype}_single.pickle'\n",
    "    with open(file_path, \"wb\") as out_file:\n",
    "        pickle.dump(freq_list, out_file)\n",
    "        \n",
    "    file_path = f'archetype_freq_15112020/{archetype}_ngrams.pickle'\n",
    "    with open(file_path, \"wb\") as out_file:\n",
    "        pickle.dump(ngram_list, out_file)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (frapol)",
   "language": "python",
   "name": "frapol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
