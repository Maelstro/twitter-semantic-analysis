{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different approach - use only document frequency as importance coefficient\n",
    "# SPMF RETURS DOCUMENT FREQUENCY!!!!!!!!!\n",
    "from spmf import Spmf\n",
    "import pandas as pd\n",
    "from text_cleaner import *\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "archetype_list = ['artist',\n",
    "                 'caregiver',\n",
    "                 'everyman',\n",
    "                 'explorer',\n",
    "                 'guru',\n",
    "                 'hero',\n",
    "                 'innocent',\n",
    "                 'jester',\n",
    "                 'magician',\n",
    "                 'rebel',\n",
    "                 'ruler',\n",
    "                 'seducer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>username</th>\n",
       "      <th>created_at</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>archetype</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cdc</td>\n",
       "      <td>@AndruEdwards The hard work has paid off, this...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-11-01 19:32:05.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[hard, work, paid, awesome]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cdd</td>\n",
       "      <td>@soosupersam A great way to surprise your love...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-11-01 19:09:40.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[great, way, surprise, loved, one]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cde</td>\n",
       "      <td>You can now just bring the fun home, and reliv...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-11-01 14:00:36.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[bring, fun, home, relive, favorite, childhood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5f9f1c36b38e10f823bf2cdf</td>\n",
       "      <td>@at_knb Happy birthday to the master builder! ...</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-31 17:16:57.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[happy, birthday, master, builder, hope, magic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5f9f1c36b38e10f823bf2ce2</td>\n",
       "      <td>@Ranchie This is the way! ðŸ˜€</td>\n",
       "      <td>LEGO_Group</td>\n",
       "      <td>2020-10-31 15:16:26.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist</td>\n",
       "      <td>[way]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  5f9f1c36b38e10f823bf2cdc   \n",
       "1  5f9f1c36b38e10f823bf2cdd   \n",
       "2  5f9f1c36b38e10f823bf2cde   \n",
       "3  5f9f1c36b38e10f823bf2cdf   \n",
       "6  5f9f1c36b38e10f823bf2ce2   \n",
       "\n",
       "                                          tweet_text    username  \\\n",
       "0  @AndruEdwards The hard work has paid off, this...  LEGO_Group   \n",
       "1  @soosupersam A great way to surprise your love...  LEGO_Group   \n",
       "2  You can now just bring the fun home, and reliv...  LEGO_Group   \n",
       "3  @at_knb Happy birthday to the master builder! ...  LEGO_Group   \n",
       "6                        @Ranchie This is the way! ðŸ˜€  LEGO_Group   \n",
       "\n",
       "                created_at timestamp archetype  \\\n",
       "0  2020-11-01 19:32:05.000       NaN    artist   \n",
       "1  2020-11-01 19:09:40.000       NaN    artist   \n",
       "2  2020-11-01 14:00:36.000       NaN    artist   \n",
       "3  2020-10-31 17:16:57.000       NaN    artist   \n",
       "6  2020-10-31 15:16:26.000       NaN    artist   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0                        [hard, work, paid, awesome]  \n",
       "1                 [great, way, surprise, loved, one]  \n",
       "2  [bring, fun, home, relive, favorite, childhood...  \n",
       "3  [happy, birthday, master, builder, hope, magic...  \n",
       "6                                              [way]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Twitter dataset\n",
    "twitter_df = pd.read_csv('tweets_06_03_2021.csv', index_col=0)\n",
    "\n",
    "# Print the head of the loaded dataset\n",
    "twitter_df.head()\n",
    "\n",
    "# Clean-up the texts\n",
    "twitter_df[\"cleaned_text\"] = twitter_df[\"tweet_text\"].apply(lambda x: clean_up_text(x))\n",
    "\n",
    "# Tokenize the cleaned texts\n",
    "twitter_df[\"cleaned_text\"] = twitter_df[\"cleaned_text\"].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Remove the stopwords\n",
    "twitter_df[\"cleaned_text\"] = twitter_df[\"cleaned_text\"].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# Drop the rows with empty 'cleaned_text' field\n",
    "twitter_df = twitter_df.drop(twitter_df[twitter_df['cleaned_text'].map(len) < 1].index)\n",
    "\n",
    "# Print the new head of the dataset\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          [hard, work, paid, awesome]\n",
       "1                   [great, way, surprise, loved, one]\n",
       "2    [bring, fun, home, relive, favorite, childhood...\n",
       "3    [happy, birthday, master, builder, hope, magic...\n",
       "4                                                [way]\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1 - try to get the most occurring words in the 'artist' archetype subset\n",
    "# Extract all the tweets for the 'artist' archetype\n",
    "artist_df = twitter_df.cleaned_text[twitter_df[\"archetype\"] == \"artist\"]\n",
    "\n",
    "# Reset the index of the subset\n",
    "artist_df = artist_df.reset_index(drop=True)\n",
    "\n",
    "# Print the head of the subset\n",
    "artist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the word lists to full sentences, detokenization\n",
    "artist_df = pd.concat([artist_df, artist_df.apply(lambda x: \" \".join(x))], axis=1)\n",
    "artist_df.columns.values[1] = \"full_sentence\"\n",
    "\n",
    "# Drop duplicates\n",
    "artist_df.sort_values(\"full_sentence\", inplace = True) \n",
    "artist_df.drop_duplicates(subset=\"full_sentence\", keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/mnt/HDD_Linux/Praca_magisterska/jupyter_notebooks/data_mining/spmf.jar\n",
      "Converting TEXT to SPMF format.\n",
      "Conversion completed.\n",
      "=============  PREFIXSPAN 0.99-2016 - STATISTICS =============\n",
      " Total time ~ 235 ms\n",
      " Frequent sequences count : 28796\n",
      " Max memory (mb) : 82.09123229980469\n",
      " minsup = 4 sequences.\n",
      " Pattern count : 28796\n",
      "===================================================\n",
      "\n",
      "Post-processing to show result in terms of string values.\n",
      "Post-processing completed.\n",
      "\n",
      "                         pattern       sup\n",
      "0                         [able]  0.008091\n",
      "1                   [able, able]  0.000729\n",
      "2           [able, able, advise]  0.000139\n",
      "3             [able, able, look]  0.000139\n",
      "4      [able, able, information]  0.000139\n",
      "...                          ...       ...\n",
      "28791                    [grabs]  0.000208\n",
      "28792             [grabs, today]  0.000174\n",
      "28793                 [giveaway]  0.000139\n",
      "28794                  [surreal]  0.000139\n",
      "28795                  [venture]  0.000139\n",
      "\n",
      "[28796 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Archetype vocabulary - get the vocabulary based on SPMF results (it' fast)\n",
    "text_list = artist_df[\"full_sentence\"].tolist()\n",
    "\n",
    "spmf = Spmf(\"PrefixSpan\", input_direct=text_list,\n",
    "                output_filename=f\"sequence_files_for_tfidf/output_artist.txt\", arguments=[0.0007, 3], input_type=\"text\")\n",
    "spmf.run()\n",
    "\n",
    "spmf = spmf.to_pandas_dataframe()\n",
    "\n",
    "spmf[\"sup\"] = spmf[\"sup\"] / len(spmf)\n",
    "print(spmf)\n",
    "spmf.to_csv(f\"sequence_files/output_artist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list = []\n",
    "for _, row in spmf.iterrows():\n",
    "    phrase_list.append(tuple(row[\"pattern\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5612 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-ccacee0f84f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0msent_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0msentence_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-255-ccacee0f84f5>\u001b[0m in \u001b[0;36mget_bow\u001b[0;34m(sen, vocab)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtokenized_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     combined_sentence = list(itertools.chain.from_iterable([itertools.combinations(tokenized_sentence, 1),\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# Calculate Bag of Words\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "# Get term sequency per sentence\n",
    "def get_bow(sen, vocab):\n",
    "\n",
    "    vector = [0] * len(vocab)\n",
    "    tokenized_sentence = nltk.word_tokenize(sen)\n",
    "    combined_sentence = list(itertools.chain.from_iterable([itertools.combinations(tokenized_sentence, 1),\n",
    "                                                   itertools.combinations(tokenized_sentence, 2),\n",
    "                                                   itertools.combinations(tokenized_sentence, 3)]))\n",
    "    for el in combined_sentence:\n",
    "        if el in vocab:\n",
    "            cnt = combined_sentence.count(el)\n",
    "            idx = vocab.index(el)\n",
    "            vector[idx] = cnt\n",
    "    return vector\n",
    "\n",
    "sentence_vectors = []\n",
    "for sentence in tqdm(text_list):\n",
    "    sent_vec = get_bow\n",
    "    sentence_vectors.append(get_bow(sentence, phrase_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save BoW to file\n",
    "sentence_vectors = np.asarray(sentence_vectors)\n",
    "pd.DataFrame(sentence_vectors).to_csv(\"artist_bow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF-like metric out of Bag-of-Words\n",
    "artist_tfidf = pd.DataFrame(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(able,)</th>\n",
       "      <th>(able, able)</th>\n",
       "      <th>(able, able, advise)</th>\n",
       "      <th>(able, able, look)</th>\n",
       "      <th>(able, able, information)</th>\n",
       "      <th>(able, able, provide)</th>\n",
       "      <th>(able, add)</th>\n",
       "      <th>(able, printer)</th>\n",
       "      <th>(able, still)</th>\n",
       "      <th>(able, working)</th>\n",
       "      <th>...</th>\n",
       "      <th>(avez, vous)</th>\n",
       "      <th>(sit,)</th>\n",
       "      <th>(wonderland,)</th>\n",
       "      <th>(splatoween,)</th>\n",
       "      <th>(spotted,)</th>\n",
       "      <th>(grabs,)</th>\n",
       "      <th>(grabs, today)</th>\n",
       "      <th>(giveaway,)</th>\n",
       "      <th>(surreal,)</th>\n",
       "      <th>(venture,)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28796 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   (able,)  (able, able)  (able, able, advise)  (able, able, look)  \\\n",
       "0        0             0                     0                   0   \n",
       "1        0             0                     0                   0   \n",
       "2        0             0                     0                   0   \n",
       "3        0             0                     0                   0   \n",
       "4        0             0                     0                   0   \n",
       "\n",
       "   (able, able, information)  (able, able, provide)  (able, add)  \\\n",
       "0                          0                      0            0   \n",
       "1                          0                      0            0   \n",
       "2                          0                      0            0   \n",
       "3                          0                      0            0   \n",
       "4                          0                      0            0   \n",
       "\n",
       "   (able, printer)  (able, still)  (able, working)  ...  (avez, vous)  (sit,)  \\\n",
       "0                0              0                0  ...             0       0   \n",
       "1                0              0                0  ...             0       0   \n",
       "2                0              0                0  ...             0       0   \n",
       "3                0              0                0  ...             0       0   \n",
       "4                0              0                0  ...             0       0   \n",
       "\n",
       "   (wonderland,)  (splatoween,)  (spotted,)  (grabs,)  (grabs, today)  \\\n",
       "0              0              0           0         0               0   \n",
       "1              0              0           0         0               0   \n",
       "2              0              0           0         0               0   \n",
       "3              0              0           0         0               0   \n",
       "4              0              0           0         0               0   \n",
       "\n",
       "   (giveaway,)  (surreal,)  (venture,)  \n",
       "0            0           0           0  \n",
       "1            0           0           0  \n",
       "2            0           0           0  \n",
       "3            0           0           0  \n",
       "4            0           0           0  \n",
       "\n",
       "[5 rows x 28796 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artist_tfidf.columns = phrase_list\n",
    "artist_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sum of all nonzeros in all columns\n",
    "term_freq = artist_tfidf.astype(int).sum(axis=0)\n",
    "doc_freq = artist_tfidf.astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF for first term: [5379]\n"
     ]
    }
   ],
   "source": [
    "cols = artist_tfidf.columns\n",
    "print(f\"DF for first term: {np.unique(doc_freq[cols[0]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5379]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(term_freq[cols[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tf and doc frequencies to get our metrics\n",
    "import math\n",
    "metric = lambda tf, docf: (float(tf / len(cols)) * math.log(len(artist_df) / (docf + 1)))\n",
    "metric_df = term_freq.combine(doc_freq, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(able,)                      0.007886\n",
       "(able, able)                 0.000693\n",
       "(able, able, advise)         0.000243\n",
       "(able, able, look)           0.000243\n",
       "(able, able, information)    0.000243\n",
       "dtype: float64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer - get the term frequency\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class DenseCount(CountVectorizer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "    def transform(self, x, y=None) -> pd.DataFrame:\n",
    "        res = super().transform(x)\n",
    "        df = pd.DataFrame(res.toarray(), columns=self.get_feature_names())\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, x, y=None) -> pd.DataFrame:\n",
    "        # run sklearn's fit_transform\n",
    "        res = super().fit_transform(x, y=y)\n",
    "        # convert the returned sparse documents-terms matrix into a dataframe to further manipulations\n",
    "        df = pd.DataFrame(res.toarray(), columns=self.get_feature_names())\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_v2 = [' '.join(el).lstrip(' ').rstrip(' ') for el in phrase_list]\n",
    "tf_docs_terms = DenseCount(vocabulary=phrase_v2).fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>able able</th>\n",
       "      <th>able able advise</th>\n",
       "      <th>able able look</th>\n",
       "      <th>able able information</th>\n",
       "      <th>able able provide</th>\n",
       "      <th>able add</th>\n",
       "      <th>able printer</th>\n",
       "      <th>able still</th>\n",
       "      <th>able working</th>\n",
       "      <th>...</th>\n",
       "      <th>avez vous</th>\n",
       "      <th>sit</th>\n",
       "      <th>wonderland</th>\n",
       "      <th>splatoween</th>\n",
       "      <th>spotted</th>\n",
       "      <th>grabs</th>\n",
       "      <th>grabs today</th>\n",
       "      <th>giveaway</th>\n",
       "      <th>surreal</th>\n",
       "      <th>venture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28796 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  able able  able able advise  able able look  able able information  \\\n",
       "0     2          0                 0               0                      0   \n",
       "1     1          0                 0               0                      0   \n",
       "2     1          0                 0               0                      0   \n",
       "3     1          0                 0               0                      0   \n",
       "4     1          0                 0               0                      0   \n",
       "\n",
       "   able able provide  able add  able printer  able still  able working  ...  \\\n",
       "0                  0         0             0           0             0  ...   \n",
       "1                  0         0             0           0             0  ...   \n",
       "2                  0         0             0           0             0  ...   \n",
       "3                  0         0             0           0             0  ...   \n",
       "4                  0         0             0           0             0  ...   \n",
       "\n",
       "   avez vous  sit  wonderland  splatoween  spotted  grabs  grabs today  \\\n",
       "0          0    0           0           0        0      0            0   \n",
       "1          0    0           0           0        0      0            0   \n",
       "2          0    0           0           0        0      0            0   \n",
       "3          0    0           0           0        0      0            0   \n",
       "4          0    0           0           0        0      0            0   \n",
       "\n",
       "   giveaway  surreal  venture  \n",
       "0         0        0        0  \n",
       "1         0        0        0  \n",
       "2         0        0        0  \n",
       "3         0        0        0  \n",
       "4         0        0        0  \n",
       "\n",
       "[5 rows x 28796 columns]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_docs_terms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "able add printer mac still working advise calling support team mon fri excluding bank holidays would able look\n"
     ]
    }
   ],
   "source": [
    "sample_text = text_list[0]\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text.split(\" \").count(('able', 'able'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get term sequency per sentence\n",
    "def get_bow(sen, vocab):\n",
    "\n",
    "    vector = [0] * len(vocab)\n",
    "    tokenized_sentence = nltk.word_tokenize(sen)\n",
    "    combined_sentence = list(itertools.chain.from_iterable([itertools.combinations(tokenized_sentence, 1),\n",
    "                                                   itertools.combinations(tokenized_sentence, 2),\n",
    "                                                   itertools.combinations(tokenized_sentence, 3)]))\n",
    "    for el in combined_sentence:\n",
    "        if el in vocab:\n",
    "            cnt = combined_sentence.count(el)\n",
    "            idx = vocab.index(el)\n",
    "            vector[idx] += cnt\n",
    "    return vector\n",
    "\n",
    "sentence_vectors = []\n",
    "for sentence in tqdm(text_list):\n",
    "    sent_vec = get_bow\n",
    "    sentence_vectors.append(get_bow(sentence, phrase_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aa',), ('aa', 'aa'), ('aa', 'bb'), ('aa', 'bb', 'aa'), ('aa', 'bb', 'cc'), ('aa', 'cc'), ('aa', 'cc', 'aa'), ('bb',), ('bb', 'aa'), ('bb', 'cc'), ('bb', 'cc', 'aa'), ('cc',), ('cc', 'aa')]\n",
      "13\n",
      "13\n",
      "Count of ('aa',), with index 0: 2\n",
      "Count of ('bb',), with index 7: 1\n",
      "Count of ('cc',), with index 11: 1\n",
      "Count of ('aa',), with index 0: 2\n",
      "Count of ('aa', 'bb'), with index 2: 1\n",
      "Count of ('aa', 'cc'), with index 5: 1\n",
      "Count of ('aa', 'aa'), with index 1: 1\n",
      "Count of ('bb', 'cc'), with index 9: 1\n",
      "Count of ('bb', 'aa'), with index 8: 1\n",
      "Count of ('cc', 'aa'), with index 12: 1\n",
      "Count of ('aa', 'bb', 'cc'), with index 4: 1\n",
      "Count of ('aa', 'bb', 'aa'), with index 3: 1\n",
      "Count of ('aa', 'cc', 'aa'), with index 6: 1\n",
      "Count of ('bb', 'cc', 'aa'), with index 10: 1\n"
     ]
    }
   ],
   "source": [
    "pats = spmf_test[\"pattern\"].tolist()\n",
    "pats = [tuple(pat) for pat in pats]\n",
    "print\n",
    "ress = get_bow('aa bb cc aa', pats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aa',), ('aa', 'aa'), ('aa', 'bb'), ('aa', 'bb', 'aa'), ('aa', 'bb', 'cc'), ('aa', 'cc'), ('aa', 'cc', 'aa'), ('bb',), ('bb', 'aa'), ('bb', 'cc'), ('bb', 'cc', 'aa'), ('cc',), ('cc', 'aa')]\n",
      "   (aa,)  (aa, aa)  (aa, bb)  (aa, bb, aa)  (aa, bb, cc)  (aa, cc)  \\\n",
      "0      0         0         0             0             0         0   \n",
      "\n",
      "   (aa, cc, aa)  (bb,)  (bb, aa)  (bb, cc)  (bb, cc, aa)  (cc,)  (cc, aa)  \n",
      "0             0      0         0         0             0      0         0  \n"
     ]
    }
   ],
   "source": [
    "print(pats)\n",
    "print(ress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('able',), ('able', 'us', 'software'), ('able', 'back'), ('printer', 'holidays'), ('printer', 'information')]\n"
     ]
    }
   ],
   "source": [
    "print(phrase_list[:500:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able add printer mac still working advise calling support team mon fri excluding bank holidays would able look', 'absolutely one cat coyote peterson', 'accurate customs checks cause delays also causing issues expected delivery dates changing', 'add super mario flair fridge desk supermario dworld bowsersfury magnet set available platinum points shipping costs mynintendo reward get', 'additional information though pass comments team thanks']\n"
     ]
    }
   ],
   "source": [
    "print(text_list[:50:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
